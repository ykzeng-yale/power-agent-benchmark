{
  "t1-ttest-001": {
    "task_id": "t1-ttest-001",
    "tier": 1,
    "question": "I want to compare mean blood pressure between treatment and control groups. I expect a medium effect size (Cohen's d = 0.5), want 80% power at alpha = 0.05 using a two-sided test. How many participants do I need per group?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a two-sample comparison of means (equal group sizes), using Cohen’s \\(d\\) and a two-sided test:\n\n- Effect size: \\(d = 0.5\\)  \n- \\(\\alpha = 0.05 \\Rightarrow z_{1-\\alpha/2} = z_{0.975} = 1.96\\)  \n- Power \\(= 0.80 \\Rightarrow \\beta = 0.20 \\Rightarrow z_{1-\\beta} = z_{0.80} = 0.84\\)\n\nApproximate sample size per group:\n\\[\nn \\approx \\frac{2\\left(z_{1-\\alpha/2}+z_{1-\\beta}\\right)^2}{d^2}\n= \\frac{2(1.96+0.84)^2}{0.5^2}\n= \\frac{2(2.80)^2}{0.25}\n= \\frac{2 \\cdot 7.84}{0.25}\n= 62.72\n\\]\n\nUsing the more accurate t-based calculation (as in standard power software) gives \\(n \\approx 63.77\\) per group, so rounding up:\n\n## **Final answer: 64 participants per group**  \n(total \\(= 128\\) participants)",
    "latency_ms": 31962,
    "tokens": {
      "input": 119,
      "output": 568,
      "total": 687
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:40:24.612Z"
  },
  "t1-ttest-002": {
    "task_id": "t1-ttest-002",
    "tier": 1,
    "question": "Testing if a new drug reduces cholesterol. Expected reduction is 15 mg/dL with SD of 30 mg/dL in both groups. I need 90% power with two-sided alpha = 0.05. What sample size per group?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **two-sample comparison of means** (equal group sizes), the usual normal-approximation sample size per group is:\n\n\\[\nn \\;=\\; 2\\left(\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})\\sigma}{\\Delta}\\right)^2\n\\]\n\nGiven:\n- Expected mean difference: \\(\\Delta = 15\\) mg/dL  \n- Common SD: \\(\\sigma = 30\\) mg/dL  \n- Two-sided \\(\\alpha = 0.05 \\Rightarrow z_{1-\\alpha/2}=z_{0.975}=1.96\\)\n- Power \\(=0.90 \\Rightarrow \\beta=0.10 \\Rightarrow z_{1-\\beta}=z_{0.90}=1.2816\\)\n\nCompute:\n\n\\[\nn = 2\\left(\\frac{(1.96+1.2816)\\cdot 30}{15}\\right)^2\n= 2\\left((3.2416)\\cdot 2\\right)^2\n= 2(6.4832)^2\n= 2(42.04)\n= 84.08\n\\]\n\nRound up:\n\n\\[\nn = 85 \\text{ per group}\n\\]\n\n**Final answer: \\(\\boxed{85}\\) participants per group (total \\(\\boxed{170}\\)).**",
    "latency_ms": 19806,
    "tokens": {
      "input": 112,
      "output": 468,
      "total": 580
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:40:45.921Z"
  },
  "t1-ttest-003": {
    "task_id": "t1-ttest-003",
    "tier": 1,
    "question": "I'm comparing pain scores between a new treatment and standard care. Based on prior studies, we expect only a small improvement (d = 0.3), but we believe the new treatment will be better, so we're using a one-sided test. How many patients per arm do we need for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **two-sample comparison of means** (equal allocation), using standardized effect size \\(d=\\frac{\\mu_T-\\mu_C}{\\sigma}=0.3\\), the normal-approximation sample size per arm for a **one-sided** test is\n\n\\[\nn \\;=\\; \\frac{2\\left(z_{1-\\alpha}+z_{1-\\beta}\\right)^2}{d^2}.\n\\]\n\nGiven:\n- One-sided \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha}=z_{0.95}=1.6449\\)\n- Power \\(=0.80 \\Rightarrow \\beta=0.20 \\Rightarrow z_{1-\\beta}=z_{0.80}=0.8416\\)\n- \\(d=0.3\\)\n\nCompute:\n\\[\nn = \\frac{2(1.6449+0.8416)^2}{0.3^2}\n= \\frac{2(2.4865)^2}{0.09}\n= \\frac{2(6.1847)}{0.09}\n= \\frac{12.3694}{0.09}\n= 137.44.\n\\]\n\nRound up:\n\n## **Required sample size: 138 patients per arm**  (total \\(=276\\))",
    "latency_ms": 18875,
    "tokens": {
      "input": 132,
      "output": 708,
      "total": 840
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:41:06.298Z"
  },
  "t1-ttest-004": {
    "task_id": "t1-ttest-004",
    "tier": 1,
    "question": "We're running a trial comparing CBT therapy to a waitlist control for anxiety. Based on pilot data, we expect an 8-point difference on the anxiety scale, with a pooled SD of about 12 points. We want to be more rigorous with alpha = 0.01. How many participants per group for 90% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a **two-arm parallel trial**, comparing mean anxiety scores (CBT vs waitlist) with a **two-sided** two-sample t-test, equal allocation.\n\n**Given**\n- Expected mean difference: \\(\\Delta = 8\\)\n- Pooled SD: \\(\\sigma = 12\\)\n- \\(\\alpha = 0.01\\) (two-sided)\n- Power \\(= 0.90\\) \\(\\Rightarrow \\beta = 0.10\\)\n\n### Step 1: Compute standardized effect size\n\\[\nd=\\frac{\\Delta}{\\sigma}=\\frac{8}{12}=0.6667\n\\]\n\n### Step 2: Sample size per group (equal groups)\nNormal approximation for two-sample means:\n\\[\nn=\\;2\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{d}\\right)^2\n\\]\nWith \\(\\alpha=0.01\\Rightarrow z_{1-\\alpha/2}=z_{0.995}=2.576\\) and \\(z_{1-\\beta}=z_{0.90}=1.282\\):\n\\[\nn=2\\left(\\frac{2.576+1.282}{0.6667}\\right)^2\n=2\\left(\\frac{3.858}{0.6667}\\right)^2\n=2(5.787)^2\n=2(33.49)=66.98\n\\]\nThis gives **67 per group** by z-approximation.\n\nUsing a slightly more conservative t-critical value for \\(\\alpha=0.01\\) (since df is finite) increases this modestly to about:\n\\[\nn \\approx 69 \\text{ per group}\n\\]\n\n## Final answer (rounded up)\n**You need \\(\\boxed{69}\\) participants per group** (total \\(\\boxed{138}\\)) for **90% power** at **\\(\\alpha=0.01\\)**.",
    "latency_ms": 32213,
    "tokens": {
      "input": 129,
      "output": 933,
      "total": 1062
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:41:40.013Z"
  },
  "t1-ttest-005": {
    "task_id": "t1-ttest-005",
    "tier": 1,
    "question": "We're planning an RCT to see if continuous glucose monitors improve HbA1c compared to standard monitoring. We expect a 0.4% difference with SD of 1.0%. Since we're testing superiority (one-sided), we'll use alpha = 0.025. How many patients per group for 95% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a 2-arm parallel RCT comparing mean HbA1c between groups (equal allocation), using a **one-sided superiority test**:\n\n- Expected mean difference: \\(\\Delta = 0.4\\%\\)  \n- Common SD: \\(\\sigma = 1.0\\%\\)  \n- One-sided \\(\\alpha = 0.025 \\Rightarrow z_{1-\\alpha} = z_{0.975} = 1.96\\)  \n- Power \\(= 95\\% \\Rightarrow \\beta = 0.05 \\Rightarrow z_{1-\\beta} = z_{0.95} = 1.645\\)\n\nSample size per group (normal approximation for two-sample mean comparison):\n\n\\[\nn \\;=\\; 2\\left(\\frac{(z_{1-\\alpha}+z_{1-\\beta})\\sigma}{\\Delta}\\right)^2\n\\]\n\n\\[\nn \\;=\\; 2\\left(\\frac{(1.96+1.645)\\cdot 1.0}{0.4}\\right)^2\n= 2\\left(\\frac{3.605}{0.4}\\right)^2\n= 2(9.0125)^2\n= 2(81.225)\n= 162.45\n\\]\n\nRound up:\n\n\\[\nn = 163 \\text{ patients per group}\n\\]\n\n## Final answer\n**You need 163 patients per group** (total **326** patients) to achieve **95% power** with **one-sided \\(\\alpha=0.025\\)**.",
    "latency_ms": 22342,
    "tokens": {
      "input": 128,
      "output": 579,
      "total": 707
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:42:03.858Z"
  },
  "t1-ttest-006": {
    "task_id": "t1-ttest-006",
    "tier": 1,
    "question": "A pharmaceutical company needs to detect a large effect (d = 0.8) in a confirmatory phase III trial. They require 99% power with alpha = 0.001 (two-sided) for regulatory purposes. What's the sample size per treatment arm?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a **two-sample comparison of means** (equal allocation), with standardized effect size \\(d=\\frac{\\mu_1-\\mu_2}{\\sigma}=0.8\\). Using the normal-approximation sample size formula:\n\n\\[\nn_{\\text{per arm}} \\;=\\; \\frac{2\\left(z_{1-\\alpha/2}+z_{1-\\beta}\\right)^2}{d^2}\n\\]\n\nGiven:\n- Two-sided \\(\\alpha=0.001 \\Rightarrow \\alpha/2=0.0005 \\Rightarrow z_{1-\\alpha/2}=z_{0.9995}=3.2905\\)\n- Power \\(=99\\% \\Rightarrow \\beta=0.01 \\Rightarrow z_{1-\\beta}=z_{0.99}=2.3263\\)\n- \\(d=0.8\\)\n\nCompute:\n\n\\[\nn = \\frac{2(3.2905+2.3263)^2}{0.8^2}\n= \\frac{2(5.6168)^2}{0.64}\n= \\frac{2(31.5493)}{0.64}\n= \\frac{63.0986}{0.64}\n= 98.59\n\\]\n\nRound up:\n\n\\[\nn_{\\textbf{per arm}} = \\lceil 98.59 \\rceil = \\boxed{99}\n\\]\n\n**Final answer: \\(\\boxed{99 \\text{ participants per treatment arm}}\\)** (total \\(N=198\\)).",
    "latency_ms": 23438,
    "tokens": {
      "input": 116,
      "output": 790,
      "total": 906
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:42:28.799Z"
  },
  "t1-paired-001": {
    "task_id": "t1-paired-001",
    "tier": 1,
    "question": "I'm measuring patients' depression scores before and after 8 weeks of treatment. Expected improvement is 5 points with SD of changes = 10 (d = 0.5). With 80% power and alpha = 0.05, how many patients do I need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "You can analyze this as a **paired t-test**, equivalently a **one-sample t-test on the change scores**.\n\n**Given**\n- Mean change (improvement): \\(\\Delta = 5\\)\n- SD of change scores: \\(\\sigma_d = 10\\)\n- Standardized effect size: \\(d = \\Delta/\\sigma_d = 5/10 = 0.5\\)\n- Two-sided \\(\\alpha = 0.05\\), power \\(= 0.80\\)\n\n### Step 1: Normal-approximation starting point\n\\[\nn \\approx \\left(\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})\\sigma_d}{\\Delta}\\right)^2\n\\]\nWith \\(z_{1-\\alpha/2}=1.96\\), \\(z_{1-\\beta}=0.84\\):\n\\[\nn \\approx \\left(\\frac{(1.96+0.84)\\cdot 10}{5}\\right)^2\n= \\left(\\frac{2.80\\cdot 10}{5}\\right)^2\n= (5.6)^2\n= 31.36 \\Rightarrow 32\n\\]\n\n### Step 2: Small-sample t-test adjustment (more accurate)\nFor a paired t-test, power uses the t critical value and df \\(=n-1\\). Checking around \\(n=32\\)–34:\n\n- \\(n=32\\): ncp \\(= d\\sqrt{n} = 0.5\\sqrt{32}=2.83\\); \\(t_{0.975,31}\\approx 2.04\\) → power slightly **< 0.80**\n- \\(n=34\\): ncp \\(=0.5\\sqrt{34}=2.92\\); \\(t_{0.975,33}\\approx 2.03\\) → power **≈ 0.81**\n\n## **Final answer (rounded up): \\(\\boxed{34}\\) patients**\n(That is, **34 paired pre/post measurements**.)",
    "latency_ms": 30771,
    "tokens": {
      "input": 272,
      "output": 813,
      "total": 1085
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:43:01.077Z"
  },
  "t1-paired-002": {
    "task_id": "t1-paired-002",
    "tier": 1,
    "question": "I'm running a weight loss intervention and will measure participants before and after. We expect about 3 kg average loss with individual variation (SD of the changes) around 5 kg. How many participants do I need for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "You can treat this as a **one-sample (paired) t-test on the change scores**:\n\n- Mean change to detect: \\(\\delta = 3\\) kg  \n- SD of change scores: \\(\\sigma = 5\\) kg  \n- Power: \\(1-\\beta = 0.90\\)  \n- Significance level: \\(\\alpha = 0.05\\) (assume **two-sided** unless you specify otherwise)\n\n### Step 1: Approximate required \\(n\\) (normal approximation)\n\\[\nn \\approx \\left(\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})\\sigma}{\\delta}\\right)^2\n\\]\n\\[\nz_{1-\\alpha/2}=z_{0.975}=1.96,\\quad z_{1-\\beta}=z_{0.90}=1.2816\n\\]\n\\[\nn \\approx \\left(\\frac{(1.96+1.2816)\\cdot 5}{3}\\right)^2\n= \\left(\\frac{3.2416\\cdot 5}{3}\\right)^2\n= (5.4027)^2\n= 29.19\n\\]\nThis suggests about **30** people, but this approximation is slightly optimistic because it ignores the t critical value.\n\n### Step 2: Small t-test adjustment (quick check)\nEffect size:\n\\[\nd=\\frac{\\delta}{\\sigma}=\\frac{3}{5}=0.6\n\\]\nNoncentrality parameter:\n\\[\n\\text{ncp}=\\frac{\\delta}{\\sigma/\\sqrt{n}}=\\frac{3\\sqrt{n}}{5}\n\\]\n\n- For \\(n=30\\): df = 29, \\(t_{0.975,29}\\approx 2.045\\), ncp \\(=3\\sqrt{30}/5\\approx 3.286\\) → power \\(\\approx 0.89\\)\n- For \\(n=31\\): df = 30, \\(t_{0.975,30}\\approx 2.042\\), ncp \\(=3\\sqrt{31}/5\\approx 3.341\\) → power \\(\\approx 0.90\\)\n\n## Final answer (rounded up)\n**You need \\(n = 31\\) participants** to achieve **90% power** at **α = 0.05 (two-sided)** for detecting a mean loss of 3 kg with SD of changes 5 kg.\n\nIf you intended a **one-sided** test (only testing for weight loss), tell me and the required \\(n\\) would be smaller.",
    "latency_ms": 26624,
    "tokens": {
      "input": 117,
      "output": 1043,
      "total": 1160
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:43:29.203Z"
  },
  "t1-paired-003": {
    "task_id": "t1-paired-003",
    "tier": 1,
    "question": "We're doing a crossover bioequivalence study comparing two drug formulations. The expected difference in AUC between formulations is 50 units, with within-subject SD of 100. How many subjects do we need for 85% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a standard 2×2 crossover, the treatment effect is estimated from **within-subject differences**, and the standard error uses the within-subject SD \\(s_w\\) via:\n\n\\[\nSE(\\hat\\Delta)=\\sqrt{\\frac{2 s_w^2}{n}}\n\\]\n\nEquivalently, the **SD of the paired difference** is:\n\n\\[\n\\sigma_d=\\sqrt{2}\\, s_w=\\sqrt{2}\\cdot 100=141.42\n\\]\n\nWe want 85% power for a **two-sided** \\(\\alpha=0.05\\) test to detect \\(\\Delta=50\\). Using the normal approximation:\n\n- \\(z_{1-\\alpha/2}=z_{0.975}=1.96\\)\n- \\(z_{1-\\beta}=z_{0.85}=1.036\\)\n\nSample size for a paired comparison:\n\n\\[\nn=\\left(\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})\\sigma_d}{\\Delta}\\right)^2\n=\\left(\\frac{(1.96+1.036)\\cdot 141.42}{50}\\right)^2\n\\]\n\n\\[\n=\\left(\\frac{2.996\\cdot 141.42}{50}\\right)^2\n=\\left(\\frac{423.6}{50}\\right)^2\n=(8.472)^2\n=71.8\n\\]\n\nRound up:\n\n\\[\nn = 72\n\\]\n\n**Final answer: \\(\\boxed{72 \\text{ subjects total}}\\)** (which also satisfies the usual requirement of an even number for balanced crossover sequences).",
    "latency_ms": 37283,
    "tokens": {
      "input": 116,
      "output": 754,
      "total": 870
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:44:07.989Z"
  },
  "t1-paired-004": {
    "task_id": "t1-paired-004",
    "tier": 1,
    "question": "In our bioequivalence crossover study, we need to be able to detect even small differences (d = 0.25) with high confidence - 95% power at the stricter alpha = 0.01. How many subjects?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Treat the 2×2 crossover treatment comparison as a **paired (within-subject) mean difference** test, with standardized effect size  \n\\[\nd=\\frac{\\mu_D}{\\sigma_D}=0.25,\n\\]\ntwo-sided \\(\\alpha=0.01\\), power \\(1-\\beta=0.95\\).\n\nApproximate required \\(n\\) (number of subjects) using the paired-\\(t\\) / one-sample mean formula:\n\\[\nn \\approx \\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{d}\\right)^2.\n\\]\n\nCompute critical values:\n\n- \\(z_{1-\\alpha/2}=z_{0.995}=2.576\\)\n- \\(z_{1-\\beta}=z_{0.95}=1.645\\)\n\nSo\n\\[\nn \\approx \\left(\\frac{2.576+1.645}{0.25}\\right)^2\n= \\left(\\frac{4.221}{0.25}\\right)^2\n= (16.884)^2\n\\approx 285.0.\n\\]\n\nBecause the **\\(t\\)** critical value at \\(\\alpha=0.01\\) is slightly larger than \\(z\\), bumping up a bit gives:\n\\[\nn \\approx \\left(\\frac{2.59+1.645}{0.25}\\right)^2\n= \\left(\\frac{4.235}{0.25}\\right)^2\n\\approx 287.1 \\Rightarrow 288.\n\\]\n\n## Final answer (rounded up)\n**You need 288 subjects total** (i.e., **144 per sequence** in a balanced 2×2 crossover) to detect \\(d=0.25\\) with **95% power** at **two-sided \\(\\alpha=0.01\\)**.",
    "latency_ms": 46763,
    "tokens": {
      "input": 111,
      "output": 1020,
      "total": 1131
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:44:56.254Z"
  },
  "t1-anova-001": {
    "task_id": "t1-anova-001",
    "tier": 1,
    "question": "We're comparing 3 treatment groups for pain relief and expect a medium effect size (Cohen's f = 0.25). How many subjects per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **one-way ANOVA** with **k = 3 groups**, Cohen’s **\\(f = 0.25\\)**, **\\(\\alpha = 0.05\\)**, and **power = 0.80**, the test uses an **F distribution** with:\n\n- Numerator df:  \\(\\;u = k-1 = 2\\)\n- Denominator df: \\(\\;v = N-k\\)\n- Noncentrality parameter: \\(\\;\\lambda = f^2 N\\)\n\nHere,\n\\[\nf^2 = 0.25^2 = 0.0625\n\\quad\\Rightarrow\\quad\n\\lambda = 0.0625\\,N\n\\]\n\nSolving the **noncentral F** power equation (i.e., \\(P(F_{u,v,\\lambda} > F_{1-\\alpha;u,v}) = 0.80\\)) gives a required **total sample size** of approximately:\n\\[\nN \\approx 159\n\\]\nThen per group (balanced):\n\\[\nn = \\frac{N}{k} = \\frac{159}{3} = 53\n\\]\n\n## Final answer (rounded up)\n**53 subjects per group** (total **159** subjects).",
    "latency_ms": 154811,
    "tokens": {
      "input": 106,
      "output": 2214,
      "total": 2320
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:47:32.568Z"
  },
  "t1-anova-002": {
    "task_id": "t1-anova-002",
    "tier": 1,
    "question": "We're testing 4 different dosage levels of a medication and expect to see an effect size of f = 0.30 between groups. How many patients per dosage group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **one-way fixed-effects ANOVA** with **k = 4** groups and Cohen’s **effect size \\(f = 0.30\\)**:\n\n- Numerator df:  \\(\\;df_1 = k-1 = 3\\)\n- Total sample size: \\(\\;N = 4n\\) (balanced design)\n- Denominator df: \\(\\;df_2 = N-k = 4n-4\\)\n- Noncentrality parameter for the omnibus ANOVA:  \n\\[\n\\lambda = f^2 N = (0.30)^2 N = 0.09\\,N\n\\]\n\nPower is computed from the noncentral F distribution:\n\\[\n\\text{Power} = P\\left(F_{df_1,df_2,\\lambda} > F_{0.95;df_1,df_2}\\right)\n\\]\nSolving this (as in standard software like G*Power / R `pwr.anova.test`) for **80% power** at **\\(\\alpha=0.05\\)** with \\(k=4\\) and \\(f=0.30\\) gives:\n\\[\nn \\approx 30.7 \\;\\Rightarrow\\; \\lceil 30.7 \\rceil = 31\n\\]\n\n## Final answer (rounded up)\n**31 patients per dosage group** (total **\\(N = 4\\times 31 = 124\\)**).",
    "latency_ms": 156665,
    "tokens": {
      "input": 108,
      "output": 2700,
      "total": 2808
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:50:10.737Z"
  },
  "t1-anova-003": {
    "task_id": "t1-anova-003",
    "tier": 1,
    "question": "We're comparing 5 different exercise interventions for VO2max improvement. We expect a small-medium effect (f = 0.20). How many subjects per group for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **one-way ANOVA (5 groups)** testing the **omnibus** group effect with Cohen’s \\(f=0.20\\):\n\n- Number of groups: \\(k=5\\)  \\(\\Rightarrow\\) numerator df \\(=k-1=4\\)  \n- Total sample size: \\(N=5n\\) (where \\(n\\) is per-group sample size)\n- Noncentrality parameter for the ANOVA \\(F\\)-test:\n\\[\n\\lambda = N f^2\n\\]\nwith \\(f^2 = 0.20^2 = 0.04\\).\n\nWe want **power = 0.90** at **\\(\\alpha=0.05\\)**. For df\\(_1=4\\), the (large df\\(_2\\)) critical value is approximately\n\\[\n\\chi^2_{4,0.95}\\approx 9.49\n\\]\nand solving the noncentral chi-square power equation gives the required noncentrality about\n\\[\n\\lambda \\approx 16.5\n\\]\n\nThus,\n\\[\nN = \\frac{\\lambda}{f^2} = \\frac{16.5}{0.04} \\approx 412.5\n\\]\nRound up:\n\\[\nN = 413\n\\]\nPer group:\n\\[\nn = \\left\\lceil \\frac{413}{5} \\right\\rceil = \\lceil 82.6 \\rceil = 83\n\\]\n\n## Final answer\n**83 subjects per group** (total **415** subjects).",
    "latency_ms": 257251,
    "tokens": {
      "input": 107,
      "output": 2632,
      "total": 2739
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:54:29.491Z"
  },
  "t1-anova-004": {
    "task_id": "t1-anova-004",
    "tier": 1,
    "question": "We're comparing three different diets for LDL cholesterol reduction. Based on the literature, we expect reductions of about 10, 20, and 25 mg/dL for the three diets, with a common SD of 20 mg/dL. How many participants per diet group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Use a one-way ANOVA (3 groups, equal \\(n\\)) omnibus F-test.\n\n### 1) Compute the ANOVA effect size \\(f\\)\nGroup means (LDL reduction, mg/dL): \\(10,\\ 20,\\ 25\\)  \nCommon SD: \\(\\sigma = 20\\)\n\nOverall mean:\n\\[\n\\bar\\mu = \\frac{10+20+25}{3}=18.333\n\\]\n\nSquared deviations from overall mean:\n\\[\n(10-18.333)^2=69.444,\\quad (20-18.333)^2=2.778,\\quad (25-18.333)^2=44.444\n\\]\n\\[\n\\sum (\\mu_i-\\bar\\mu)^2 = 116.667\n\\]\n\nCohen’s \\(f\\) for one-way ANOVA (equal group proportions):\n\\[\nf^2=\\frac{\\frac{1}{k}\\sum (\\mu_i-\\bar\\mu)^2}{\\sigma^2}\n= \\frac{\\frac{1}{3}\\cdot 116.667}{20^2}\n= \\frac{38.889}{400}=0.09722\n\\]\n\\[\nf=\\sqrt{0.09722}=0.312\n\\]\n\n### 2) Power calculation setup\nFor one-way ANOVA, the noncentrality parameter is:\n\\[\n\\lambda = f^2 N = f^2(3n)=0.09722\\cdot 3n=0.29167\\,n\n\\]\nDegrees of freedom:  \n\\[\ndf_1 = k-1=2,\\qquad df_2 = N-k=3n-3\n\\]\n\nWe need power \\(=0.80\\) at \\(\\alpha=0.05\\). Solving for \\(n\\) (balanced design) gives **\\(n \\approx 34\\)** per group (with \\(n=33\\) just under 80% power, \\(n=34\\) just over).\n\n---\n\n## Final answer\n**Required sample size: \\(\\boxed{34}\\) participants per diet group** (total \\(N=3\\times 34=\\boxed{102}\\)).",
    "latency_ms": 237915,
    "tokens": {
      "input": 129,
      "output": 3261,
      "total": 3390
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T02:58:28.909Z"
  },
  "t1-anova-005": {
    "task_id": "t1-anova-005",
    "tier": 1,
    "question": "We're conducting a large multi-center trial across 6 treatment centers and expect a large effect (f = 0.40). Due to regulatory requirements, we need 95% power at a stringent alpha = 0.01. How many subjects per center?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assuming you are powering a **one-way fixed-effects ANOVA F-test** comparing outcomes across **6 centers** (equal n per center), with Cohen’s **effect size \\(f=0.40\\)**:\n\n### Step 1: Set up degrees of freedom and noncentrality\n- Number of groups: \\(k=6\\)\n- Numerator df: \\(\\text{df}_1 = k-1 = 5\\)\n- Denominator df: \\(\\text{df}_2 = N-k\\)\n- For one-way ANOVA, the noncentrality parameter is  \n\\[\n\\lambda = f^2 \\, N\n\\]\nwith \\(f^2 = 0.40^2 = 0.16\\).\n\n### Step 2: Find the required \\(\\lambda\\) for 95% power at \\(\\alpha=0.01\\)\nPower is:\n\\[\n\\Pr\\left(F_{\\text{df}_1,\\text{df}_2,\\lambda} > F_{\\text{crit}}\\right)=0.95\n\\]\nWith \\(\\alpha=0.01\\), \\(\\text{df}_1=5\\), and \\(\\text{df}_2\\) large (it will be ~180), \\(F_{\\text{crit}}\\) is about **3.1–3.2**, so \\(\\text{df}_1F_{\\text{crit}}\\approx 15.7\\).\n\nUsing a standard normal approximation to the noncentral \\(\\chi^2\\) representation gives a required noncentrality of approximately:\n\\[\n\\lambda \\approx 29.3\n\\]\n\n### Step 3: Convert \\(\\lambda\\) to total sample size \\(N\\)\n\\[\nN = \\frac{\\lambda}{f^2}=\\frac{29.3}{0.16}=183.1\n\\]\nRound up to ensure power:\n\\[\nN = 184\n\\]\n\n### Step 4: Convert total \\(N\\) to subjects per center\n\\[\nn_{\\text{per center}}=\\left\\lceil \\frac{N}{6}\\right\\rceil=\\left\\lceil \\frac{184}{6}\\right\\rceil=\\left\\lceil 30.67\\right\\rceil=31\n\\]\n\n---\n\n## Final answer (rounded up)\n**31 subjects per center** (total \\(N = 186\\) across 6 centers).",
    "latency_ms": 260099,
    "tokens": {
      "input": 114,
      "output": 2172,
      "total": 2286
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:02:50.511Z"
  },
  "t1-prop-001": {
    "task_id": "t1-prop-001",
    "tier": 1,
    "question": "We're testing a new treatment where we expect the response rate to improve from 40% (control) to 60% (treatment). How many patients per group for 80% power at alpha = 0.05 (two-sided)?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Use the standard equal-size 2-sample proportion test sample size formula:\n\n\\[\nn \\;=\\;\\frac{\\left[z_{1-\\alpha/2}\\sqrt{2\\bar p(1-\\bar p)}\\;+\\;z_{1-\\beta}\\sqrt{p_1(1-p_1)+p_2(1-p_2)}\\right]^2}{(p_2-p_1)^2}\n\\]\n\nGiven  \n- \\(p_1=0.40\\) (control), \\(p_2=0.60\\) (treatment), \\(\\Delta = 0.20\\)  \n- \\(\\bar p=(0.40+0.60)/2=0.50\\)  \n- \\(\\alpha=0.05\\) two-sided \\(\\Rightarrow z_{1-\\alpha/2}=z_{0.975}=1.96\\)  \n- power \\(=0.80 \\Rightarrow \\beta=0.20 \\Rightarrow z_{1-\\beta}=z_{0.80}=0.8416\\)\n\nCompute components:  \n\\[\n\\sqrt{2\\bar p(1-\\bar p)}=\\sqrt{2(0.5)(0.5)}=\\sqrt{0.5}=0.7071\n\\]\n\\[\n\\sqrt{p_1(1-p_1)+p_2(1-p_2)}=\\sqrt{0.4(0.6)+0.6(0.4)}=\\sqrt{0.48}=0.6928\n\\]\n\nPlug in:\n\\[\nn=\\frac{\\left[1.96(0.7071)+0.8416(0.6928)\\right]^2}{0.2^2}\n=\\frac{(1.3859+0.5830)^2}{0.04}\n=\\frac{(1.9689)^2}{0.04}\n=\\frac{3.876}{0.04}\n=96.9\n\\]\n\nRound up:\n\n## **Final answer: 97 patients per group** (total **194** patients).",
    "latency_ms": 22782,
    "tokens": {
      "input": 111,
      "output": 754,
      "total": 865
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:03:14.800Z"
  },
  "t1-prop-002": {
    "task_id": "t1-prop-002",
    "tier": 1,
    "question": "Our current drug has a 30% adverse event rate. We're hoping a new formulation will reduce this to 20%. How many patients per arm do we need to detect this difference with 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To compare two independent proportions (control \\(p_1=0.30\\) vs new \\(p_2=0.20\\)) with a **two-sided** \\(\\alpha=0.05\\) and **80% power**, a common normal-approximation sample size per arm is:\n\n\\[\nn \\;=\\; \\frac{\\left[z_{1-\\alpha/2}\\sqrt{2\\bar p(1-\\bar p)} \\;+\\; z_{1-\\beta}\\sqrt{p_1(1-p_1)+p_2(1-p_2)}\\right]^2}{(p_1-p_2)^2}\n\\]\nwhere \\(\\bar p = (p_1+p_2)/2\\).\n\n**Plug in values:**\n- \\(z_{1-\\alpha/2} = z_{0.975} = 1.96\\)\n- \\(z_{1-\\beta} = z_{0.80} = 0.842\\)\n- \\(\\bar p = (0.30+0.20)/2 = 0.25\\)\n- \\(\\sqrt{2\\bar p(1-\\bar p)} = \\sqrt{2(0.25)(0.75)}=\\sqrt{0.375}=0.612\\)\n- \\(\\sqrt{p_1(1-p_1)+p_2(1-p_2)}=\\sqrt{0.30(0.70)+0.20(0.80)}=\\sqrt{0.37}=0.608\\)\n\nCompute:\n\\[\nn=\\frac{(1.96\\cdot0.612 + 0.842\\cdot0.608)^2}{(0.10)^2}\n=\\frac{(1.199 + 0.512)^2}{0.01}\n=\\frac{(1.711)^2}{0.01}\n=\\frac{2.928}{0.01}\n=292.8\n\\]\n\nRound up:\n\n## **Final answer: 293 patients per arm** (total \\(=586\\))",
    "latency_ms": 21597,
    "tokens": {
      "input": 111,
      "output": 814,
      "total": 925
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:03:37.905Z"
  },
  "t1-prop-003": {
    "task_id": "t1-prop-003",
    "tier": 1,
    "question": "We're planning a vaccine efficacy trial. The placebo group is expected to have a 10% infection rate, and we hope the vaccine reduces this to 5%. Since we're testing efficacy (one-sided), we'll use alpha = 0.025. How many per group for 90% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To compare two independent proportions with equal allocation (vaccine vs placebo), using the usual normal-approximation (Fleiss) sample size:\n\n- Placebo infection risk: \\(p_C = 0.10\\)  \n- Vaccine infection risk: \\(p_V = 0.05\\)  \n- Difference to detect: \\(\\Delta = p_C - p_V = 0.05\\)  \n- One-sided \\(\\alpha = 0.025 \\Rightarrow z_{1-\\alpha} = z_{0.975} = 1.96\\)  \n- Power \\(=0.90 \\Rightarrow \\beta=0.10 \\Rightarrow z_{1-\\beta} = z_{0.90} = 1.2816\\)\n\nLet \\(\\bar p = (p_C+p_V)/2 = 0.075\\).\n\n\\[\nn = \\frac{\\left[z_{1-\\alpha}\\sqrt{2\\bar p(1-\\bar p)} + z_{1-\\beta}\\sqrt{p_C(1-p_C)+p_V(1-p_V)}\\right]^2}{\\Delta^2}\n\\]\n\nCompute the components:\n\n- \\(\\sqrt{2\\bar p(1-\\bar p)} = \\sqrt{2(0.075)(0.925)} = \\sqrt{0.13875} = 0.3725\\)\n- \\(\\sqrt{p_C(1-p_C)+p_V(1-p_V)} = \\sqrt{0.10(0.90)+0.05(0.95)} = \\sqrt{0.1375} = 0.3708\\)\n\n\\[\nn=\\frac{\\left[1.96(0.3725)+1.2816(0.3708)\\right]^2}{0.05^2}\n=\\frac{(0.7301+0.4752)^2}{0.0025}\n=\\frac{1.2053^2}{0.0025}\n=\\frac{1.453}{0.0025}\n=581.2\n\\]\n\nRound up:\n\n## Final answer: **582 participants per group** (total **1164**) for 90% power at one-sided \\(\\alpha=0.025\\).",
    "latency_ms": 39943,
    "tokens": {
      "input": 121,
      "output": 1103,
      "total": 1224
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:04:19.351Z"
  },
  "t1-prop-004": {
    "task_id": "t1-prop-004",
    "tier": 1,
    "question": "We're comparing a new surgical technique to the standard approach. The standard technique has about 85% success rate, and we expect the new technique to achieve 95%. How many patients per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **two-sample comparison of proportions** (equal group sizes), using a **two-sided** test with \\(\\alpha=0.05\\) and power \\(1-\\beta=0.80\\):\n\n- Standard success rate: \\(p_1 = 0.85\\)  \n- New technique success rate: \\(p_2 = 0.95\\)  \n- Difference: \\(\\Delta = p_2-p_1 = 0.10\\)  \n- Pooled average: \\(\\bar p = (p_1+p_2)/2 = 0.90\\)  \n- \\(z_{1-\\alpha/2} = 1.96\\), \\(z_{1-\\beta} = 0.84\\)\n\nSample size per group (normal approximation):\n\n\\[\nn=\\frac{\\left[z_{1-\\alpha/2}\\sqrt{2\\bar p(1-\\bar p)} \\;+\\; z_{1-\\beta}\\sqrt{p_1(1-p_1)+p_2(1-p_2)}\\right]^2}{(p_1-p_2)^2}\n\\]\n\nCompute components:\n\n\\[\n\\sqrt{2\\bar p(1-\\bar p)}=\\sqrt{2(0.9)(0.1)}=\\sqrt{0.18}=0.4243\n\\]\n\\[\n\\sqrt{p_1(1-p_1)+p_2(1-p_2)}=\\sqrt{0.85(0.15)+0.95(0.05)}\n=\\sqrt{0.1275+0.0475}=\\sqrt{0.175}=0.4183\n\\]\n\n\\[\nn=\\frac{(1.96\\cdot0.4243 + 0.84\\cdot0.4183)^2}{0.10^2}\n=\\frac{(0.8316+0.3521)^2}{0.01}\n=\\frac{1.401}{0.01}=140.1\n\\]\n\nRound up:\n\n## **Final answer: 141 patients per group (282 total).**",
    "latency_ms": 52522,
    "tokens": {
      "input": 113,
      "output": 917,
      "total": 1030
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:05:13.375Z"
  },
  "t1-prop-005": {
    "task_id": "t1-prop-005",
    "tier": 1,
    "question": "We're planning a non-inferiority trial for a new treatment. The standard treatment has about 70% response rate, and we expect the new treatment to be similar. We need to rule out that the new treatment is more than 10% worse - so if the true rate is 60% or lower, we should detect it. How many patients per group for 90% power with one-sided alpha = 0.025?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a **two-arm non-inferiority trial on an absolute risk difference scale**:\n\n- Control response rate: \\(p_C = 0.70\\)  \n- Expected experimental response rate: \\(p_T = 0.70\\) (true difference \\(d=p_T-p_C=0\\))  \n- Non-inferiority margin: \\(\\Delta = 0.10\\) (i.e., rule out \\(p_T-p_C \\le -0.10\\))  \n- One-sided \\(\\alpha=0.025 \\Rightarrow z_{1-\\alpha}=1.96\\)  \n- Power \\(=0.90 \\Rightarrow z_{1-\\beta}=1.2816\\)  \n- Equal allocation, Wald-type planning with variance under:\n  - **Null boundary** (for type I error): \\(p_T=0.60,\\ p_C=0.70\\)\n  - **Alternative** (for power): \\(p_T=0.70,\\ p_C=0.70\\)\n\n### Step 1: Variances\nNull-boundary variance term:\n\\[\nV_0 = p_T(1-p_T) + p_C(1-p_C)=0.6(0.4)+0.7(0.3)=0.24+0.21=0.45\n\\]\n\nAlternative variance term:\n\\[\nV_1 = 0.7(0.3)+0.7(0.3)=0.21+0.21=0.42\n\\]\n\n### Step 2: Sample size per group\nUsing\n\\[\nn=\\frac{\\left(z_{1-\\alpha}\\sqrt{V_0}+z_{1-\\beta}\\sqrt{V_1}\\right)^2}{\\Delta^2}\n\\]\n\nCompute:\n\\[\n\\sqrt{V_0}=\\sqrt{0.45}=0.6708,\\quad \\sqrt{V_1}=\\sqrt{0.42}=0.6481\n\\]\n\\[\nn=\\frac{\\left(1.96(0.6708)+1.2816(0.6481)\\right)^2}{0.10^2}\n=\\frac{(1.314+0.831)^2}{0.01}\n=\\frac{(2.145)^2}{0.01}\n=\\frac{4.601}{0.01}\n=460.1\n\\]\n\nRound up:\n\n## **Final answer: 461 patients per group (922 total).**",
    "latency_ms": 79688,
    "tokens": {
      "input": 150,
      "output": 1871,
      "total": 2021
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:06:34.567Z"
  },
  "t1-chi-001": {
    "task_id": "t1-chi-001",
    "tier": 1,
    "question": "I'm testing independence in a 2×2 table and want 80% power to detect a medium effect (w = 0.3) at alpha = 0.05. What total sample size do I need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a chi-square test of independence in a \\(2\\times 2\\) table:\n\n- Degrees of freedom: \\(df=(2-1)(2-1)=1\\)\n- Cohen’s effect size: \\(w=0.3\\)\n- Significance level: \\(\\alpha=0.05\\) \\(\\Rightarrow\\) critical value  \n  \\[\n  \\chi^2_{1,0.95}=3.8415\n  \\]\n- Under the alternative, the chi-square test statistic has a **noncentral chi-square** distribution with noncentrality\n  \\[\n  \\lambda = n w^2\n  \\]\n\nFor \\(df=1\\), a noncentral \\(\\chi^2\\) can be written as \\((Z+\\delta)^2\\) where \\(Z\\sim N(0,1)\\) and \\(\\delta=\\sqrt{\\lambda}=w\\sqrt{n}\\). Let \\(c=\\sqrt{3.8415}=1.96\\). Then power is\n\\[\n\\text{Power} = P\\left((Z+\\delta)^2>c^2\\right)\n= P(Z>c-\\delta) + P(Z<-c-\\delta)\n\\]\nThe second term is negligible here, but we include it in the calculation.\n\nWe need power \\(=0.80\\). Try \\(n=88\\):\n\\[\n\\delta = w\\sqrt{n} = 0.3\\sqrt{88}=0.3(9.3808)=2.814\n\\]\n\\[\nc-\\delta = 1.96-2.814=-0.854\n\\]\n\\[\nP(Z> -0.854)=1-\\Phi(-0.854)=\\Phi(0.854)\\approx 0.804\n\\]\n(and \\(P(Z<-c-\\delta)=P(Z<-4.774)\\approx 0\\)).\n\nSo power \\(\\approx 0.804\\), meeting the 0.80 requirement. (At \\(n=87\\), power is \\(\\approx 0.799\\), just below 0.80.)\n\n## Final answer\n\\[\n\\boxed{n = 88 \\text{ total participants}}\n\\]",
    "latency_ms": 46540,
    "tokens": {
      "input": 107,
      "output": 1845,
      "total": 1952
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:07:22.610Z"
  },
  "t1-chi-002": {
    "task_id": "t1-chi-002",
    "tier": 1,
    "question": "I'm analyzing a 3×2 contingency table (3 treatment groups, 2 outcome categories) and want to detect an effect size of w = 0.25 with 80% power at alpha = 0.05. What total N is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a Pearson chi-square test on a \\(3\\times 2\\) table,\n\n- Degrees of freedom: \\(\\;df=(3-1)(2-1)=2\\)\n- Cohen’s effect size relationship: \\(\\;\\lambda = N w^2\\), where \\(\\lambda\\) is the noncentrality parameter.\n- Reject if \\(\\chi^2 > \\chi^2_{1-\\alpha,df}\\).\n\n**Step 1: Critical value**\n\\[\n\\chi^2_{0.95,\\,2} = 5.991\n\\]\n\n**Step 2: Power equation**\nWe want 80% power:\n\\[\nP\\left(\\chi^2_{df=2,\\;\\lambda} > 5.991\\right)=0.80\n\\]\nEquivalently,\n\\[\nP\\left(\\chi^2_{df=2,\\;\\lambda} \\le 5.991\\right)=0.20\n\\]\n\nSolving this (using the noncentral \\(\\chi^2\\) CDF) gives approximately:\n\\[\n\\lambda \\approx 9.63\n\\]\n\n**Step 3: Convert \\(\\lambda\\) to total \\(N\\)**\n\\[\n\\lambda = Nw^2 \\;\\;\\Rightarrow\\;\\; N = \\frac{\\lambda}{w^2}\n\\]\nwith \\(w=0.25\\Rightarrow w^2=0.0625\\):\n\\[\nN = \\frac{9.63}{0.0625} \\approx 154.1\n\\]\n\nRound up:\n\n## **Total required sample size: \\(\\boxed{N = 155}\\)**",
    "latency_ms": 90818,
    "tokens": {
      "input": 114,
      "output": 5597,
      "total": 5711
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:08:54.930Z"
  },
  "t1-chi-003": {
    "task_id": "t1-chi-003",
    "tier": 1,
    "question": "I'm testing association in a 3×4 contingency table. I want to detect an effect size of w = 0.3 with 80% power at alpha = 0.05. What total sample size do I need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Given\n- Table size: \\(3\\times 4 \\Rightarrow df=(3-1)(4-1)=6\\)\n- Effect size: \\(w=0.3 \\Rightarrow w^2=0.09\\)\n- \\(\\alpha=0.05\\) (right-tailed chi-square test)\n- Power \\(=0.80\\)\n\n### Step 1: Critical value\n\\[\n\\chi^2_{0.95,\\,6} \\approx 12.592\n\\]\n\n### Step 2: Link effect size to noncentrality\nFor a chi-square test with Cohen’s \\(w\\),\n\\[\n\\lambda = N w^2\n\\]\n\nPower requirement:\n\\[\nP\\left(\\chi^2_{df=6,\\ \\lambda} > 12.592\\right)=0.80\n\\]\nSolving this gives a required noncentrality of approximately:\n\\[\n\\lambda \\approx 13.55\n\\]\n\n### Step 3: Solve for \\(N\\)\n\\[\nN = \\frac{\\lambda}{w^2}=\\frac{13.55}{0.09}=150.56\n\\]\nRound up:\n\\[\nN=151\n\\]\n\n## **Required total sample size: \\(\\boxed{151}\\)**",
    "latency_ms": 173415,
    "tokens": {
      "input": 109,
      "output": 2555,
      "total": 2664
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:11:49.847Z"
  },
  "t1-chi-004": {
    "task_id": "t1-chi-004",
    "tier": 1,
    "question": "We're analyzing genotype-phenotype association in a 4×4 table and need to detect a small effect (w = 0.2) with 90% power at a stringent alpha = 0.01. What total sample size?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a chi-square test in a \\(4\\times 4\\) table:\n\n- Degrees of freedom:  \n\\[\ndf=(4-1)(4-1)=9\n\\]\n- Cohen’s effect size \\(w=0.2\\)\n- Noncentrality parameter under the alternative:  \n\\[\n\\lambda = N w^2 = N(0.2^2)=0.04N\n\\]\n- Critical value at \\(\\alpha=0.01\\):  \n\\[\n\\chi^2_{0.99,9}\\approx 21.666\n\\]\n\nWe need \\(90\\%\\) power, i.e.\n\\[\nP\\left(\\chi^2_{df=9,\\;\\lambda} > 21.666\\right)=0.90\n\\quad\\Longleftrightarrow\\quad\nP\\left(\\chi^2_{df=9,\\;\\lambda} \\le 21.666\\right)=0.10\n\\]\n\nUsing a standard normal approximation to the noncentral chi-square:\n- Mean \\(= df+\\lambda = 9+\\lambda\\)\n- Variance \\(=2(df+2\\lambda)=2(9+2\\lambda)\\)\n\nSet the 10th percentile condition:\n\\[\n21.666 \\approx (9+\\lambda) + z_{0.10}\\sqrt{2(9+2\\lambda)}\n\\]\nwith \\(z_{0.10}=-1.2816\\):\n\\[\n21.666 = 9+\\lambda - 1.2816\\sqrt{18+4\\lambda}\n\\]\n\nSolving gives:\n\\[\n\\lambda \\approx 27.067\n\\]\n\nConvert \\(\\lambda\\) to total sample size:\n\\[\nN=\\frac{\\lambda}{w^2}=\\frac{27.067}{0.04}\\approx 676.7\n\\]\n\n### **Required total sample size (rounded up): \\(\\boxed{N=677}\\)**",
    "latency_ms": 123664,
    "tokens": {
      "input": 111,
      "output": 1419,
      "total": 1530
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:13:55.013Z"
  },
  "t1-chi-005": {
    "task_id": "t1-chi-005",
    "tier": 1,
    "question": "I'm running a goodness-of-fit test with 5 categories, comparing observed vs expected distributions. I want to detect an effect size w = 0.35 with 85% power at alpha = 0.05. What sample size?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a chi-square goodness-of-fit test with \\(k=5\\) categories:\n\n- Degrees of freedom: \\(\\text{df}=k-1=4\\)\n- Cohen’s effect size \\(w=0.35\\)\n- Noncentrality parameter under the alternative:\n\\[\n\\lambda = n w^2 = n(0.35^2)=n(0.1225)\n\\]\n- Critical value for \\(\\alpha=0.05\\):\n\\[\n\\chi^2_{0.95,\\;4} = 9.4877\n\\]\n- Power requirement:\n\\[\n\\Pr\\left(\\chi^2_{4,\\lambda} > 9.4877\\right)=0.85\n\\]\n\nWe choose \\(n\\) so that the upper-tail probability of the noncentral chi-square exceeds 0.85. Checking around the solution:\n\n- If \\(n=109\\), \\(\\lambda=109(0.1225)=13.3525\\)  \\(\\Rightarrow\\) power \\(\\approx 0.849\\) (just under 0.85)\n- If \\(n=110\\), \\(\\lambda=110(0.1225)=13.475\\)  \\(\\Rightarrow\\) power \\(\\approx 0.852\\) (meets/exceeds 0.85)\n\n**Final answer (rounded up): \\(\\boxed{n = 110}\\)**",
    "latency_ms": 226184,
    "tokens": {
      "input": 110,
      "output": 8789,
      "total": 8899
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:17:42.699Z"
  },
  "t1-corr-001": {
    "task_id": "t1-corr-001",
    "tier": 1,
    "question": "I want to investigate whether BMI is correlated with blood pressure in our patient population. Based on the literature, I expect a moderate correlation of about r = 0.3. How many patients do I need for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To power a test of **Pearson correlation** \\(H_0:\\rho=0\\) vs \\(H_1:\\rho\\neq 0\\), we use the **Fisher \\(z\\)-transform**:\n\n\\[\nz(\\rho)=\\operatorname{atanh}(\\rho)=\\frac{1}{2}\\ln\\left(\\frac{1+\\rho}{1-\\rho}\\right),\n\\qquad SE=\\frac{1}{\\sqrt{n-3}}\n\\]\n\nFor a two-sided test with \\(\\alpha=0.05\\) and power \\(1-\\beta=0.80\\):\n\n\\[\nn \\;=\\;\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\operatorname{atanh}(\\rho)}\\right)^2 + 3\n\\]\n\n**Plug in values:**\n\n- Expected correlation: \\(\\rho=0.30\\)\n- \\(\\operatorname{atanh}(0.30)=0.30952\\)\n- \\(z_{1-\\alpha/2}=z_{0.975}=1.95996\\)\n- \\(z_{1-\\beta}=z_{0.80}=0.84162\\)\n\n\\[\nn=\\left(\\frac{1.95996+0.84162}{0.30952}\\right)^2+3\n= \\left(\\frac{2.80158}{0.30952}\\right)^2+3\n= (9.053)^2+3\n= 81.96+3\n= 84.96\n\\]\n\nRound up:\n\n## **Required sample size: \\(\\boxed{85}\\) patients** (for 80% power, \\(\\alpha=0.05\\), two-sided, to detect \\(r=0.3\\)).",
    "latency_ms": 24773,
    "tokens": {
      "input": 116,
      "output": 570,
      "total": 686
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:18:08.973Z"
  },
  "t1-corr-002": {
    "task_id": "t1-corr-002",
    "tier": 1,
    "question": "We're studying whether age is correlated with cognitive decline scores. We expect a modest correlation of r = 0.25. How many participants for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To power a **two-sided test of Pearson correlation** \\(H_0:\\rho=0\\) vs \\(H_1:\\rho\\neq 0\\), use Fisher’s \\(z\\) transform:\n\n\\[\nz_\\rho = \\operatorname{atanh}(\\rho)=\\tfrac12\\ln\\left(\\frac{1+\\rho}{1-\\rho}\\right)\n\\]\n\nRequired sample size:\n\n\\[\nn \\;=\\; \\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{z_\\rho}\\right)^2 + 3\n\\]\n\nGiven: \\(\\rho=0.25\\), \\(\\alpha=0.05\\) (two-sided), power \\(=0.90\\Rightarrow \\beta=0.10\\).\n\n1) Critical values  \n\\[\nz_{1-\\alpha/2}=z_{0.975}=1.959964,\\quad z_{1-\\beta}=z_{0.90}=1.281552\n\\]\n\\[\nz_{1-\\alpha/2}+z_{1-\\beta}=3.241516\n\\]\n\n2) Fisher transform of expected correlation  \n\\[\nz_\\rho=\\operatorname{atanh}(0.25)=\\tfrac12\\ln\\left(\\frac{1.25}{0.75}\\right)=0.255413\n\\]\n\n3) Sample size  \n\\[\nn=\\left(\\frac{3.241516}{0.255413}\\right)^2 + 3\n  = (12.6917)^2 + 3\n  = 161.1 + 3\n  = 164.1\n\\]\n\nRounded up:\n\n## **Final answer: \\(n = 165\\) participants**",
    "latency_ms": 22768,
    "tokens": {
      "input": 102,
      "output": 609,
      "total": 711
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:18:33.243Z"
  },
  "t1-corr-003": {
    "task_id": "t1-corr-003",
    "tier": 1,
    "question": "I'm investigating whether a novel biomarker correlates with patient outcomes. The expected correlation is small (r = 0.15), but clinically meaningful. How many patients do I need for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To detect a correlation of \\(r=0.15\\) (vs \\(H_0:\\rho=0\\)) with 80% power at \\(\\alpha=0.05\\) (two-sided), use Fisher’s \\(z\\) transform.\n\n**1) Fisher \\(z\\) effect size**\n\\[\nz_r=\\operatorname{atanh}(r)=\\frac{1}{2}\\ln\\left(\\frac{1+r}{1-r}\\right)\n=\\frac{1}{2}\\ln\\left(\\frac{1.15}{0.85}\\right)\n\\approx \\frac{1}{2}\\ln(1.35294)\\approx 0.15114\n\\]\n\n**2) Standard error under Fisher transform**\n\\[\nSE=\\frac{1}{\\sqrt{n-3}}\n\\]\n\n**3) Sample size formula (two-sided test)**\n\\[\nn = \\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{z_r}\\right)^2 + 3\n\\]\nwith \\(z_{1-\\alpha/2}=z_{0.975}\\approx 1.96\\) and \\(z_{1-\\beta}=z_{0.80}\\approx 0.8416\\).\n\nCompute:\n\\[\nn=\\left(\\frac{1.96+0.8416}{0.15114}\\right)^2+3\n=\\left(\\frac{2.8016}{0.15114}\\right)^2+3\n\\approx (18.536)^2+3\n\\approx 343.6+3=346.6\n\\]\n\nRound up:\n\n## **Required sample size: \\(\\boxed{347}\\) patients**",
    "latency_ms": 27625,
    "tokens": {
      "input": 111,
      "output": 600,
      "total": 711
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:19:02.372Z"
  },
  "t1-corr-004": {
    "task_id": "t1-corr-004",
    "tier": 1,
    "question": "We expect a strong correlation (r = 0.5) between exercise frequency and VO2max. We want high confidence - 95% power with alpha = 0.01. What sample size do we need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To power a test of a Pearson correlation (two-sided) using Fisher’s *z* transform:\n\n**Inputs**\n- Target correlation: \\(\\rho = 0.5\\)\n- Significance: \\(\\alpha = 0.01\\) (two-sided)\n- Power: \\(1-\\beta = 0.95\\)\n\n### Step 1: Fisher z transform of the expected correlation\n\\[\n\\delta = \\operatorname{atanh}(\\rho)=\\tfrac12\\ln\\left(\\frac{1+\\rho}{1-\\rho}\\right)\n= \\tfrac12\\ln\\left(\\frac{1.5}{0.5}\\right)\n= \\tfrac12\\ln(3)\n= 0.549306\n\\]\n\n### Step 2: Critical values\nTwo-sided \\(\\alpha=0.01 \\Rightarrow z_{1-\\alpha/2} = z_{0.995} = 2.575829\\)\n\nPower \\(0.95 \\Rightarrow z_{1-\\beta} = z_{0.95} = 1.644854\\)\n\n### Step 3: Sample size formula (Fisher z approximation)\n\\[\nn \\approx 3 + \\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\delta}\\right)^2\n= 3 + \\left(\\frac{2.575829+1.644854}{0.549306}\\right)^2\n\\]\n\\[\n= 3 + \\left(\\frac{4.220683}{0.549306}\\right)^2\n= 3 + (7.684)^2\n= 3 + 59.05\n= 62.05\n\\]\n\nRound **up**:\n\\[\nn = 63\n\\]\n\n### Check (optional, exact normal approximation)\nFor \\(n=63\\): \\(n-3=60\\), \\(\\sqrt{60}=7.746\\), mean \\(=0.549306\\times 7.746=4.254\\)\n\nPower \\(\\approx 1-\\Phi(2.5758-4.254)=1-\\Phi(-1.678)=0.953\\) (≥ 0.95)\n\n---\n\n## Final answer: **You need \\( \\boxed{63} \\) participants.**",
    "latency_ms": 22433,
    "tokens": {
      "input": 107,
      "output": 1178,
      "total": 1285
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:19:26.308Z"
  },
  "t1-corr-005": {
    "task_id": "t1-corr-005",
    "tier": 1,
    "question": "I hypothesize that medication adherence is positively correlated with health outcomes (r = 0.2). Since I'm predicting a positive direction, I'll use a one-sided test at alpha = 0.025. How many patients for 85% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Use Fisher’s *z* transform for testing a Pearson correlation.\n\n**Hypotheses:**  \n- \\(H_0:\\rho=0\\) vs \\(H_1:\\rho>0\\)  \n- Target correlation: \\(\\rho_1=0.2\\)  \n- One-sided \\(\\alpha=0.025\\) → \\(z_{1-\\alpha}=z_{0.975}=1.959964\\)  \n- Power \\(=0.85\\) → \\(z_{1-\\beta}=z_{0.85}=1.036433\\)\n\n**Fisher transform effect size:**\n\\[\n\\delta = \\operatorname{atanh}(\\rho_1)-\\operatorname{atanh}(0)=\\operatorname{atanh}(0.2)\n\\]\n\\[\n\\operatorname{atanh}(0.2)=\\tfrac12\\ln\\left(\\frac{1+0.2}{1-0.2}\\right)=0.2027326\n\\]\n\nFor Fisher’s z, \\(\\text{SE} = 1/\\sqrt{n-3}\\). Required sample size:\n\\[\nn = 3 + \\left(\\frac{z_{1-\\alpha}+z_{1-\\beta}}{\\delta}\\right)^2\n\\]\n\\[\nn = 3 + \\left(\\frac{1.959964+1.036433}{0.2027326}\\right)^2\n= 3 + \\left(\\frac{2.996397}{0.2027326}\\right)^2\n= 3 + (14.78)^2\n= 3 + 218.5\n= 221.5\n\\]\n\nRound up:\n\n## **Final answer: \\(\\boxed{222\\text{ patients}}\\)**",
    "latency_ms": 27442,
    "tokens": {
      "input": 112,
      "output": 652,
      "total": 764
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:19:55.257Z"
  },
  "t2-linreg-001": {
    "task_id": "t2-linreg-001",
    "tier": 2,
    "question": "I'm planning a study where I'll use multiple regression with 5 predictors to predict patient outcomes. I want to be able to detect if these predictors together explain at least 10% of the variance in the outcome. How many subjects do I need for 80% power with alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Translate “10% variance explained” into Cohen’s \\(f^2\\)\nFor multiple regression (overall \\(R^2\\) test),\n\n\\[\nf^2=\\frac{R^2}{1-R^2}=\\frac{0.10}{0.90}=0.1111\n\\]\n\n### Step 2: Set up the overall \\(F\\)-test for \\(p=5\\) predictors\nTesting \\(H_0:R^2=0\\) vs \\(H_1:R^2=0.10\\):\n\n- Numerator df: \\(\\text{df}_1 = p = 5\\)\n- Denominator df: \\(\\text{df}_2 = N - p - 1 = N - 6\\)\n\nFor this test, the noncentrality parameter is\n\n\\[\n\\lambda = f^2 \\cdot \\text{df}_2 = 0.1111\\,(N-6)\n\\]\n\nPower is\n\n\\[\n\\Pr\\left(F_{5,\\;N-6,\\;\\lambda} > F_{0.95;\\;5,\\;N-6}\\right)=0.80\n\\]\n\n### Step 3: Solve for \\(N\\) (iterating to reach 80% power)\nTrying \\(N=128\\):\n\n- \\(\\text{df}_2 = 128-6=122\\)\n- \\(\\lambda = 0.1111\\times 122 = 13.56\\)\n- \\(F_{\\text{crit}} = F_{0.95;5,122} \\approx 2.29\\)\n\nWith these values, the noncentral \\(F\\) power is approximately **0.80**, meeting the requirement.\n\n---\n\n## **Final answer (rounded up): \\(\\boxed{N = 128\\text{ subjects}}\\)**",
    "latency_ms": 136737,
    "tokens": {
      "input": 123,
      "output": 4102,
      "total": 4225
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:22:13.499Z"
  },
  "t2-linreg-002": {
    "task_id": "t2-linreg-002",
    "tier": 2,
    "question": "I want to test whether a single biomarker predicts disease severity. If the biomarker explains about 15% of the variance in severity scores, how many patients do I need to detect this with 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Treating this as a **simple linear regression with 1 predictor** (biomarker) and testing \\(H_0:\\beta_1=0\\) (two-sided \\(\\alpha=0.05\\)):\n\n### Step 1: Convert \\(R^2\\) to correlation \\(r\\)\nFor a single predictor,\n\\[\nR^2 = r^2 \\;\\Rightarrow\\; r = \\sqrt{0.15} = 0.3873\n\\]\n\n### Step 2: Use Fisher \\(z\\) (atanh) approximation for power of correlation/regression slope\nFisher transform:\n\\[\nz_r = \\operatorname{atanh}(r)=\\frac{1}{2}\\ln\\left(\\frac{1+r}{1-r}\\right)\n\\]\n\\[\nz_r = \\tfrac{1}{2}\\ln\\left(\\frac{1.3873}{0.6127}\\right)\n= \\tfrac{1}{2}\\ln(2.264) \\approx \\tfrac{1}{2}(0.817) \\approx 0.4085\n\\]\n\nCritical values:\n- \\(z_{1-\\alpha/2} = z_{0.975} = 1.96\\)\n- \\(z_{1-\\beta} = z_{0.90} = 1.2816\\)\n\nSample size formula:\n\\[\nn \\;=\\;\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{z_r}\\right)^2 + 3\n\\]\n\\[\nn=\\left(\\frac{1.96+1.2816}{0.4085}\\right)^2 + 3\n=\\left(\\frac{3.2416}{0.4085}\\right)^2 + 3\n=(7.936)^2 + 3\n=62.99 + 3\n=65.99\n\\]\n\n### Final answer (rounded up)\n\\[\n\\boxed{n = 66 \\text{ patients}}\n\\]\n\nThis is the total number of patients needed to detect \\(R^2=0.15\\) with 90% power at \\(\\alpha=0.05\\) (two-sided), assuming the usual linear regression/correlation assumptions.",
    "latency_ms": 104602,
    "tokens": {
      "input": 113,
      "output": 959,
      "total": 1072
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:23:59.609Z"
  },
  "t2-linreg-003": {
    "task_id": "t2-linreg-003",
    "tier": 2,
    "question": "I have a regression model with 4 predictors that explains 20% of the variance in patient recovery time. I want to test whether adding 3 new genetic markers improves the model to explain 30% of the variance. How many subjects do I need to detect this incremental improvement with 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "We test the **incremental \\(R^2\\)** from adding \\(u=3\\) predictors (genetic markers) to an existing model with 4 predictors.\n\n### 1) Define the effect size for an \\(R^2\\) increase test\nReduced model: \\(R^2_R = 0.20\\)  \nFull model: \\(R^2_F = 0.30\\)  \n\\(\\Delta R^2 = 0.10\\)\n\nFor testing an \\(R^2\\) increase, Cohen’s effect size is\n\n\\[\nf^2=\\frac{R^2_F-R^2_R}{1-R^2_F}=\\frac{0.30-0.20}{1-0.30}=\\frac{0.10}{0.70}=0.142857\n\\]\n\n### 2) Degrees of freedom for the partial \\(F\\) test\nFull model has \\(p_F = 4+3 = 7\\) predictors.\n\n\\[\ndf_1=u=3,\\qquad df_2 = N - p_F - 1 = N - 8\n\\]\n\n### 3) Noncentrality parameter under the alternative\nFor the partial \\(F\\) test, the noncentrality parameter is approximately\n\n\\[\n\\lambda = f^2 \\cdot df_2 = 0.142857\\,(N-8)\n\\]\n\nWe need \\(N\\) such that\n\n\\[\n\\Pr\\left(F_{(3,\\;N-8,\\;\\lambda)} > F_{0.95;\\;3,\\;N-8}\\right)=0.80\n\\]\n\nUsing a standard large-\\(df_2\\) approximation to the noncentral \\(F\\) (via a noncentral \\(\\chi^2\\) approximation for the numerator), the required noncentrality is about\n\n\\[\n\\lambda \\approx 11.2\n\\]\n\nSo,\n\n\\[\n0.142857\\,(N-8) \\approx 11.2\n\\]\n\\[\nN-8 \\approx \\frac{11.2}{0.142857}=78.4\n\\]\n\\[\nN \\approx 86.4\n\\]\n\nRound up:\n\n---\n\n## **Final answer: \\(\\boxed{N = 87\\text{ subjects}}\\)**\n\n(That is, about 87 subjects to detect an increase from \\(R^2=0.20\\) to \\(R^2=0.30\\) when adding 3 predictors, with 80% power at \\(\\alpha=0.05\\).)",
    "latency_ms": 160312,
    "tokens": {
      "input": 131,
      "output": 4156,
      "total": 4287
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:26:41.429Z"
  },
  "t2-linreg-004": {
    "task_id": "t2-linreg-004",
    "tier": 2,
    "question": "We're building a prediction model with 10 predictors. We want to detect a medium effect size (Cohen's f² = 0.15) with 85% power at a more stringent alpha = 0.01. What sample size do we need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assuming you are powering the **overall multiple linear regression F‑test** (i.e., \\(H_0:R^2=0\\)) with **\\(u=10\\)** predictors:\n\n### Step 1) Convert effect size\n\\[\nf^2=0.15 \\quad\\Rightarrow\\quad R^2=\\frac{f^2}{1+f^2}=\\frac{0.15}{1.15}=0.1304\n\\]\n\n### Step 2) Regression F-test degrees of freedom and noncentrality\n\\[\ndf_1=u=10,\\qquad df_2=v=N-u-1=N-11\n\\]\nFor this test, the noncentrality parameter is\n\\[\n\\lambda=f^2\\cdot v = 0.15(N-11)\n\\]\n\n### Step 3) Find \\(N\\) giving 85% power at \\(\\alpha=0.01\\)\nWe need\n\\[\nP\\left(F_{(10,\\;N-11,\\;\\lambda)} > F_{0.99;\\;10,\\;N-11}\\right)=0.85\n\\]\nSolving (by standard noncentral F power calculation; here approximated with large-\\(df_2\\) noncentral-\\(\\chi^2\\) math and then checked for \\(df_2\\approx 170\\)) gives approximately:\n\\[\nN \\approx 185\n\\]\nCheck at \\(N=185\\):\n\\[\nv=185-11=174,\\quad \\lambda=0.15(174)=26.1\n\\]\nwhich yields power ≈ 0.85 at \\(\\alpha=0.01\\).\n\n---\n\n## **Required sample size: \\(\\boxed{N=185}\\)**",
    "latency_ms": 163515,
    "tokens": {
      "input": 115,
      "output": 6415,
      "total": 6530
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:29:26.447Z"
  },
  "t2-linreg-005": {
    "task_id": "t2-linreg-005",
    "tier": 2,
    "question": "In my regression model with 8 predictors, I want to test whether one specific predictor contributes meaningfully. I expect this predictor to explain an additional 5% of variance (partial f² ≈ 0.053). How many subjects for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To test **one specific predictor** in a multiple regression with **8 total predictors**, the usual test is the **incremental \\(R^2\\)** (equivalently the \\(t\\)-test for that coefficient), which is an **\\(F\\)** test with:\n\n- \\(u = 1\\) tested predictor  \\(\\Rightarrow df_1 = 1\\)\n- total predictors in full model \\(k = 8\\)\n- \\(df_2 = N - k - 1 = N - 9\\)\n\nYou gave **partial \\(f^2 \\approx 0.053\\)** (i.e., the predictor adds ~5% partial variance).\n\n### Noncentrality for the \\(F\\) test\nFor an \\(R^2\\) increase test, the noncentrality parameter is:\n\\[\n\\lambda = f^2 \\cdot df_2 = 0.053\\,(N-9)\n\\]\n\n### Power requirement\nWe need \\(N\\) such that:\n\\[\n\\Pr\\left(F_{(1,\\,N-9,\\;\\lambda)} > F_{0.95;\\,1,\\,N-9}\\right)=0.90\n\\]\n\n### Check \\(N=209\\)\n- \\(df_2 = 209 - 9 = 200\\)\n- \\(\\lambda = 0.053 \\times 200 = 10.6\\)\n- \\(F_{0.95;1,200}\\approx 3.89\\) (since \\(F_{1,df} = t_{0.975,df}^2\\))\n\nWith \\(df_2=200\\) and \\(\\lambda=10.6\\), the resulting power is approximately **0.90** (and \\(N=208\\) is just under 0.90).\n\n---\n\n## Final answer (rounded up)\n**You need \\(N = 209\\) subjects** for **90% power** at **\\(\\alpha=0.05\\)** to detect **partial \\(f^2=0.053\\)** for one predictor in a model with **8 predictors**.",
    "latency_ms": 129941,
    "tokens": {
      "input": 123,
      "output": 2573,
      "total": 2696
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:31:37.893Z"
  },
  "t2-linreg-006": {
    "task_id": "t2-linreg-006",
    "tier": 2,
    "question": "We're conducting a large epidemiological study with 15 risk factors as predictors. We expect only a small effect (R² around 5%) since these are observational data. What sample size do we need for 95% power with alpha = 0.01?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assuming you want **95% power to detect an overall model \\(R^2=0.05\\)** (omnibus \\(F\\)-test that all 15 slopes are 0) in a multiple linear regression with **\\(u=15\\)** predictors at **\\(\\alpha=0.01\\)**.\n\n### 1) Convert \\(R^2\\) to Cohen’s \\(f^2\\)\n\\[\nf^2=\\frac{R^2}{1-R^2}=\\frac{0.05}{0.95}=0.05263\n\\]\n\n### 2) Regression \\(F\\)-test setup\n- Numerator df: \\(u = 15\\)  \n- Denominator df: \\(v = N-u-1 = N-16\\)\n- Noncentrality parameter:\n\\[\n\\lambda = f^2\\,(N-u-1)=0.05263\\,(N-16)\n\\]\n\nWe need \\(N\\) such that:\n\\[\n\\Pr\\left(F_{u,v,\\lambda} > F_{1-\\alpha;u,v}\\right)=0.95\n\\]\n\nFor large \\(v\\), \\(F_{1-\\alpha;15,v}\\) is close to \\(\\approx 2.06\\) at \\(\\alpha=0.01\\), so the critical threshold on the equivalent chi-square scale is approximately:\n\\[\nc \\approx u \\cdot F_{\\text{crit}} \\approx 15 \\cdot 2.06 \\approx 30.9\n\\]\n\nUsing a standard normal approximation to the noncentral \\(\\chi^2\\) to achieve 95% power at that threshold gives a required noncentrality of about:\n\\[\n\\lambda \\approx 38.1\n\\]\n\n### 3) Solve for \\(N\\)\n\\[\n0.05263\\,(N-16)=38.1\n\\]\n\\[\nN-16=\\frac{38.1}{0.05263}\\approx 724.0\n\\]\n\\[\nN\\approx 740.0 \\;\\Rightarrow\\; \\text{round up} \\Rightarrow 741\n\\]\n\n---\n\n## **Final answer (rounded up): \\(\\boxed{N \\approx 741}\\)**\n\nThis is the **total sample size** needed (before inflating for missing data, measurement error, clustering/design effects, etc.).",
    "latency_ms": 257252,
    "tokens": {
      "input": 115,
      "output": 4487,
      "total": 4602
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:35:56.650Z"
  },
  "t2-logreg-001": {
    "task_id": "t2-logreg-001",
    "tier": 2,
    "question": "I'm analyzing whether a binary exposure (present in 50% of patients) predicts disease. The outcome rate is 20% in unexposed patients, and I expect an odds ratio of 2.0 for exposed vs unexposed. How many patients total for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Convert the expected odds ratio to an event rate in the exposed group\n\nGiven unexposed risk \\(p_0 = 0.20\\) and expected odds ratio \\(OR = 2.0\\):\n\n- Unexposed odds:  \n\\[\n\\text{odds}_0=\\frac{p_0}{1-p_0}=\\frac{0.20}{0.80}=0.25\n\\]\n- Exposed odds (multiply by OR):  \n\\[\n\\text{odds}_1 = OR \\cdot \\text{odds}_0 = 2.0 \\cdot 0.25 = 0.50\n\\]\n- Convert back to probability:  \n\\[\np_1=\\frac{\\text{odds}_1}{1+\\text{odds}_1}=\\frac{0.50}{1.50}=0.3333\n\\]\n\nSo we are comparing **20% vs 33.33%** event rates, with exposure prevalence 50% (equal group sizes).\n\n---\n\n### Step 2: Two-sample proportions sample size (two-sided \\(\\alpha=0.05\\), power 80%)\n\nLet \\(z_{1-\\alpha/2}=1.96\\), \\(z_{1-\\beta}=0.8416\\). With equal groups:\n\n\\[\n\\bar p=\\frac{p_0+p_1}{2}=\\frac{0.20+0.3333}{2}=0.2667\n\\]\n\nCompute components:\n\n\\[\n\\sqrt{2\\bar p(1-\\bar p)}=\\sqrt{2(0.2667)(0.7333)}=\\sqrt{0.3911}=0.6255\n\\]\n\n\\[\n\\sqrt{p_0(1-p_0)+p_1(1-p_1)}=\\sqrt{0.20(0.80)+0.3333(0.6667)}\n=\\sqrt{0.16+0.2222}=\\sqrt{0.3822}=0.6183\n\\]\n\nDifference:\n\\[\n\\Delta = p_1-p_0=0.3333-0.20=0.1333\n\\]\n\nSample size per group:\n\\[\nn=\\frac{\\left(1.96(0.6255)+0.8416(0.6183)\\right)^2}{(0.1333)^2}\n=\\frac{(1.226+0.520)^2}{0.01778}\n=\\frac{3.047}{0.01778}=171.4\n\\]\n\nRound up: \\(n = 172\\) per group.\n\n---\n\n## Final answer (rounded up)\n**Total sample size needed = \\(172 \\times 2 = \\mathbf{344}\\) patients** (about **172 exposed** and **172 unexposed**) for 80% power at \\(\\alpha=0.05\\).",
    "latency_ms": 31210,
    "tokens": {
      "input": 128,
      "output": 1129,
      "total": 1257
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:36:29.368Z"
  },
  "t2-logreg-002": {
    "task_id": "t2-logreg-002",
    "tier": 2,
    "question": "In our case-control study, I want to detect whether a continuous exposure is associated with disease. I expect an odds ratio of 1.5 per standard deviation increase in the exposure. The baseline disease risk is about 10%. How many subjects for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Use a 1-predictor logistic regression sample size approximation (Hsieh et al.) for testing the slope \\( \\beta \\) (Wald test), with the exposure standardized (\\(\\mathrm{Var}(X)=1\\)) and no other covariates (\\(R^2=0\\)):\n\n**Given**\n- Target OR per 1 SD: \\(\\text{OR}=1.5 \\Rightarrow \\beta=\\ln(1.5)=0.4055\\)\n- Two-sided \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha/2}=1.96\\)\n- Power \\(=0.80 \\Rightarrow z_{1-\\beta}=0.842\\)\n- Case-control with **1:1** sampling \\(\\Rightarrow\\) fraction of cases in the *sample* \\(p=0.5\\)\n\n**Approximate required total \\(n\\):**\n\\[\nn \\approx \\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{p(1-p)\\,\\beta^2\\,\\mathrm{Var}(X)}\n= \\frac{(1.96+0.842)^2}{(0.5)(0.5)\\,(0.4055)^2\\,(1)}\n\\]\n\\[\n(1.96+0.842)^2 = 2.802^2 = 7.85,\\quad\np(1-p)\\beta^2 = 0.25\\times 0.1644=0.0411\n\\]\n\\[\nn \\approx \\frac{7.85}{0.0411}=191.2 \\Rightarrow \\boxed{192}\n\\]\n\n### Final answer (rounded up)\n\\[\n\\boxed{\\textbf{Total } n=192 \\textbf{ subjects (about 96 cases + 96 controls)}}\n\\]\n\n**Using the 10% baseline disease risk:** to *obtain* ~96 cases from a source population with 10% disease risk, you’d need to screen about \\(96/0.10 \\approx \\boxed{960}\\) people (then sample 96 controls from the non-cases).",
    "latency_ms": 144217,
    "tokens": {
      "input": 124,
      "output": 2176,
      "total": 2300
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:38:55.089Z"
  },
  "t2-logreg-003": {
    "task_id": "t2-logreg-003",
    "tier": 2,
    "question": "I'm building a multivariable logistic regression to predict readmission risk with 5 covariates. My main predictor is a continuous biomarker (normally distributed). I expect an odds ratio of 1.8 per SD increase in this biomarker, while adjusting for confounders that together explain about 10% of the variance in the predictor. Baseline readmission rate is 15%. How many patients for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using the Hsieh et al. (1998) large-sample approximation for testing a **continuous** predictor in logistic regression, with adjustment for other covariates via the **partial variance** of the predictor:\n\n### Inputs\n- Target power: \\(1-\\beta = 0.80 \\Rightarrow z_{1-\\beta}=0.8416\\)\n- Two-sided \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha/2}=1.9600\\)\n- Baseline event rate (at mean biomarker): \\(p_0=0.15\\)\n- Odds ratio per 1 SD: \\(\\text{OR}=1.8 \\Rightarrow \\beta=\\log(1.8)=0.5878\\)\n- Biomarker standardized: \\(\\mathrm{Var}(X)=1\\)\n- Confounders explain \\(R^2=0.10\\) of biomarker variance  \n  \\(\\Rightarrow \\mathrm{Var}(X\\mid \\text{confounders}) = 1-R^2 = 0.90\\)\n\n### Sample size formula\n\\[\nn \\approx \\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{\\beta^2 \\; p_0(1-p_0)\\; (1-R^2)}\n\\]\n\n### Compute\n1) Numerator:\n\\[\n(z_{1-\\alpha/2}+z_{1-\\beta})^2=(1.96+0.8416)^2=(2.8016)^2=7.8489\n\\]\n\n2) Denominator:\n\\[\n\\beta^2 p_0(1-p_0)(1-R^2)\n= (0.5878)^2 \\times (0.15)(0.85)\\times 0.90\n\\]\n\\[\n=0.3455\\times 0.1275\\times 0.90=0.03964\n\\]\n\n3) Sample size:\n\\[\nn=\\frac{7.8489}{0.03964}=197.98\n\\]\n\n---\n\n## Final answer (rounded up)\n\\[\n\\boxed{n = 198\\text{ patients total}}\n\\]\n\n(At 15% readmission, this corresponds to \\(\\approx 0.15\\times 198 \\approx 30\\) readmissions.)",
    "latency_ms": 122345,
    "tokens": {
      "input": 157,
      "output": 1873,
      "total": 2030
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:40:58.938Z"
  },
  "t2-logreg-004": {
    "task_id": "t2-logreg-004",
    "tier": 2,
    "question": "I'm studying a protective factor with an expected odds ratio of 0.6 (40% reduction in odds). The exposure is binary with 50% prevalence, and baseline disease rate in unexposed is 25%. What sample size do I need for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Given:\n\n- Target odds ratio (protective): \\(\\text{OR}=0.6\\)  \n- Baseline disease risk in unexposed: \\(p_0=0.25\\)  \n- Exposure prevalence: 50% \\(\\Rightarrow\\) equal group sizes  \n- Two-sided \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha/2}=1.96\\)  \n- Power \\(=90\\% \\Rightarrow z_{1-\\beta}=1.2816\\)\n\n## 1) Convert OR to expected risk in exposed (\\(p_1\\))\n\n\\[\n\\text{odds}_0=\\frac{p_0}{1-p_0}=\\frac{0.25}{0.75}=0.3333\n\\]\n\\[\n\\text{odds}_1=\\text{OR}\\cdot \\text{odds}_0 = 0.6 \\times 0.3333 = 0.2\n\\]\n\\[\np_1=\\frac{\\text{odds}_1}{1+\\text{odds}_1}=\\frac{0.2}{1.2}=0.1667\n\\]\n\nSo we are comparing \\(p_0=0.25\\) vs \\(p_1\\approx 0.1667\\).\n\n## 2) Two-sample proportions sample size (equal \\(n\\) per group)\n\nLet \\(\\bar p=(p_0+p_1)/2=(0.25+0.1667)/2=0.2083\\).\n\n\\[\nn_{\\text{per group}}=\n\\frac{\\left[z_{1-\\alpha/2}\\sqrt{2\\bar p(1-\\bar p)} \\;+\\; z_{1-\\beta}\\sqrt{p_0(1-p_0)+p_1(1-p_1)}\\right]^2}{(p_0-p_1)^2}\n\\]\n\nCompute terms:\n\n- \\(\\sqrt{2\\bar p(1-\\bar p)}=\\sqrt{2(0.2083)(0.7917)}=\\sqrt{0.3299}=0.5744\\)\n- \\(\\sqrt{p_0(1-p_0)+p_1(1-p_1)}=\\sqrt{0.25(0.75)+0.1667(0.8333)}=\\sqrt{0.3264}=0.5714\\)\n- Difference: \\(p_0-p_1=0.08333\\)\n\nNumerator:\n\n\\[\n(1.96\\cdot 0.5744 + 1.2816\\cdot 0.5714)^2\n= (1.1258 + 0.7323)^2\n= (1.8581)^2\n= 3.4526\n\\]\n\nDenominator:\n\n\\[\n(0.08333)^2=0.006944\n\\]\n\n\\[\nn_{\\text{per group}}=\\frac{3.4526}{0.006944}=497.2 \\Rightarrow 498\n\\]\n\n## Final answer (rounded up)\n\n**Required sample size:**\n- **498 exposed + 498 unexposed = 996 total participants** (for 90% power, \\(\\alpha=0.05\\), OR=0.6, exposure prevalence 50%, \\(p_0=0.25\\)).",
    "latency_ms": 39154,
    "tokens": {
      "input": 125,
      "output": 1256,
      "total": 1381
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:41:39.595Z"
  },
  "t2-logreg-005": {
    "task_id": "t2-logreg-005",
    "tier": 2,
    "question": "We're studying a rare outcome (5% baseline risk) and a binary exposure that's present in only 30% of the population. We expect an OR of 2.5 for exposed vs unexposed. What total sample size is needed for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume the **5% baseline risk is in the unexposed** group ( \\(p_0=0.05\\) ), exposure prevalence is 30% so the allocation ratio is\n\\[\nr=\\frac{n_1}{n_0}=\\frac{0.30}{0.70}=0.4286\n\\]\nwhere \\(n_1\\)=exposed, \\(n_0\\)=unexposed.\n\n## 1) Convert OR to risk in the exposed\nGiven \\(OR=2.5\\),\n\\[\np_1=\\frac{OR\\cdot p_0}{(1-p_0)+OR\\cdot p_0}\n=\\frac{2.5\\cdot 0.05}{0.95+2.5\\cdot 0.05}\n=\\frac{0.125}{1.075}=0.1163\n\\]\nRisk difference:\n\\[\nd=p_1-p_0=0.1163-0.05=0.0663\n\\]\n\n## 2) Two-sample proportions sample size (unequal group sizes)\nUse the standard normal-approximation formula for a two-sided \\(\\alpha=0.05\\) test with power 80%:\n\nLet\n\\[\n\\bar p=\\frac{p_0+r p_1}{1+r}=\\frac{0.05+0.4286(0.1163)}{1.4286}=0.0699\n\\]\nCritical values: \\(z_{0.975}=1.96,\\; z_{0.80}=0.842\\).\n\nCompute:\n\\[\nn_0=\\frac{\\left[z_{0.975}\\sqrt{\\bar p(1-\\bar p)\\left(1+\\frac{1}{r}\\right)}\\;+\\;z_{0.80}\\sqrt{p_0(1-p_0)+\\frac{p_1(1-p_1)}{r}}\\right]^2}{(p_1-p_0)^2}\n\\]\n\nNumbers:\n- \\(\\bar p(1-\\bar p)=0.0699(0.9301)=0.0650\\)\n- \\(1+1/r = 1+2.3333=3.3333\\)\n\nSo first term:\n\\[\n1.96\\sqrt{0.0650\\cdot 3.3333}=1.96(0.4653)=0.912\n\\]\n\nSecond term components:\n- \\(p_0(1-p_0)=0.05(0.95)=0.0475\\)\n- \\(p_1(1-p_1)=0.1163(0.8837)=0.1028\\)\n- \\(p_1(1-p_1)/r = 0.1028/0.4286=0.2398\\)\n\nSo second term:\n\\[\n0.842\\sqrt{0.0475+0.2398}=0.842\\sqrt{0.2873}=0.842(0.5360)=0.451\n\\]\n\nSum \\(=0.912+0.451=1.363\\). Square \\(=1.858\\).\n\nDenominator: \\(d^2=(0.0663)^2=0.004393\\).\n\n\\[\nn_0=\\frac{1.858}{0.004393}=423.0\n\\]\n\\[\nn_1=r n_0 = 0.4286(423.0)=181.3\n\\]\n\nTotal:\n\\[\nN=n_0+n_1=423.0+181.3=604.3 \\Rightarrow \\boxed{605}\n\\]\n\n## Final answer (rounded up)\n**Total sample size needed: \\(\\boxed{N=605}\\)**  \n(approximately **423 unexposed** and **182 exposed** to match 70%/30% exposure prevalence).",
    "latency_ms": 111942,
    "tokens": {
      "input": 123,
      "output": 3190,
      "total": 3313
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:43:33.040Z"
  },
  "t2-logreg-006": {
    "task_id": "t2-logreg-006",
    "tier": 2,
    "question": "I'm testing for an interaction between two binary risk factors in a logistic regression. Each factor has 50% prevalence and they're independent. Baseline risk (neither factor present) is 20%, main effect OR for each is 1.5, and the interaction OR is 2.0. What sample size do I need to detect this interaction with 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### 1) Translate the ORs into cell risks\n\nLet \\(X_1,X_2\\in\\{0,1\\}\\) with \\(P(X_1=1)=P(X_2=1)=0.5\\) and independence \\(\\Rightarrow\\) each of the 4 cells has probability \\(\\pi_{ij}=0.25\\).\n\nBaseline risk (00):  \n\\[\np_{00}=0.20,\\quad \\text{odds}_{00}=\\frac{0.20}{0.80}=0.25\n\\]\n\nMain-effect OR for each factor: \\(1.5\\). Interaction OR: \\(2.0\\).  \nThus:\n\\[\n\\text{odds}_{10}=0.25(1.5)=0.375 \\Rightarrow p_{10}=\\frac{0.375}{1+0.375}=0.272727\n\\]\n\\[\n\\text{odds}_{01}=0.375 \\Rightarrow p_{01}=0.272727\n\\]\n\\[\n\\text{odds}_{11}=0.25(1.5)(1.5)(2.0)=1.125 \\Rightarrow p_{11}=\\frac{1.125}{1+1.125}=0.529412\n\\]\n\nThe interaction coefficient is\n\\[\n\\beta_3=\\log(2)=0.693147\n\\]\n\n---\n\n### 2) Fisher information for \\(\\beta_3\\) (expected, under the alternative)\n\nFor logistic regression, each cell contributes weight \\(w=p(1-p)\\). Compute:\n\n- \\(w_{00}=0.20(0.80)=0.16\\)\n- \\(w_{10}=w_{01}=0.272727(0.727273)=0.198347\\)\n- \\(w_{11}=0.529412(0.470588)=0.249135\\)\n\nPer-subject expected information is\n\\[\nM=\\sum_{cells}\\pi_{cell}\\, w_{cell}\\, x x^\\top,\\quad x=(1,X_1,X_2,X_1X_2)\n\\]\n\nCarrying this out gives (per subject):\n\\[\nM=\n\\begin{pmatrix}\n0.2014573 & 0.1118705 & 0.1118705 & 0.0622837\\\\\n0.1118705 & 0.1118705 & 0.0622837 & 0.0622837\\\\\n0.1118705 & 0.0622837 & 0.1118705 & 0.0622837\\\\\n0.0622837 & 0.0622837 & 0.0622837 & 0.0622837\n\\end{pmatrix}\n\\]\n\nWith \\(n\\) subjects: \\(I = nM\\), so\n\\[\n\\mathrm{Var}(\\hat\\beta_3)=\\frac{(M^{-1})_{44}}{n}\n\\]\n\nUsing a Schur complement calculation for \\((M^{-1})_{44}\\):\n\\[\n(M^{-1})_{44} \\approx 81.387\n\\]\n\nSo\n\\[\n\\mathrm{SE}(\\hat\\beta_3)=\\sqrt{\\frac{81.387}{n}}\n\\]\n\n---\n\n### 3) Power requirement for a 2-sided Wald test of the interaction\n\nFor \\(\\alpha=0.05\\) (two-sided): \\(z_{1-\\alpha/2}=1.959964\\).  \nFor 80% power: \\(z_{0.80}=0.841621\\).\n\nRequired noncentrality:\n\\[\n\\frac{|\\beta_3|}{\\mathrm{SE}} \\ge z_{1-\\alpha/2}+z_{0.80}=2.801585\n\\]\n\nSo the target variance is:\n\\[\n\\mathrm{SE}\\le \\frac{0.693147}{2.801585}=0.24742\n\\Rightarrow \\mathrm{Var}\\le 0.061216\n\\]\n\nSet \\(\\frac{81.387}{n}\\le 0.061216\\):\n\\[\nn \\ge \\frac{81.387}{0.061216}=1329.5\n\\]\n\n---\n\n## **Final answer (rounded up): \\(\\boxed{n = 1330}\\) total subjects**",
    "latency_ms": 213415,
    "tokens": {
      "input": 442,
      "output": 5601,
      "total": 6043
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:47:07.958Z"
  },
  "t2-mixed-001": {
    "task_id": "t2-mixed-001",
    "tier": 2,
    "question": "I'm planning a longitudinal study with 4 repeated measures per subject, comparing treatment vs control. I expect a medium effect size (d = 0.5) between groups, and the ICC for within-subject correlation is about 0.5. How many subjects per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a continuous outcome analyzed for an overall **between-group difference** using all 4 repeated measures, with **compound symmetry** (constant within-subject correlation/ICC = 0.5). A convenient approximation is to treat the 4 measures as a “within-subject cluster” of size \\(m=4\\), which increases precision by the factor\n\\[\n\\text{information multiplier}=\\frac{m}{1+(m-1)\\rho}.\n\\]\n\n### 1) Start with the usual 2-sample size (independent observations)\nFor a two-sided \\(\\alpha=0.05\\) and 80% power:\n- \\(z_{1-\\alpha/2}=1.96\\)\n- \\(z_{1-\\beta}=0.842\\)\n\nWith Cohen’s \\(d=0.5\\),\n\\[\nn_{\\text{ind}} \\;=\\; 2\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{d}\\right)^2\n=2\\left(\\frac{1.96+0.842}{0.5}\\right)^2\n=2(5.604)^2\n=62.8 \\approx 63 \\text{ per group.}\n\\]\n\n### 2) Adjust for 4 repeated measures with ICC = 0.5\nDesign effect:\n\\[\nDE = 1+(m-1)\\rho = 1+3(0.5)=2.5.\n\\]\nInformation multiplier:\n\\[\n\\frac{m}{DE} = \\frac{4}{2.5}=1.6.\n\\]\n\nSo required sample size per group is reduced by this factor:\n\\[\nn_{\\text{RM}} = \\frac{n_{\\text{ind}}}{1.6} = \\frac{62.8}{1.6}=39.25 \\;\\Rightarrow\\; 40.\n\\]\n\n## Final answer (rounded up)\n**You need \\(\\boxed{40}\\) subjects per group (total \\(\\boxed{80}\\) subjects).**\n\n(If your primary hypothesis is a *group × time interaction* rather than an overall group difference, the required \\(n\\) can be larger and depends on the expected time trend and covariance structure.)",
    "latency_ms": 34125,
    "tokens": {
      "input": 130,
      "output": 1390,
      "total": 1520
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:47:43.587Z"
  },
  "t2-mixed-002": {
    "task_id": "t2-mixed-002",
    "tier": 2,
    "question": "We're doing a simple pre-post study comparing treatment vs control, with measurements at baseline and 8 weeks. We expect a treatment effect of d = 0.4, and within-patient correlation is about ICC = 0.6. How many subjects per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a **two-sided** test at \\(\\alpha=0.05\\), **80% power**, equal allocation, and the primary analysis is the **between-group difference in change from baseline** (difference-in-differences). Let:\n\n- Expected standardized treatment effect (in SD units at a single time point): \\(d = 0.4\\)\n- Within-subject correlation: \\(r = 0.6\\)\n- \\(z_{1-\\alpha/2} = 1.96\\), \\(z_{1-\\beta}=0.84\\) so \\(z_{1-\\alpha/2}+z_{1-\\beta}=2.80\\)\n\nFor change scores, the variance inflation/deflation due to correlation is:\n\\[\n\\mathrm{Var}(Y_1-Y_0)=2\\sigma^2(1-r)\n\\]\nThe required per-group sample size for comparing mean changes is:\n\\[\nn \\;=\\; \\frac{4(1-r)\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{d^2}\n\\]\n\nPlug in:\n\\[\nn = \\frac{4(1-0.6)\\,(2.80)^2}{0.4^2}\n= \\frac{4(0.4)\\,(7.84)}{0.16}\n= \\frac{12.544}{0.16}\n= 78.4\n\\]\n\nRound up:\n\n## **Final answer: 79 subjects per group** (158 total)\n\n*(If instead you plan an ANCOVA adjusting for baseline, the required \\(n\\) is smaller: \\(n \\approx 63\\) per group using the \\((1-r^2)\\) variance reduction.)*",
    "latency_ms": 67104,
    "tokens": {
      "input": 128,
      "output": 2184,
      "total": 2312
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:48:52.194Z"
  },
  "t2-mixed-003": {
    "task_id": "t2-mixed-003",
    "tier": 2,
    "question": "We're running a clinical trial with monthly visits over 6 months to detect a treatment effect of d = 0.35. The within-patient correlation (compound symmetry) is about ICC = 0.4. How many subjects per arm for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a continuous outcome analyzed with a repeated-measures model where the treatment effect corresponds to a standardized mean difference on a *single* visit of \\(d=0.35\\), with \\(m=6\\) monthly measurements per subject and compound-symmetry correlation \\(\\rho=\\text{ICC}=0.4\\). Two-sided \\(\\alpha=0.05\\), power \\(1-\\beta=0.90\\).\n\n## 1) Required \\(n\\) per arm with a single measurement\nFor a two-sample comparison with standardized effect size \\(d\\):\n\\[\nn_{\\text{single}}=\\frac{2\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{d^2}\n\\]\n\\[\nz_{1-\\alpha/2}=1.96,\\quad z_{1-\\beta}=z_{0.90}=1.2816\n\\]\n\\[\nn_{\\text{single}}=\\frac{2\\,(1.96+1.2816)^2}{0.35^2}\n=\\frac{2\\,(3.2416)^2}{0.1225}\n=\\frac{2\\cdot 10.512}{0.1225}\n=171.6\n\\]\nSo about **172 per arm** if you only had one post-treatment measurement.\n\n## 2) Repeated-measures gain under compound symmetry\nFor \\(m\\) repeated measures with correlation \\(\\rho\\), the variance of the subject mean is reduced by:\n\\[\n\\text{DE}=\\frac{1+(m-1)\\rho}{m}\n\\]\nSo the required sample size is:\n\\[\nn_{\\text{RM}} = n_{\\text{single}}\\times \\text{DE}\n\\]\nWith \\(m=6\\), \\(\\rho=0.4\\):\n\\[\n\\text{DE}=\\frac{1+5(0.4)}{6}=\\frac{1+2}{6}=\\frac{3}{6}=0.5\n\\]\n\\[\nn_{\\text{RM}}=171.6\\times 0.5=85.8\n\\]\n\n## Final answer (rounded up)\n**\\(\\boxed{86\\text{ subjects per arm}}\\)**\n\n(If your “monthly over 6 months” includes baseline plus 6 follow-ups, i.e., \\(m=7\\), the same calculation gives \\(\\approx 84\\) per arm.)",
    "latency_ms": 45159,
    "tokens": {
      "input": 123,
      "output": 1110,
      "total": 1233
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:49:38.858Z"
  },
  "t2-mixed-004": {
    "task_id": "t2-mixed-004",
    "tier": 2,
    "question": "Our trial has patients nested within 10 clinics per treatment arm. The treatment effect is d = 0.3 and the ICC between patients within clinics is 0.05. How many patients per clinic do we need for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "We have a 2-arm **clustered** design with:\n\n- **Clusters (clinics) per arm:** \\(k = 10\\)  \n- **Patients per clinic:** \\(m\\) (unknown)  \n- **ICC:** \\(\\rho = 0.05\\)  \n- **Standardized effect (Cohen’s d):** \\(d = 0.3\\)  \n- **Power:** 0.80 \\(\\Rightarrow z_{1-\\beta}=0.84\\)  \n- **Two-sided \\(\\alpha=0.05\\) \\(\\Rightarrow z_{1-\\alpha/2}=1.96\\)\n\n### 1) Individual-level sample size needed per arm (ignoring clustering)\nFor a two-sample comparison of means with standardized effect \\(d\\):\n\n\\[\nn_{\\text{ind}} = \\frac{2\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{d^2}\n= \\frac{2\\,(1.96+0.84)^2}{0.3^2}\n= \\frac{2\\,(2.80)^2}{0.09}\n= \\frac{2\\cdot 7.84}{0.09}\n= \\frac{15.68}{0.09}\n= 174.22\n\\]\n\nSo we need an **effective** sample size of about **174.22 per arm**.\n\n### 2) Adjust for clustering via the design effect\nDesign effect:\n\n\\[\nDE = 1+(m-1)\\rho = 1+(m-1)0.05 = 0.95+0.05m\n\\]\n\nEffective sample size per arm with \\(k\\) clusters:\n\n\\[\nn_{\\text{eff}} = \\frac{k m}{DE} = \\frac{10m}{0.95+0.05m}\n\\]\n\nSet this equal to the required effective sample size:\n\n\\[\n\\frac{10m}{0.95+0.05m} = 174.22\n\\]\n\nSolve:\n\n\\[\n10m = 174.22(0.95+0.05m)=165.509 + 8.711m\n\\]\n\\[\n10m-8.711m = 165.509\n\\]\n\\[\n1.289m = 165.509\n\\]\n\\[\nm = 128.43\n\\]\n\nRound up:\n\n\\[\nm = 129\n\\]\n\n---\n\n## Final answer (rounded up)\n**You need 129 patients per clinic** (with 10 clinics per arm) to achieve **80% power at \\(\\alpha=0.05\\)** given **\\(d=0.3\\)** and **ICC = 0.05**.\n\n(That is **1290 patients per arm**, **2580 total**.)",
    "latency_ms": 151751,
    "tokens": {
      "input": 119,
      "output": 1498,
      "total": 1617
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:52:12.112Z"
  },
  "t2-mixed-005": {
    "task_id": "t2-mixed-005",
    "tier": 2,
    "question": "We have a three-level study design: 3 measurements within patients, 15 patients within sites, and 5 sites per treatment arm. The variance is partitioned as: site variance = 0.02, patient variance = 0.50, residual = 0.48 (total = 1.0). We want to detect a treatment effect of d = 0.4. What power does this design achieve at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Work out the variance of a **site mean** (since treatment is assigned at the site level)\n\nModel implied by the variance partitioning:\n\n\\[\nY_{spm}=\\mu + \\text{(site)}_s + \\text{(patient)}_{p(s)} + \\varepsilon_{spm}\n\\]\n\nGiven:\n- 3 measurements per patient (\\(n_m=3\\))\n- 15 patients per site (\\(n_p=15\\))\n- Variance components:  \n  \\(\\sigma^2_{\\text{site}}=0.02,\\;\\sigma^2_{\\text{patient}}=0.50,\\;\\sigma^2_{\\text{res}}=0.48\\)\n\nA site mean averages over all \\(15\\times 3=45\\) observations. Its variance is:\n\n\\[\n\\mathrm{Var}(\\bar Y_{\\text{site}})=\n\\sigma^2_{\\text{site}}\n+\\frac{\\sigma^2_{\\text{patient}}}{n_p}\n+\\frac{\\sigma^2_{\\text{res}}}{n_p n_m}\n\\]\n\n\\[\n=0.02+\\frac{0.50}{15}+\\frac{0.48}{45}\n=0.02+0.033333+0.010667\n=0.064\n\\]\n\nSo:\n\n\\[\nSD(\\bar Y_{\\text{site}})=\\sqrt{0.064}=0.253\n\\]\n\n---\n\n### Step 2: Standard error for the treatment contrast (5 sites per arm)\n\nThere are \\(m=5\\) sites/arm, so the variance of an arm mean is \\(0.064/5\\). The variance of the difference in arm means is:\n\n\\[\n\\mathrm{Var}(\\bar Y_T-\\bar Y_C)=\\frac{0.064}{5}+\\frac{0.064}{5}=\\frac{2(0.064)}{5}=0.0256\n\\]\n\n\\[\nSE=\\sqrt{0.0256}=0.16\n\\]\n\n---\n\n### Step 3: Noncentrality parameter for the treatment effect \\(d=0.4\\)\n\nTreatment mean difference to detect: \\(\\Delta = 0.4\\) (since total variance was standardized to 1.0)\n\n\\[\nt_{\\text{alt}}=\\frac{\\Delta}{SE}=\\frac{0.4}{0.16}=2.5\n\\]\n\nDegrees of freedom for a site-level two-sample t-test:\n\n\\[\ndf = (5+5-2)=8\n\\]\n\nCritical value (two-sided \\(\\alpha=0.05\\)):\n\n\\[\nt_{0.975,8}\\approx 2.306\n\\]\n\nPower:\n\n\\[\n\\text{Power} = P\\left(|T|>2.306\\right),\\quad T\\sim t_{df=8}(\\text{ncp}=2.5)\n\\]\n\nNumerically this is approximately:\n\n\\[\n\\textbf{Power} \\approx 0.59\n\\]\n\n---\n\n## **Final answer (prominently):**\n\\[\n\\boxed{\\textbf{Power} \\approx 0.59 \\text{ (about 59%) at } \\alpha=0.05}\n\\]",
    "latency_ms": 187374,
    "tokens": {
      "input": 156,
      "output": 13002,
      "total": 13158
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:55:20.991Z"
  },
  "t2-mixed-006": {
    "task_id": "t2-mixed-006",
    "tier": 2,
    "question": "I'm analyzing a growth curve study comparing two groups over 8 time points (0-7). I expect the groups to differ in their slopes by 0.1 units per time point. The random intercept SD is 1.0, random slope SD is 0.3, and residual SD is 1.0. What's the minimum sample size per group to achieve 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Model/test setup (fixed *group × time* interaction)\n- 2 groups, balanced with \\(n\\) subjects per group  \n- 8 time points: \\(t=0,1,\\dots,7\\)\n- Effect of interest (difference in slopes): \\(\\beta_{GT}=0.1\\) per time unit\n- Random intercept SD \\(=1.0 \\Rightarrow \\sigma_{b0}^2=1\\)\n- Random slope SD \\(=0.3 \\Rightarrow \\sigma_{b1}^2=0.09\\)\n- Residual SD \\(=1.0 \\Rightarrow \\sigma_e^2=1\\)\n- Assume random intercept and random slope are uncorrelated\n- Two-sided \\(\\alpha=0.05\\), target power \\(=0.80\\)\n\n### Within-subject covariance\nFor times \\(t_i,t_j\\),\n\\[\nV_{ij}=\\sigma_{b0}^2 + \\sigma_{b1}^2\\, t_i t_j + \\sigma_e^2\\,\\mathbf{1}(i=j)\n= 1 + 0.09\\,t_i t_j + 1\\cdot \\mathbf{1}(i=j).\n\\]\n\n### Variance of the estimated slope-difference\nUsing GLS information for the fixed effects in the LMM with design \\([1,\\ t,\\ \\text{group},\\ t\\cdot\\text{group}]\\) and balanced groups, the per-*group* sample size \\(n\\) yields:\n\\[\n\\operatorname{Var}(\\widehat{\\beta}_{GT})=\\frac{239}{1050}\\cdot\\frac{1}{n}.\n\\]\nSo\n\\[\nSE(\\widehat{\\beta}_{GT})=\\sqrt{\\frac{239}{1050\\,n}}.\n\\]\n\n### Power requirement (normal/Wald approximation)\nFor a two-sided test at \\(\\alpha=0.05\\),\n\\[\nz_{1-\\alpha/2}=1.959964,\\quad z_{0.80}=0.841621,\n\\]\nand an accurate planning approximation is\n\\[\n\\frac{\\beta_{GT}}{SE} \\ge z_{1-\\alpha/2}+z_{0.80}=2.801585.\n\\]\n\nSo,\n\\[\n\\frac{0.1}{\\sqrt{239/(1050n)}} \\ge 2.801585\n\\]\n\\[\n\\sqrt{\\frac{239}{1050n}} \\le \\frac{0.1}{2.801585}=0.035694\n\\]\n\\[\n\\frac{239}{1050n} \\le 0.035694^2 = 0.001273\n\\]\n\\[\nn \\ge \\frac{239}{1050\\cdot 0.001273} = 178.77.\n\\]\n\n---\n\n## **Minimum required sample size: \\(\\boxed{179}\\) subjects per group** (total \\(\\boxed{358}\\))",
    "latency_ms": 122688,
    "tokens": {
      "input": 149,
      "output": 7096,
      "total": 7245
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:57:25.182Z"
  },
  "t2-mixed-007": {
    "task_id": "t2-mixed-007",
    "tier": 2,
    "question": "I'm planning a 2-period AB/BA crossover trial. The within-subject correlation is 0.7, and the treatment effect is d = 0.6 (standardized by total SD). How many subjects do I need for 85% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a 2-period AB/BA crossover with no carryover, the treatment effect is estimated from **within-subject differences** (paired comparison).\n\n### 1) Convert the given standardized effect to the *paired* standardized effect\nLet \\(d\\) be standardized by the **marginal (total) SD** \\(\\sigma\\):  \n\\[\nd=\\frac{\\mu_T-\\mu_C}{\\sigma}=0.6\n\\]\nWith within-subject correlation \\(\\rho=0.7\\), the SD of the within-subject difference is:\n\\[\nSD(D)=\\sqrt{\\mathrm{Var}(Y_T-Y_C)}=\\sqrt{2\\sigma^2(1-\\rho)}=\\sigma\\sqrt{2(1-\\rho)}\n\\]\nSo the standardized effect for the paired t-test is:\n\\[\n\\delta=\\frac{\\mu_T-\\mu_C}{SD(D)}=\\frac{d}{\\sqrt{2(1-\\rho)}}=\\frac{0.6}{\\sqrt{2(1-0.7)}}\n=\\frac{0.6}{\\sqrt{0.6}}=0.7746\n\\]\n\n### 2) Sample size for paired t-test (two-sided \\(\\alpha=0.05\\), power \\(=0.85\\))\nUsing the usual planning formula (normal approx) as a start:\n\\[\nn \\approx \\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\delta}\\right)^2\n\\]\nwith \\(z_{0.975}=1.96\\), \\(z_{0.85}=1.036\\):\n\\[\nn \\approx \\left(\\frac{1.96+1.036}{0.7746}\\right)^2\n= \\left(\\frac{2.996}{0.7746}\\right)^2\n= (3.869)^2\n= 14.97 \\approx 15\n\\]\n\nBecause \\(n\\) is small, using a **t-based refinement** (iterating with \\(t\\) quantiles for df \\(=n-1\\)) increases the requirement slightly; this converges to about:\n\\[\nn \\approx 17\n\\]\n\n---\n\n## Final answer (rounded up)\n**You need \\( \\boxed{17} \\) subjects total** to achieve **85% power** at **\\(\\alpha=0.05\\)**.\n\nIf you want perfectly balanced sequences (equal numbers AB and BA), use **\\(\\boxed{18}\\)** (9 per sequence).",
    "latency_ms": 104351,
    "tokens": {
      "input": 122,
      "output": 1880,
      "total": 2002
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T03:59:11.047Z"
  },
  "t2-mixed-008": {
    "task_id": "t2-mixed-008",
    "tier": 2,
    "question": "I'm running a trial with 2 correlated outcomes (correlation = 0.6) measured at 3 time points. The treatment effect on the primary outcome is d = 0.4, and within-subject ICC is 0.5. I only need to power for the primary outcome (no multiplicity correction). How many subjects per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Because you are only powering the **primary outcome**, the **correlation with the second outcome (0.6)** does **not** affect the required sample size.\n\nAssume a standard repeated-measures analysis that effectively compares groups on the **average of the 3 repeated measurements**, with **compound symmetry** (within-subject correlation = ICC = 0.5). Let \\(m=3\\) time points.\n\n---\n\n## Step 1: Two-sample (single time point) sample size for \\(d=0.4\\)\n\nFor a 2-arm trial, two-sided \\(\\alpha=0.05\\), power \\(=0.80\\):\n\n- \\(z_{1-\\alpha/2}=1.96\\)\n- \\(z_{1-\\beta}=0.842\\)\n\n\\[\nn_{\\text{per group, single}} \\;=\\; \\frac{2\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{d^2}\n= \\frac{2\\,(1.96+0.842)^2}{0.4^2}\n\\]\n\n\\[\n= \\frac{2\\,(2.802)^2}{0.16}\n= \\frac{2\\cdot 7.85}{0.16}\n= \\frac{15.7}{0.16}\n\\approx 98.1\n\\]\n\nSo single-timepoint would be about **99 per group** (rounding up).\n\n---\n\n## Step 2: Variance reduction from averaging 3 correlated measures\n\nVariance of the subject-level mean of \\(m\\) repeated measures (each with variance \\(\\sigma^2\\)) under compound symmetry with correlation \\(\\rho\\) is:\n\n\\[\n\\mathrm{Var}(\\bar{Y}) = \\sigma^2 \\cdot \\frac{1+(m-1)\\rho}{m}\n\\]\n\nSo the variance *factor* relative to a single measurement is:\n\n\\[\n\\frac{1+(m-1)\\rho}{m}\n= \\frac{1+(3-1)\\cdot 0.5}{3}\n= \\frac{1+1}{3}\n= \\frac{2}{3}\n\\approx 0.6667\n\\]\n\nThus required \\(n\\) is multiplied by this factor:\n\n\\[\nn_{\\text{per group, RM}} \\approx 98.1 \\times \\frac{2}{3} \\approx 65.4\n\\]\n\nRound up:\n\n\\[\nn_{\\text{per group}} = 66\n\\]\n\n---\n\n# Final answer (rounded up)\n## **66 subjects per group** (i.e., **132 total**) for 80% power at \\(\\alpha=0.05\\).",
    "latency_ms": 55716,
    "tokens": {
      "input": 146,
      "output": 1443,
      "total": 1589
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:00:08.265Z"
  },
  "t2-surv-001": {
    "task_id": "t2-surv-001",
    "tier": 2,
    "question": "We're planning a survival trial where we expect the treatment to reduce the hazard by 30% (HR = 0.7). Control group median survival is 2 years. We'll have 3 years of accrual and 2 years of additional follow-up. How many patients per arm for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### 1) Convert control median to hazard rates (assume exponential survival)\nControl median \\(m_c = 2\\) years:\n\n\\[\n\\lambda_c=\\frac{\\ln 2}{m_c}=\\frac{0.6931}{2}=0.3466\\ \\text{per year}\n\\]\n\nTreatment HR \\(=0.7\\):\n\n\\[\n\\lambda_t = 0.7\\lambda_c = 0.7(0.3466)=0.2426\\ \\text{per year}\n\\]\n\n---\n\n### 2) Required number of events (Schoenfeld / log-rank)\nTwo-sided \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha/2}=1.96\\)  \nPower \\(=80\\% \\Rightarrow z_{1-\\beta}=0.8416\\)  \nEqual allocation \\(\\Rightarrow p=0.5,\\; p(1-p)=0.25\\)  \n\\(\\log(HR)=\\log(0.7)=-0.3567\\)\n\n\\[\nD=\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{(\\log HR)^2\\,p(1-p)}\n= \\frac{(1.96+0.8416)^2}{(0.3567)^2 \\cdot 0.25}\n\\approx \\frac{7.8489}{0.0318}=246.8\n\\]\n\nSo, **required events**:\n\\[\nD \\approx 247\\ \\text{events}\n\\]\n\n---\n\n### 3) Expected event probability given 3 years accrual + 2 years additional follow-up\nUniform accrual over \\(A=3\\) years, additional follow-up \\(F=2\\) years.  \nAverage event probability under exponential hazard \\(\\lambda\\):\n\n\\[\n\\Pr(\\text{event}) = 1 - \\frac{e^{-\\lambda F}-e^{-\\lambda(F+A)}}{A\\lambda}\n\\]\n\n**Control: \\(\\lambda_c=0.3466\\)**\n- \\(e^{-\\lambda_c F}=e^{-0.6931}=0.5000\\)\n- \\(e^{-\\lambda_c(F+A)}=e^{-1.7329}=0.1768\\)\n\n\\[\n\\Pr_c = 1-\\frac{0.5000-0.1768}{3(0.3466)} \n= 1-\\frac{0.3232}{1.0398}\n= 0.6891\n\\]\n\n**Treatment: \\(\\lambda_t=0.2426\\)**\n- \\(e^{-\\lambda_t F}=e^{-0.4852}=0.6155\\)\n- \\(e^{-\\lambda_t(F+A)}=e^{-1.2129}=0.2975\\)\n\n\\[\n\\Pr_t = 1-\\frac{0.6155-0.2975}{3(0.2426)} \n= 1-\\frac{0.3180}{0.7278}\n= 0.5630\n\\]\n\nWith equal allocation, average event probability:\n\n\\[\n\\bar{\\Pr}=\\frac{\\Pr_c+\\Pr_t}{2}=\\frac{0.6891+0.5630}{2}=0.6261\n\\]\n\n---\n\n### 4) Total sample size and per-arm sample size\n\\[\nN = \\frac{D}{\\bar{\\Pr}}=\\frac{247}{0.6261}=394.5\n\\]\n\nRound up and keep equal arms \\(\\Rightarrow N=396\\) total, **198 per arm**.\n\n---\n\n## **Final answer: 198 patients per arm (396 total)**  \n(Assuming exponential survival, uniform accrual, two-sided \\(\\alpha=0.05\\), no dropout.)",
    "latency_ms": 52219,
    "tokens": {
      "input": 132,
      "output": 1763,
      "total": 1895
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:01:01.987Z"
  },
  "t2-surv-002": {
    "task_id": "t2-surv-002",
    "tier": 2,
    "question": "In our survival study, the control group has 50% 2-year survival and we expect HR = 0.65 for the treatment. With 2-year accrual and 1-year additional follow-up, how many patients per arm for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume exponential survival, proportional hazards, uniform accrual over 2 years, and only administrative censoring at study end (no other drop-out). Use the log-rank (Freedman) event-based method.\n\n## 1) Convert 2-year control survival to a hazard rate\nControl: \\(S_c(2)=0.50 = e^{-\\lambda_c \\cdot 2}\\)\n\n\\[\n\\lambda_c = -\\frac{\\ln(0.50)}{2} = \\frac{\\ln 2}{2} = 0.3466\\ \\text{per year}\n\\]\n\nTreatment hazard (HR = 0.65):\n\n\\[\n\\lambda_t = 0.65\\,\\lambda_c = 0.65(0.3466)=0.2253\n\\]\n\n## 2) Required number of events for log-rank test (equal allocation)\nFor two-sided \\(\\alpha=0.05\\): \\(z_{1-\\alpha/2}=1.96\\)  \nFor 80% power: \\(z_{1-\\beta}=0.84\\)\n\n\\[\nD=\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{(\\ln HR)^2 \\, p(1-p)},\\quad p=0.5\n\\]\n\\[\n\\ln(HR)=\\ln(0.65)=-0.4308,\\quad p(1-p)=0.25\n\\]\n\\[\nD=\\frac{(1.96+0.84)^2}{(0.4308)^2 \\cdot 0.25}\n= \\frac{(2.80)^2}{0.1856 \\cdot 0.25}\n= \\frac{7.84}{0.0464}\n=168.9 \\approx 169\\ \\text{events}\n\\]\n\n## 3) Expected event proportion given 2-year accrual + 1-year additional follow-up\nTotal study length = 3 years. With uniform entry \\(U\\sim \\text{Unif}(0,2)\\), administrative follow-up time is \\(\\tau=3-U\\in[1,3]\\).\n\nFor exponential hazard \\(\\lambda\\), the probability of observing an event by study end is:\n\\[\nq(\\lambda)=1-\\mathbb{E}\\left[e^{-\\lambda \\tau}\\right]\n=1-\\frac{e^{-\\lambda}-e^{-3\\lambda}}{2\\lambda}\n\\]\n\nControl (\\(\\lambda_c=0.3466\\)):\n- \\(e^{-\\lambda_c}=0.7071\\), \\(e^{-3\\lambda_c}=0.3535\\)\n\\[\nq_c = 1 - \\frac{0.7071-0.3535}{2(0.3466)}\n=1-\\frac{0.3536}{0.6931}\n=0.4900\n\\]\n\nTreatment (\\(\\lambda_t=0.2253\\)):\n- \\(e^{-\\lambda_t}=0.7983\\), \\(e^{-3\\lambda_t}=0.5087\\)\n\\[\nq_t = 1 - \\frac{0.7983-0.5087}{2(0.2253)}\n=1-\\frac{0.2896}{0.4506}\n=0.3574\n\\]\n\nWith equal allocation, overall event proportion:\n\\[\n\\bar q = \\frac{q_c+q_t}{2}=\\frac{0.4900+0.3574}{2}=0.4237\n\\]\n\n## 4) Convert required events to required sample size\n\\[\nN = \\frac{D}{\\bar q} = \\frac{169}{0.4237} = 398.9 \\Rightarrow 399\\ \\text{total}\n\\]\n\nPer arm (equal allocation):\n\\[\nn = \\lceil 399/2 \\rceil = \\lceil 199.5 \\rceil = 200\\ \\text{per arm}\n\\]\n\n---\n\n# Final answer: **200 patients per arm** (≈ **400 total**)",
    "latency_ms": 62348,
    "tokens": {
      "input": 123,
      "output": 1913,
      "total": 2036
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:02:05.838Z"
  },
  "t2-surv-003": {
    "task_id": "t2-surv-003",
    "tier": 2,
    "question": "I'm analyzing whether a continuous biomarker predicts survival. I expect an HR of 1.3 per standard deviation increase in the biomarker. About 25% of patients will experience the event during the study. How many patients for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Schoenfeld’s approximation for a Cox model with a **single continuous predictor**:\n\n### 1) Required number of events\nFor a standardized biomarker (per **1 SD** increase), \\( \\mathrm{Var}(X)=1 \\).\n\n\\[\nD \\;=\\; \\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{(\\log HR)^2 \\, \\mathrm{Var}(X)}\n\\]\n\nInputs:\n\n- \\(HR = 1.3 \\Rightarrow \\log(HR)=\\ln(1.3)=0.26236\\)\n- \\(\\alpha=0.05\\) (two-sided) \\(\\Rightarrow z_{1-\\alpha/2}=1.96\\)\n- Power \\(=0.80 \\Rightarrow \\beta=0.20 \\Rightarrow z_{1-\\beta}=0.8416\\)\n\nCompute:\n\n\\[\n(z_{1-\\alpha/2}+z_{1-\\beta})^2 = (1.96+0.8416)^2 = 2.8016^2 = 7.8489\n\\]\n\n\\[\nD = \\frac{7.8489}{(0.26236)^2 \\cdot 1} = \\frac{7.8489}{0.06884} = 114.0 \\text{ events}\n\\]\n\nSo you need **about 114 events**.\n\n### 2) Convert events to total sample size\nIf **25%** experience the event:\n\n\\[\nN = \\frac{D}{0.25} = \\frac{114.0}{0.25} = 456.1\n\\]\n\nRound up:\n\n## Final answer: **457 patients** (to achieve ~80% power at \\(\\alpha=0.05\\), assuming 25% event rate).",
    "latency_ms": 26236,
    "tokens": {
      "input": 120,
      "output": 727,
      "total": 847
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:02:33.576Z"
  },
  "t2-surv-004": {
    "task_id": "t2-surv-004",
    "tier": 2,
    "question": "We're planning a survival trial expecting HR = 0.75. Control 1-year survival is 70%. We'll have 18 months of accrual and 12 months of follow-up, but expect 20% dropout. How many patients per arm do we need for 85% power at alpha = 0.05, accounting for the dropout?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### 1) Convert 1-year control survival to an exponential hazard\nControl 1-year survival \\(S_C(1)=0.70\\). Under an exponential model:\n\\[\n\\lambda_C = -\\ln(0.70)=0.3567 \\text{ per year}\n\\]\nTarget HR \\(=0.75 \\Rightarrow \\lambda_T = 0.75\\lambda_C = 0.2675\\).\n\n### 2) Required number of events (Schoenfeld / log-rank)\nEqual allocation, two-sided \\(\\alpha=0.05\\), power \\(=85\\%\\) (\\(\\beta=0.15\\)):\n\n- \\(z_{1-\\alpha/2}=1.96\\)\n- \\(z_{1-\\beta}=z_{0.85}=1.036\\)\n- \\(\\log(HR)=\\log(0.75)=-0.28768\\)\n\nSchoenfeld required events:\n\\[\nD=\\frac{4(z_{1-\\alpha/2}+z_{1-\\beta})^2}{(\\log HR)^2}\n= \\frac{4(1.96+1.036)^2}{(-0.28768)^2}\n\\approx \\frac{4(2.996)^2}{0.08276}\n\\approx 434 \\text{ events}\n\\]\n\n### 3) Expected event proportion given accrual + follow-up\nAccrual = 18 months = 1.5 years, additional follow-up = 12 months = 1.0 year.  \nSo study ends at 2.5 years after first patient; with uniform accrual, each patient is followed for\n\\[\nV \\sim \\text{Uniform}(1.0,\\ 2.5)\\ \\text{years}.\n\\]\n\nFor exponential survival with hazard \\(\\lambda\\), the average event probability by study end is:\n\\[\n\\bar{p}(\\lambda)=1-\\frac{1}{1.5}\\int_{1}^{2.5} e^{-\\lambda v}\\,dv\n=1-\\frac{e^{-\\lambda}-e^{-2.5\\lambda}}{1.5\\lambda}.\n\\]\n\n- Control (\\(\\lambda_C=0.3567\\)):\n  \\[\n  \\bar{p}_C \\approx 0.458\n  \\]\n- Treatment (\\(\\lambda_T=0.2675\\)):\n  \\[\n  \\bar{p}_T \\approx 0.370\n  \\]\n\nWith equal randomization, average event proportion:\n\\[\n\\bar{p}=\\frac{\\bar{p}_C+\\bar{p}_T}{2}\\approx \\frac{0.458+0.370}{2}=0.414.\n\\]\n\n### 4) Convert events to total sample size, then account for 20% dropout\nWithout dropout:\n\\[\nN_0=\\frac{D}{\\bar{p}}=\\frac{434}{0.414}\\approx 1049 \\text{ total}.\n\\]\n\nWith 20% dropout, inflate by \\(1/(1-0.20)=1.25\\):\n\\[\nN = 1049 \\times 1.25 \\approx 1311.25 \\Rightarrow 1312 \\text{ total}.\n\\]\n\nPer arm (equal allocation):\n\\[\nN_{\\text{per arm}} = 1312/2 = 656.\n\\]\n\n## **Final answer: 656 patients per arm (1312 total), after accounting for 20% dropout.**",
    "latency_ms": 121229,
    "tokens": {
      "input": 1015,
      "output": 1561,
      "total": 2576
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:04:36.309Z"
  },
  "t2-surv-005": {
    "task_id": "t2-surv-005",
    "tier": 2,
    "question": "We're planning a survival study to detect HR = 0.70 (treatment benefit). Control median survival is 18 months and total study duration is 36 months. With 1:1 allocation, 80% power, and alpha = 0.05, what's the total sample size needed across both arms?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### 1) Required number of events (log-rank / Schoenfeld)\n\nFor equal allocation (1:1), the required total number of events is\n\n\\[\nD=\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{\\left(\\log(HR)\\right)^2\\;p(1-p)}\n\\]\n\n- \\(HR=0.70 \\Rightarrow \\log(HR)=\\log(0.70)=-0.3567\\)\n- \\(\\alpha=0.05\\) (two-sided) \\(\\Rightarrow z_{1-\\alpha/2}=z_{0.975}=1.96\\)\n- Power \\(=80\\%\\Rightarrow \\beta=0.20 \\Rightarrow z_{1-\\beta}=z_{0.80}=0.84\\)\n- \\(p=0.5 \\Rightarrow p(1-p)=0.25\\)\n\n\\[\nD=\\frac{(1.96+0.84)^2}{(0.3567)^2\\cdot 0.25}\n=\\frac{(2.80)^2}{0.1272\\cdot 0.25}\n=\\frac{7.84}{0.0318}\n=246.6 \\Rightarrow 247 \\text{ events}\n\\]\n\n---\n\n### 2) Convert events to total sample size using study duration = 36 months\n\nAssume **exponential survival** and **uniform accrual over the full 36 months**, with analysis at month 36.\n\nControl median = 18 months:\n\n\\[\n\\lambda_C=\\frac{\\ln 2}{18}=0.03851 \\text{ per month}\n\\quad,\\quad\n\\lambda_T=HR\\cdot \\lambda_C=0.70(0.03851)=0.02696\n\\]\n\nWith uniform entry over \\([0,36]\\), average event probability by 36 months for hazard \\(\\lambda\\) is:\n\n\\[\n\\bar{P}(\\text{event}) = 1-\\frac{1-e^{-36\\lambda}}{36\\lambda}\n\\]\n\n- Control: \\(36\\lambda_C=1.386\\), \\(e^{-1.386}=0.25\\)\n\n\\[\n\\bar{P}_C = 1-\\frac{1-0.25}{1.386}=1-\\frac{0.75}{1.386}=0.459\n\\]\n\n- Treatment: \\(36\\lambda_T=0.970\\), \\(e^{-0.970}=0.379\\)\n\n\\[\n\\bar{P}_T = 1-\\frac{1-0.379}{0.970}=1-\\frac{0.621}{0.970}=0.360\n\\]\n\nOverall (1:1 allocation):\n\n\\[\n\\bar{P}=\\frac{\\bar{P}_C+\\bar{P}_T}{2}=\\frac{0.459+0.360}{2}=0.4095\n\\]\n\nSo total sample size:\n\n\\[\nN = \\frac{D}{\\bar{P}}=\\frac{247}{0.4095}=603.1 \\Rightarrow 604\n\\]\n\n---\n\n## **Final answer (rounded up): 604 total participants (302 per arm)**\n\n*(This assumes exponential survival and uniform accrual over the entire 36-month study duration with analysis at 36 months.)*",
    "latency_ms": 68292,
    "tokens": {
      "input": 126,
      "output": 2107,
      "total": 2233
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:05:46.105Z"
  },
  "t2-surv-006": {
    "task_id": "t2-surv-006",
    "tier": 2,
    "question": "We're studying a condition where about 30% of control patients are long-term survivors (essentially cured). We expect the treatment to increase this cure fraction to 45%. Among those who aren't cured, median survival is 2 years and we expect a modest HR of 0.8 for treatment vs control. With 5-year follow-up, how many per arm for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assumptions (since not otherwise specified): equal randomization, no loss to follow-up, and **administrative censoring at 5 years** (i.e., each subject can be observed up to 5 years). Log-rank test with \\(\\alpha=0.05\\) two-sided, power \\(=0.80\\).\n\n## 1) Model setup (mixture cure + exponential for uncured)\nCure model survival:\n\\[\nS(t)=p_{\\text{cure}}+(1-p_{\\text{cure}})\\exp(-\\lambda t)\n\\]\n\nControl:\n- \\(p_{0}=0.30\\)\n- median among uncured = 2 years \\(\\Rightarrow \\lambda_0=\\ln(2)/2=0.3466\\)\n\nTreatment:\n- \\(p_{1}=0.45\\)\n- HR among uncured \\(=0.8 \\Rightarrow \\lambda_1=0.8\\lambda_0=0.2773\\)\n\nOverall hazards under a cure model:\n\\[\nh(t)=\\frac{(1-p)\\lambda e^{-\\lambda t}}{p+(1-p)e^{-\\lambda t}}\n\\]\n\nSo:\n- \\(S_0(t)=0.30+0.70e^{-0.3466t}\\), \\(h_0(t)=\\dfrac{0.70\\cdot0.3466 e^{-0.3466t}}{S_0(t)}\\)\n- \\(S_1(t)=0.45+0.55e^{-0.2773t}\\), \\(h_1(t)=\\dfrac{0.55\\cdot0.2773 e^{-0.2773t}}{S_1(t)}\\)\n\n## 2) Expected log-rank mean/variance under non-PH (fixed follow-up to 5y)\nWith equal allocation (\\(\\pi=0.5\\)) and censoring only at \\(T=5\\), the large-sample approximations are:\n\n\\[\nE[U] = nA,\\qquad Var(U)=nB,\\qquad E[Z]=\\frac{E[U]}{\\sqrt{Var(U)}}=\\sqrt{n}\\frac{A}{\\sqrt{B}}\n\\]\n\nwhere (integrals from 0 to 5):\n\\[\nA=\\int_0^5 \\frac{1}{2}\\left[\\frac{S_0(t)S_1(t)}{S_0(t)+S_1(t)}\\right]\\big(h_1(t)-h_0(t)\\big)\\,dt\n\\]\n\\[\nB=\\int_0^5 \\frac{1}{2}\\left[\\frac{S_0(t)S_1(t)}{(S_0(t)+S_1(t))^2}\\right]\\big(S_1(t)h_1(t)+S_0(t)h_0(t)\\big)\\,dt\n\\]\n\nNumerical quadrature over \\(0\\) to \\(5\\) years (step 0.5y, Simpson’s rule) gives approximately:\n\\[\nA \\approx -0.0585,\\qquad B \\approx 0.1226\n\\]\n\nFor \\(\\alpha=0.05\\) two-sided and 80% power:\n\\[\nz_{1-\\alpha/2}+z_{1-\\beta}=1.96+0.842=2.802\n\\]\n\nRequired total sample size:\n\\[\nn=\\left(\\frac{2.802\\sqrt{B}}{|A|}\\right)^2\n=2.802^2\\frac{B}{A^2}\n\\approx 7.85\\cdot \\frac{0.1226}{0.0585^2}\n\\approx 281.0\n\\]\n\nThus per arm:\n\\[\nn_{\\text{per arm}}=\\lceil 281/2\\rceil=\\lceil 140.5\\rceil=141\n\\]\n\n---\n\n# Final answer (rounded up)\n**Required sample size ≈ 141 patients per arm (≈ 282 total)** for 80% power at \\(\\alpha=0.05\\) with 5-year follow-up under the stated cure-fraction + exponential-uncured assumptions.",
    "latency_ms": 178975,
    "tokens": {
      "input": 148,
      "output": 8416,
      "total": 8564
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:08:46.582Z"
  },
  "t2-surv-007": {
    "task_id": "t2-surv-007",
    "tier": 2,
    "question": "In our survival study, 15% of patients may experience a competing event (e.g., death from unrelated cause) that's independent of treatment. We want to detect a cause-specific HR = 0.7 for our primary event, with a control event rate of 30%. What sample size per arm for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Required number of *primary* events (Schoenfeld / log-rank for cause-specific HR)\n\nFor equal allocation, the required number of observed primary events is\n\n\\[\nD=\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{(\\log HR)^2 \\, p(1-p)},\\quad p=0.5\n\\]\n\n- \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha/2}=1.96\\)\n- Power \\(=0.80 \\Rightarrow z_{1-\\beta}=0.842\\)\n- \\(HR=0.7 \\Rightarrow \\log(HR)=\\log(0.7)=-0.3567\\)\n\n\\[\nD=\\frac{(1.96+0.842)^2}{(0.3567)^2 \\cdot 0.25}\n=\\frac{(2.802)^2}{0.1272\\cdot 0.25}\n=\\frac{7.85}{0.0318}\n\\approx 246.8\n\\]\n\nSo we need **\\(D=247\\)** primary events.\n\n---\n\n### Step 2: Convert required events to sample size, accounting for 15% competing events\n\nAssume exponential cause-specific hazards and that the given “event rates” are end-of-study cumulative incidences in control:\n\n- Control primary event CIF: \\(CIF_{1C}=0.30\\)\n- Control competing event CIF: \\(CIF_{2C}=0.15\\)\n\nUnder exponential competing risks, the ratio of cause-specific hazards equals the ratio of CIFs:\n\n\\[\n\\frac{\\lambda_{1C}}{\\lambda_2}=\\frac{0.30}{0.15}=2 \\Rightarrow \\lambda_{1C}=2\\lambda_2\n\\]\n\nLet \\(\\lambda_2=\\lambda\\), then \\(\\lambda_{1C}=2\\lambda\\), total hazard in control \\(=3\\lambda\\).\n\nTotal event probability in control is \\(0.45\\):\n\n\\[\n0.45 = 1-e^{-3\\lambda T}\\Rightarrow e^{-3\\lambda T}=0.55\n\\Rightarrow 3\\lambda T = -\\ln(0.55)=0.5978\n\\Rightarrow \\lambda T=0.1993\n\\]\n\nTreatment has cause-specific HR = 0.7 for the primary event:\n\n\\[\n\\lambda_{1T}=0.7\\lambda_{1C}=0.7(2\\lambda)=1.4\\lambda\n\\]\nTotal hazard in treatment \\(=1.4\\lambda+\\lambda=2.4\\lambda\\).\n\nTreatment primary-event CIF:\n\n\\[\nCIF_{1T}= \\frac{1.4\\lambda}{2.4\\lambda}\\left(1-e^{-2.4\\lambda T}\\right)\n= \\frac{1.4}{2.4}\\left(1-e^{-2.4(0.1993)}\\right)\n\\]\n\n\\[\n2.4(0.1993)=0.4783,\\quad e^{-0.4783}=0.6199,\\quad 1-0.6199=0.3801\n\\]\n\n\\[\nCIF_{1T}=0.5833\\times 0.3801 \\approx 0.2217\n\\]\n\nAverage primary-event probability across arms:\n\n\\[\n\\bar{p}=\\frac{0.30+0.2217}{2}=0.26085\n\\]\n\nTotal sample size needed:\n\n\\[\nN=\\frac{D}{\\bar{p}}=\\frac{247}{0.26085}\\approx 946.7\n\\]\n\nPer arm (equal allocation):\n\n\\[\nn=\\frac{946.7}{2}\\approx 473.4 \\Rightarrow \\boxed{474 \\text{ per arm}}\n\\]\n\n---\n\n## Final answer (rounded up)\n\\[\n\\boxed{\\textbf{Sample size } \\approx 474 \\textbf{ patients per arm (948 total)}}\n\\]\n\n(Uses Schoenfeld events requirement and an exponential competing-risks approximation with a 15% independent competing event rate.)",
    "latency_ms": 73566,
    "tokens": {
      "input": 136,
      "output": 2081,
      "total": 2217
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:10:01.651Z"
  },
  "t2-surv-008": {
    "task_id": "t2-surv-008",
    "tier": 2,
    "question": "We're conducting a stratified survival analysis across 3 strata with different baseline hazards, but the treatment HR = 0.75 is assumed constant across strata. Over the 2-year study period, we expect 40% of controls to experience an event. What sample size per arm for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Required number of events (Schoenfeld / log-rank, stratified with common HR)\nFor a (stratified) log-rank test with proportional hazards and a common treatment effect, the **required total number of events** is essentially the same as the unstratified case:\n\n\\[\nD=\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2}{\\left[\\ln(HR)\\right]^2 \\, p(1-p)}\n\\]\n\nGiven:  \n- \\(HR=0.75\\)  \\(\\Rightarrow \\ln(HR)=\\ln(0.75)=-0.28768\\)  \n- \\(\\alpha=0.05\\) (two-sided) \\(\\Rightarrow z_{1-\\alpha/2}=1.96\\)  \n- Power \\(=0.80\\) \\(\\Rightarrow z_{1-\\beta}=0.8416\\)  \n- Equal allocation \\(\\Rightarrow p=0.5\\), so \\(p(1-p)=0.25\\)\n\nCompute:\n\n\\[\nD=\\frac{(1.96+0.8416)^2}{(-0.28768)^2 \\cdot 0.25}\n=\\frac{(2.8016)^2}{0.08276 \\cdot 0.25}\n=\\frac{7.8489}{0.02069}\n=379.4\n\\]\n\nSo we need **\\(D \\approx 380\\) total events**.\n\n---\n\n### Step 2: Convert required events to total sample size\nWe’re told over 2 years, **control event proportion** is \\(0.40\\), i.e. \\(S_C(2)=0.60\\).\n\nUnder proportional hazards, a common approximation is:\n\\[\nS_T(2) = [S_C(2)]^{HR} = 0.60^{0.75} = 0.6818\n\\]\nSo treatment event proportion:\n\\[\n\\Pr(\\text{event}|T)=1-0.6818=0.3182\n\\]\n\nWith equal randomization, the **overall event proportion** is:\n\\[\n\\bar{q}=\\frac{0.40+0.3182}{2}=0.3591\n\\]\n\nTotal sample size:\n\\[\nN=\\frac{D}{\\bar{q}}=\\frac{380}{0.3591}=1058.4\n\\]\n\nRound up: \\(N=1059\\) total, which implies **per arm**:\n\\[\nN_{\\text{per arm}} = \\lceil 1059/2 \\rceil = 530\n\\]\n\n---\n\n## **Final answer (prominent):**\n**Sample size ≈ 530 participants per arm** (≈ **1060 total**) to achieve **80% power** at **α = 0.05** for detecting **HR = 0.75**, given **40% 2-year event rate in controls**.",
    "latency_ms": 63641,
    "tokens": {
      "input": 131,
      "output": 1212,
      "total": 1343
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:11:06.794Z"
  },
  "t2-poisson-001": {
    "task_id": "t2-poisson-001",
    "tier": 2,
    "question": "We're comparing event rates between treatment and control. The control group has about 2 events per person-year, and we expect the treatment to increase this by 50% (rate ratio = 1.5). With 1-year follow-up, how many subjects per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Use the large-sample Wald test for the log rate ratio comparing two Poisson rates with equal follow-up time.\n\n- Control rate: \\(\\lambda_0 = 2\\) events/person-year  \n- Treatment rate: \\(\\lambda_1 = 1.5\\times 2 = 3\\) events/person-year  \n- Follow-up per subject: \\(t=1\\) year  \n- Equal allocation: \\(n\\) per group  \n- Two-sided \\(\\alpha=0.05 \\Rightarrow z_{1-\\alpha/2}=1.96\\)  \n- Power \\(=0.80 \\Rightarrow z_{1-\\beta}=0.8416\\)\n\nFor equal person-time \\(T_0=T_1 = nt\\), the variance of \\(\\log(\\widehat{RR})\\) under the alternative is approximately:\n\\[\n\\mathrm{Var}\\big(\\log(\\widehat{RR})\\big)\\approx \\frac{1}{\\lambda_1 nt}+\\frac{1}{\\lambda_0 nt}\n= \\frac{1}{nt}\\left(\\frac{1}{\\lambda_1}+\\frac{1}{\\lambda_0}\\right)\n\\]\n\nRequire:\n\\[\n\\frac{\\log(RR)}{\\sqrt{\\mathrm{Var}}} \\ge z_{1-\\alpha/2}+z_{1-\\beta}\n\\]\nSolving for \\(n\\):\n\\[\nn = \\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2\\left(\\frac{1}{\\lambda_1}+\\frac{1}{\\lambda_0}\\right)}{t\\,[\\log(RR)]^2}\n\\]\n\nPlug in values:\n- \\(\\log(RR)=\\log(1.5)=0.405465\\)\n- \\(\\left(\\frac{1}{\\lambda_1}+\\frac{1}{\\lambda_0}\\right)=\\frac{1}{3}+\\frac{1}{2}=0.833333\\)\n- \\((1.96+0.8416)^2 = 2.8016^2 = 7.8489\\)\n\n\\[\nn=\\frac{7.8489 \\times 0.833333}{1 \\times (0.405465)^2}\n=\\frac{6.54075}{0.164401}\n=39.78\n\\]\n\n**Final answer (rounded up): \\(\\boxed{40\\text{ subjects per group}}\\)** (1 year follow-up each, 80% power, two-sided \\(\\alpha=0.05\\)).",
    "latency_ms": 28647,
    "tokens": {
      "input": 129,
      "output": 1074,
      "total": 1203
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:11:36.944Z"
  },
  "t2-poisson-002": {
    "task_id": "t2-poisson-002",
    "tier": 2,
    "question": "We're testing whether a treatment reduces infection rate. Control patients average 0.5 infections per month, and we hope to reduce this to 0.3 per month. With 6 months of follow-up, how many per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Treat infections as Poisson counts with equal follow-up time \\(t=6\\) months per subject.\n\n- Control rate: \\(\\lambda_0=0.5\\) /month  \\(\\Rightarrow t\\lambda_0 = 6(0.5)=3.0\\) expected infections per control subject  \n- Treatment rate: \\(\\lambda_1=0.3\\) /month \\(\\Rightarrow t\\lambda_1 = 6(0.3)=1.8\\) expected infections per treated subject  \n- Target log rate ratio:  \n\\[\n\\log(RR)=\\log(\\lambda_1/\\lambda_0)=\\log(0.3/0.5)=\\log(0.6)=-0.5108\n\\]\n\nUsing a Wald test on \\(\\log(RR)\\), with equal \\(n\\) per group:\n\\[\n\\mathrm{Var}(\\log \\widehat{RR}) \\approx \\frac{1}{n t \\lambda_1}+\\frac{1}{n t \\lambda_0}\n\\]\nSo the required per-group sample size is:\n\\[\nn=\\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2\\left(\\frac{1}{t\\lambda_1}+\\frac{1}{t\\lambda_0}\\right)}{\\left[\\log(\\lambda_1/\\lambda_0)\\right]^2}\n\\]\n\nPlug in \\(\\alpha=0.05\\) (two-sided) \\(\\Rightarrow z_{1-\\alpha/2}=1.96\\), and 80% power \\(\\Rightarrow z_{1-\\beta}=0.8416\\):\n\\[\n(1.96+0.8416)^2 = 2.8016^2 = 7.8489\n\\]\n\\[\n\\frac{1}{t\\lambda_1}+\\frac{1}{t\\lambda_0} = \\frac{1}{1.8}+\\frac{1}{3.0}=0.5556+0.3333=0.8889\n\\]\n\\[\n[\\log(0.6)]^2 = 0.5108^2 = 0.2609\n\\]\n\\[\nn=\\frac{7.8489 \\times 0.8889}{0.2609}=\\frac{6.9768}{0.2609}=26.73\n\\]\n\n**Final answer (rounded up): \\(\\boxed{27\\text{ patients per group}}\\)** (total \\(=54\\)).",
    "latency_ms": 53178,
    "tokens": {
      "input": 121,
      "output": 1062,
      "total": 1183
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:12:31.624Z"
  },
  "t2-poisson-003": {
    "task_id": "t2-poisson-003",
    "tier": 2,
    "question": "Our outcome is count data with overdispersion (variance greater than mean). The control group averages 5 events, treatment group expected to average 4 events, with dispersion parameter k = 2. How many per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a negative binomial (NB2) model with mean \\(\\mu\\) and variance\r\n\\[\n\\mathrm{Var}(Y)=\\mu+\\frac{\\mu^2}{k},\\quad k=2,\r\n\\]\r\nequal follow-up in both groups, and a 2-sided test of the log rate ratio (Wald test). Let \\(\\mu_C=5\\) and \\(\\mu_T=4\\).\r\n\r\n**Effect size (log rate ratio):**\r\n\\[\n\\delta=\\log(\\mu_T/\\mu_C)=\\log(4/5)=\\log(0.8)=-0.22314,\\quad \\delta^2=0.04979.\r\n\\]\r\n\r\nFor NB2, using the delta method for \\(\\log(\\bar Y)\\),\r\n\\[\n\\mathrm{Var}(\\log\\bar Y)\\approx \\frac{1}{n}\\left(\\frac{1}{\\mu}+\\frac{1}{k}\\right).\r\n\\]\r\nSo for the difference in logs:\r\n\\[\n\\mathrm{Var}\\big(\\log \\hat\\mu_T-\\log \\hat\\mu_C\\big)\\approx \\frac{1}{n}\\left(\\frac{1}{\\mu_T}+\\frac{1}{\\mu_C}+\\frac{2}{k}\\right).\r\n\\]\r\nCompute the bracketed term:\r\n\\[\n\\frac{1}{4}+\\frac{1}{5}+\\frac{2}{2}=0.25+0.20+1.00=1.45.\r\n\\]\r\n\r\nCritical values: \\(\\alpha=0.05\\) (2-sided) \\(\\Rightarrow z_{0.975}=1.96\\); power \\(=0.80\\) \\(\\Rightarrow z_{0.80}=0.842\\).\r\n\\[\n(z_{0.975}+z_{0.80})^2=(1.96+0.842)^2=2.802^2\\approx 7.849.\r\n\\]\r\n\r\nSample size per group:\r\n\\[\nn=\\frac{(z_{0.975}+z_{0.80})^2\\left(\\frac{1}{\\mu_T}+\\frac{1}{\\mu_C}+\\frac{2}{k}\\right)}{\\left[\\log(\\mu_T/\\mu_C)\\right]^2}\r\n=\\frac{7.849\\times 1.45}{0.04979}\\approx 228.6.\r\n\\]\r\n\r\n### Final answer\r\n**\\(n = 229\\) per group** (total \\(= 458\\)).",
    "latency_ms": 38105,
    "tokens": {
      "input": 119,
      "output": 1074,
      "total": 1193
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:13:11.232Z"
  },
  "t2-poisson-004": {
    "task_id": "t2-poisson-004",
    "tier": 2,
    "question": "We're analyzing count data where patients have variable follow-up time (mean 0.8 years, SD 0.3 years). The baseline event rate is 3 per year, and we want to detect a rate ratio of 1.4. How many subjects per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume two independent Poisson rates compared by a log–rate ratio (Poisson regression with log(time) offset), equal allocation, two-sided \\(\\alpha=0.05\\).\n\n**Given**\n- Control (baseline) rate: \\(\\lambda_0 = 3\\) per year  \n- Target rate ratio: \\(RR=1.4 \\Rightarrow \\lambda_1 = 4.2\\) per year  \n- Mean follow-up time: \\(\\bar T = 0.8\\) years (use mean exposure; total exposure \\(\\approx n\\bar T\\) per group)\n- Power \\(=0.80\\), \\(\\alpha=0.05\\)\n\n### 1) Effect size on log scale\n\\[\n\\beta=\\log(RR)=\\log(1.4)=0.33647\n\\]\n\n### 2) Variance of \\(\\hat\\beta\\)\nFor two-group Poisson rates, an approximation is:\n\\[\n\\mathrm{Var}(\\hat\\beta)\\approx \\frac{1}{\\mu_1}+\\frac{1}{\\mu_0}\n\\]\nwhere \\(\\mu_g\\) is the **expected total number of events** in group \\(g\\).\n\nWith \\(n\\) subjects per group and mean follow-up \\(\\bar T\\):\n\\[\n\\mu_0=\\lambda_0\\, n\\,\\bar T = 3\\cdot n\\cdot 0.8 = 2.4n\n\\]\n\\[\n\\mu_1=\\lambda_1\\, n\\,\\bar T = 4.2\\cdot n\\cdot 0.8 = 3.36n\n\\]\nSo\n\\[\n\\mathrm{Var}(\\hat\\beta)\\approx \\frac{1}{3.36n}+\\frac{1}{2.4n}\n= \\frac{0.297619+0.416667}{n}\n= \\frac{0.714286}{n}\n\\]\n\n### 3) Wald-type sample size equation\n\\[\n\\frac{|\\beta|}{\\sqrt{\\mathrm{Var}(\\hat\\beta)}} \\ge z_{1-\\alpha/2}+z_{1-\\text{power}}\n\\]\n\\[\nz_{1-\\alpha/2}=1.96,\\quad z_{1-\\text{power}}=z_{0.80}=0.8416\n\\]\n\\[\nz\\text{-sum}=2.8016\n\\]\nRequired SE:\n\\[\nSE \\le \\frac{\\beta}{2.8016}=\\frac{0.33647}{2.8016}=0.1201\n\\]\nThus required variance:\n\\[\n\\mathrm{Var} \\le 0.1201^2 = 0.01442\n\\]\nSet \\(\\frac{0.714286}{n} \\le 0.01442\\):\n\\[\nn \\ge \\frac{0.714286}{0.01442} = 49.55\n\\]\n\n## **Final answer (rounded up): \\(\\boxed{50\\text{ subjects per group}}\\)**\n\n(Uses mean follow-up time 0.8 years; the given SD mainly affects the variability of total exposure, but the planning calculation is driven by expected total exposure.)",
    "latency_ms": 65435,
    "tokens": {
      "input": 130,
      "output": 1741,
      "total": 1871
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:14:18.170Z"
  },
  "t2-poisson-005": {
    "task_id": "t2-poisson-005",
    "tier": 2,
    "question": "Our count outcome has 30% structural zeros (patients who can never experience the event). Among those who can have events, the control mean is 3 and treatment mean is 2. What sample size per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Let \\(p_0=0.30\\) be the structural-zero probability (same in both groups). Among “susceptible” patients (\\(1-p_0=0.70\\)), counts are Poisson with mean \\(\\lambda\\).\n\n## 1) Overall mean in each group\n\\[\n\\mu = (1-p_0)\\lambda\n\\]\nControl: \\(\\lambda_C=3 \\Rightarrow \\mu_C=0.7\\times 3=2.1\\)\n\nTreatment: \\(\\lambda_T=2 \\Rightarrow \\mu_T=0.7\\times 2=1.4\\)\n\nDifference to detect:\n\\[\n\\delta = \\mu_C-\\mu_T = 2.1-1.4=0.7\n\\]\n\n## 2) Overall variance for a zero-inflated Poisson\nFor structural zeros + Poisson among susceptibles:\n\\[\n\\mathrm{Var}(Y)= (1-p_0)\\lambda + \\lambda^2 p_0(1-p_0)\n= (1-p_0)\\lambda\\left(1+p_0\\lambda\\right)\n\\]\n\nControl variance:\n\\[\n\\sigma_C^2 = 0.7\\cdot 3\\cdot(1+0.3\\cdot 3)=2.1(1.9)=3.99\n\\]\n\nTreatment variance:\n\\[\n\\sigma_T^2 = 0.7\\cdot 2\\cdot(1+0.3\\cdot 2)=1.4(1.6)=2.24\n\\]\n\n## 3) Two-sample normal approximation for difference in means (equal \\(n\\))\n\\[\nn = \\frac{\\left(z_{1-\\alpha/2}+z_{1-\\beta}\\right)^2(\\sigma_C^2+\\sigma_T^2)}{\\delta^2}\n\\]\nWith \\(\\alpha=0.05\\) (two-sided): \\(z_{1-\\alpha/2}=1.96\\)\n\nPower \\(=80\\%\\): \\(z_{1-\\beta}=z_{0.8}=0.8416\\)\n\n\\[\nn=\\frac{(1.96+0.8416)^2(3.99+2.24)}{0.7^2}\n=\\frac{(2.8016)^2(6.23)}{0.49}\n=\\frac{7.8489\\times 6.23}{0.49}\n\\approx \\frac{48.90}{0.49}\n\\approx 99.8\n\\]\n\nRound up:\n\n## **Final answer: \\(n = 100\\) patients per group (total \\(N=200\\)).**",
    "latency_ms": 64824,
    "tokens": {
      "input": 118,
      "output": 1212,
      "total": 1330
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:15:24.497Z"
  },
  "t2-poisson-006": {
    "task_id": "t2-poisson-006",
    "tier": 2,
    "question": "I'm studying a condition where the event rate increases by about 10% each year. At baseline (time 0), the rate is 1 event per person-year. I want to detect if a treatment reduces the rate ratio to 0.8 over a 3-year follow-up period. What sample size per group for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume each subject is followed for the full 3 years and events follow a Poisson process with a common time trend in both arms; treatment multiplies the rate by a constant rate ratio \\( \\theta=0.8\\).\n\n## 1) Expected cumulative events per subject (control)\nEvent rate increases 10% per year from 1 event/person-year at baseline. Using a piecewise-constant annual rate:\n\n- Year 1: \\(1\\)\n- Year 2: \\(1.1\\)\n- Year 3: \\(1.1^2=1.21\\)\n\nSo cumulative mean events per control subject over 3 years:\n\\[\n\\Lambda_0 = 1 + 1.1 + 1.21 = 3.31\n\\]\nTreatment arm cumulative mean:\n\\[\n\\Lambda_1 = \\theta \\Lambda_0 = 0.8\\times 3.31 = 2.648\n\\]\n\n## 2) Sample size formula (Wald test for \\(\\log\\) rate ratio)\nWith equal \\(n\\) per group, expected event totals are\n\\[\n\\mu_0 = n\\Lambda_0,\\quad \\mu_1 = n\\Lambda_1 = n\\theta\\Lambda_0\n\\]\nFor \\(\\log(\\text{RR})\\), an approximation is\n\\[\n\\mathrm{Var}(\\log \\widehat{\\text{RR}})\\approx \\frac{1}{\\mu_1}+\\frac{1}{\\mu_0}\n\\]\nPower requirement (two-sided \\(\\alpha=0.05\\), power \\(=0.80\\)):\n\\[\n\\frac{|\\log\\theta|}{\\sqrt{\\frac{1}{\\mu_1}+\\frac{1}{\\mu_0}}} \\ge z_{1-\\alpha/2}+z_{1-\\beta}\n\\]\nRearranging gives:\n\\[\nn \\ge \\frac{(z_{1-\\alpha/2}+z_{1-\\beta})^2\\,(1+1/\\theta)}{\\Lambda_0\\,(\\log\\theta)^2}\n\\]\n\nPlug in:\n- \\(z_{1-\\alpha/2}=1.96\\)\n- \\(z_{1-\\beta}=0.8416\\)\n- \\((1.96+0.8416)^2 = 2.8016^2 = 7.8489\\)\n- \\(\\log(0.8)=-0.223143\\), so \\((\\log 0.8)^2=0.049793\\)\n- \\(1+1/\\theta = 1+1/0.8 = 2.25\\)\n- \\(\\Lambda_0=3.31\\)\n\n\\[\nn \\ge \\frac{7.8489\\times 2.25}{3.31\\times 0.049793}\n= \\frac{17.660}{0.164815}\n= 107.17\n\\]\n\n## Final answer (rounded up)\n**Required sample size: \\(\\boxed{108\\text{ participants per group}}\\)** (for 80% power at \\(\\alpha=0.05\\), 3-year follow-up, 10% annual increase, target RR = 0.8).\n\n*Note:* If you model the 10% increase as continuous compounding over time rather than stepwise yearly, \\(\\Lambda_0\\) is slightly larger and the result is about **103 per group** instead of 108.",
    "latency_ms": 50506,
    "tokens": {
      "input": 139,
      "output": 1853,
      "total": 1992
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:16:16.505Z"
  },
  "t2-poisson-007": {
    "task_id": "t2-poisson-007",
    "tier": 2,
    "question": "We have 20 clinics per arm with 25 subjects per clinic. The ICC is 0.03, baseline rate is 4 events per person-year, and we want to detect a rate ratio of 1.25. What power does this cluster design achieve at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume each subject contributes **1 person-year** of follow-up (if follow-up differs, multiply event totals by the average person-years per subject; power will increase with more follow-up).\n\n### 1) Cluster design effect\n- Clusters per arm: \\(k=20\\)  \n- Subjects per cluster: \\(m=25\\)  \n- ICC: \\(\\rho=0.03\\)\n\n\\[\nDE = 1 + (m-1)\\rho = 1 + 24(0.03) = 1.72\n\\]\n\n### 2) Effective sample size per arm\nNominal subjects per arm: \\(n = 20\\times 25 = 500\\)\n\n\\[\nn_{\\text{eff}} = \\frac{n}{DE} = \\frac{500}{1.72} = 290.70\n\\]\n\n### 3) Expected event totals (Poisson rates)\nBaseline (control) rate: \\(\\lambda_0=4\\) events/PY  \nTarget rate ratio: \\(RR=1.25 \\Rightarrow \\lambda_1=5\\) events/PY\n\n\\[\nE_0 = n_{\\text{eff}}\\lambda_0 = 290.70\\times 4 = 1162.79\n\\]\n\\[\nE_1 = n_{\\text{eff}}\\lambda_1 = 290.70\\times 5 = 1453.49\n\\]\n\n### 4) Standard error of \\(\\log(RR)\\)\nFor two independent Poisson totals:\n\\[\n\\mathrm{Var}(\\log \\widehat{RR}) \\approx \\frac{1}{E_1}+\\frac{1}{E_0}\n= \\frac{1}{1453.49}+\\frac{1}{1162.79}\n=0.001548\n\\]\n\\[\nSE=\\sqrt{0.001548}=0.03935\n\\]\n\nTrue effect on log scale:\n\\[\n\\delta=\\log(1.25)=0.22314\n\\]\n\nWald Z under \\(H_1\\):\n\\[\nZ = \\frac{\\delta}{SE} = \\frac{0.22314}{0.03935}=5.67\n\\]\n\n### 5) Power at \\(\\alpha=0.05\\) (two-sided)\nCritical value \\(z_{0.975}=1.96\\). Power \\(\\approx P(Z>1.96\\mid \\mu=5.67)\\):\n\\[\n\\text{Power} \\approx 1-\\Phi(1.96-5.67)\n=1-\\Phi(-3.71)=\\Phi(3.71)=0.9999\n\\]\n\n(Using a small-sample \\(t\\) critical value with df \\(\\approx 2(k-1)=38\\) changes this negligibly.)\n\n---\n\n## **Final answer: Power \\(\\approx 0.9999\\) (about 99.99%)**",
    "latency_ms": 52272,
    "tokens": {
      "input": 123,
      "output": 1395,
      "total": 1518
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:17:10.280Z"
  },
  "t3-cluster-001": {
    "task_id": "t3-cluster-001",
    "tier": 3,
    "question": "We're planning a cluster randomized trial with 20 clinics per arm. Based on pilot data, the ICC is about 0.05 and we expect a treatment effect of d = 0.4. How many patients do we need per clinic to achieve 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Individually randomized trial sample size (per arm) for effect size \\(d=0.4\\)\n\nFor a two-sample comparison of means with equal group sizes, two-sided \\(\\alpha=0.05\\), power \\(=80\\%\\):\n\n\\[\nn_{\\text{indiv, per arm}} \\;=\\; \\frac{2\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{d^2}\n\\]\n\nWith \\(z_{1-\\alpha/2}=1.96\\), \\(z_{1-\\beta}=0.84\\), \\(d=0.4\\):\n\n\\[\nn_{\\text{indiv, per arm}} = \\frac{2(1.96+0.84)^2}{0.4^2}\n= \\frac{2(2.80)^2}{0.16}\n= \\frac{2 \\cdot 7.84}{0.16}\n= \\frac{15.68}{0.16}\n= 98\n\\]\n\nSo we need an **effective** sample size of about **98 per arm**.\n\n---\n\n### Step 2: Inflate for clustering (design effect) and solve for patients per clinic\n\nDesign effect:\n\n\\[\nDE = 1 + (m-1)\\rho\n\\]\nwith ICC \\(\\rho = 0.05\\), and \\(m=\\) patients per clinic.\n\nEffective sample size per arm in a CRT with \\(k=20\\) clinics per arm:\n\n\\[\nn_{\\text{eff, per arm}} = \\frac{k\\,m}{DE} = \\frac{20m}{1+(m-1)0.05}\n\\]\n\nSet this equal to 98 and solve:\n\n\\[\n\\frac{20m}{1+0.05(m-1)} = 98\n\\]\n\nNote \\(1+0.05(m-1)=0.95+0.05m\\):\n\n\\[\n20m = 98(0.95+0.05m)=93.1+4.9m\n\\]\n\\[\n20m-4.9m=93.1\n\\]\n\\[\n15.1m=93.1 \\Rightarrow m=6.17\n\\]\n\nRound up:\n\n\\[\nm = 7\n\\]\n\nCheck:\n- If \\(m=6\\): \\(DE=1+5(0.05)=1.25\\), \\(n_{\\text{eff}}=120/1.25=96\\) (too low)\n- If \\(m=7\\): \\(DE=1+6(0.05)=1.30\\), \\(n_{\\text{eff}}=140/1.30=107.7\\) (enough)\n\n---\n\n## **Final answer: 7 patients per clinic**\nWith **20 clinics per arm**, **ICC = 0.05**, and expected **effect size \\(d=0.4\\)**, you need **at least 7 patients per clinic** to achieve **80% power at \\(\\alpha=0.05\\)**.",
    "latency_ms": 57482,
    "tokens": {
      "input": 124,
      "output": 1368,
      "total": 1492
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:18:09.264Z"
  },
  "t3-cluster-002": {
    "task_id": "t3-cluster-002",
    "tier": 3,
    "question": "We're designing a school-based intervention with 15 schools per arm, averaging 30 students per school. The ICC is estimated at 0.08. What is the minimum detectable effect size (Cohen's d) with 80% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a **two-arm cluster RCT**, equal cluster sizes, **two-sided α = 0.05**, and Cohen’s \\(d\\) defined using the **individual-level SD**.\n\n**Given**\n- Clusters (schools) per arm: \\(k=15\\)\n- Students per school: \\(m=30\\)\n- ICC: \\(\\rho=0.08\\)\n\n### 1) Design effect\n\\[\nDE = 1 + (m-1)\\rho = 1 + (30-1)\\cdot 0.08 = 1 + 29\\cdot 0.08 = 3.32\n\\]\n\n### 2) Standard error of standardized mean difference (Cohen’s \\(d\\))\nWith equal sizes, the SE for \\(d\\) is:\n\\[\nSE(d)=\\sqrt{\\frac{2\\cdot DE}{k m}}\n      =\\sqrt{\\frac{2\\cdot 3.32}{15\\cdot 30}}\n      =\\sqrt{\\frac{6.64}{450}}\n      =\\sqrt{0.014756}\n      =0.1215\n\\]\n\n### 3) Minimum detectable effect (80% power)\nUsing a small-sample \\(t\\) correction with cluster degrees of freedom:\n\\[\ndf \\approx 2k-2 = 28\n\\]\n\\[\nt_{0.975,28}\\approx 2.048,\\quad t_{0.80,28}\\approx 0.855\n\\]\n\\[\nMDE = (t_{0.975,28}+t_{0.80,28})\\cdot SE(d)\n     =(2.048+0.855)\\cdot 0.1215\n     =2.903\\cdot 0.1215\n     =0.352\n\\]\n\n## **Final answer (MDE, Cohen’s d): \\(\\boxed{0.35}\\)**\n\n(If you used a normal \\(z\\)-approximation instead of \\(t\\), you’d get slightly smaller, about \\(d\\approx 0.34\\); \\(0.35\\) is the more conservative cluster-based estimate.)",
    "latency_ms": 42958,
    "tokens": {
      "input": 111,
      "output": 1155,
      "total": 1266
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:18:53.726Z"
  },
  "t3-cluster-003": {
    "task_id": "t3-cluster-003",
    "tier": 3,
    "question": "Our cluster RCT will have variable cluster sizes (CV = 0.4 in cluster sizes). We have 25 clusters per arm with a mean of 20 subjects per cluster. With ICC = 0.03 and effect size d = 0.3, what power do we achieve at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Compute the variance inflation (design effect) with unequal cluster sizes  \nFor variable cluster sizes with coefficient of variation \\(CV\\), a common approximation is\n\n\\[\nDE \\approx 1+\\rho\\left(\\bar m(1+CV^2)-1\\right)\n\\]\n\nGiven: \\(\\bar m=20\\), \\(CV=0.4\\Rightarrow 1+CV^2=1.16\\), \\(\\rho=ICC=0.03\\)\n\n\\[\nDE = 1+0.03\\left(20\\times 1.16 -1\\right)\n=1+0.03(23.2-1)\n=1+0.03(22.2)\n=1+0.666\n=1.666\n\\]\n\n### Step 2: Effective sample size per arm  \nNominal \\(n\\) per arm:\n\n\\[\nn_{\\text{nom}} = 25\\times 20 = 500\n\\]\n\nEffective \\(n\\) per arm:\n\n\\[\nn_{\\text{eff}} = \\frac{500}{1.666} \\approx 300.1\n\\]\n\n### Step 3: Power for two-sample comparison with standardized effect \\(d=0.3\\)  \nWith equal group sizes, the standardized SE of the difference is:\n\n\\[\nSE = \\sqrt{\\frac{2}{n_{\\text{eff}}}}=\\sqrt{\\frac{2}{300.1}} \\approx 0.0816\n\\]\n\nNoncentrality (z-scale):\n\n\\[\nz_{\\text{alt}} = \\frac{d}{SE} = \\frac{0.3}{0.0816} \\approx 3.68\n\\]\n\nUse two-sided \\(\\alpha=0.05\\). With cluster RCTs, inference is typically based on clusters, so use \\(df=2k-2=48\\), giving \\(t_{0.975,48}\\approx 2.01\\).\n\nApproximate power (normal approximation to noncentral t):\n\n\\[\n\\text{Power} \\approx 1-\\Phi\\left(2.01-3.68\\right)\n=1-\\Phi(-1.67)\n\\approx 1-0.048\n\\approx 0.952\n\\]\n\n---\n\n## **Final answer (power): \\(\\boxed{\\text{approximately }95\\%}\\)**  \nMore precisely, **about \\(\\mathbf{95.2\\%}\\)** power at \\(\\alpha=0.05\\) (two-sided).",
    "latency_ms": 69014,
    "tokens": {
      "input": 129,
      "output": 2879,
      "total": 3008
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:20:04.243Z"
  },
  "t3-cluster-004": {
    "task_id": "t3-cluster-004",
    "tier": 3,
    "question": "We're running a cluster trial with a binary outcome: 30% response in control, 20% expected in treatment. We have 12 clinics per arm and the ICC is 0.02. How many patients per clinic for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Individual-level sample size (ignoring clustering)\n\nTwo-sided test, \\(\\alpha=0.05\\), power \\(=80\\%\\)\n\n- Control \\(p_C=0.30\\)\n- Treatment \\(p_T=0.20\\)\n- Difference \\(\\Delta = 0.10\\)\n- \\(\\bar p=(0.30+0.20)/2=0.25\\)\n\nBecause there are only \\(12\\) clinics per arm, use a \\(t\\)-critical value with df \\(\\approx 2\\times 12-2=22\\):\n- \\(t_{0.975,22}=2.074\\)\n- \\(z_{0.80}=0.84\\)\n\n\\[\nn_{\\text{ind}}=\n\\frac{\\left[t_{0.975,22}\\sqrt{2\\bar p(1-\\bar p)}+z_{0.80}\\sqrt{p_C(1-p_C)+p_T(1-p_T)}\\right]^2}{(p_C-p_T)^2}\n\\]\n\nCompute components:\n\n\\[\n\\sqrt{2\\bar p(1-\\bar p)}=\\sqrt{2(0.25)(0.75)}=\\sqrt{0.375}=0.6124\n\\]\n\\[\n\\sqrt{p_C(1-p_C)+p_T(1-p_T)}=\\sqrt{0.3(0.7)+0.2(0.8)}=\\sqrt{0.37}=0.6083\n\\]\n\n\\[\nn_{\\text{ind}}=\\frac{\\left[2.074(0.6124)+0.84(0.6083)\\right]^2}{0.10^2}\n=\\frac{(1.269+0.511)^2}{0.01}\n=\\frac{(1.780)^2}{0.01}\n=\\frac{3.17}{0.01}\n\\approx 317 \\text{ per arm}\n\\]\n\nSo you need an **effective** sample size of about **317 per arm**.\n\n---\n\n### Step 2: Convert to cluster design (ICC = 0.02, 12 clinics/arm)\n\nWith equal cluster size \\(m\\) and ICC \\(\\rho=0.02\\), design effect:\n\n\\[\nDE = 1+(m-1)\\rho\n\\]\n\nEffective sample size per arm:\n\n\\[\nn_{\\text{eff}}=\\frac{k m}{DE}=\\frac{12m}{1+(m-1)0.02}\n\\]\n\nSet \\(n_{\\text{eff}} = 317\\) and solve for \\(m\\). Using the algebraic solution:\n\n\\[\nm=\\frac{n_{\\text{ind}}(1-\\rho)}{k-n_{\\text{ind}}\\rho}\n\\]\n\n\\[\nm=\\frac{317(0.98)}{12-317(0.02)}\n=\\frac{310.66}{12-6.34}\n=\\frac{310.66}{5.66}\n=54.89\n\\]\n\nRound up:\n\n\\[\nm = 55\n\\]\n\n---\n\n## Final answer\n**You need 55 patients per clinic** (with 12 clinics per arm, ICC \\(=0.02\\), \\(\\alpha=0.05\\), 80% power).\n\n(That’s **660 patients per arm**, **1320 total**.)",
    "latency_ms": 51357,
    "tokens": {
      "input": 120,
      "output": 2248,
      "total": 2368
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:20:57.105Z"
  },
  "t3-cluster-005": {
    "task_id": "t3-cluster-005",
    "tier": 3,
    "question": "We're designing a stepped-wedge trial with 6 clusters over 4 time periods. Two clusters switch to treatment each period. Within-period ICC is 0.05, between-period correlation is 0.025. We want to detect d = 0.5 with at least 80% power. What is the minimum number of subjects per cluster-period?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Translate ICCs to the cluster-period mean covariance\n\nAssume a continuous outcome with marginal variance \\( \\sigma^2=1\\) (since \\(d\\) is standardized).\n\nLet \\(m\\) = subjects per **cluster-period**.\n\n- Within-period ICC: \\(\\rho_w=0.05\\)\n- Between-period correlation (different periods, same cluster): \\(\\rho_b=0.025\\)\n\nFor a given cluster and period \\(t\\), the cluster-period mean \\(\\bar Y_{jt}\\) has\n\n\\[\n\\mathrm{Var}(\\bar Y_{jt})\n=\\frac{1}{m}\\left[1+(m-1)\\rho_w\\right]\n=\\frac{1}{m}\\left[1+0.05(m-1)\\right]\n=0.05+\\frac{0.95}{m}\n\\]\n\nSo define\n\\[\na = 0.05+\\frac{0.95}{m}, \\qquad b=\\rho_b=0.025.\n\\]\n\nThus for the 4 period means within a cluster, the covariance matrix is compound-symmetric with\n- diagonal \\(a\\),\n- off-diagonal \\(b\\).\n\nIt is convenient to define\n\\[\nu=a-b=\\left(0.05+\\frac{0.95}{m}\\right)-0.025=0.025+\\frac{0.95}{m},\n\\]\n\\[\nv=a+3b=\\left(0.05+\\frac{0.95}{m}\\right)+0.075=0.125+\\frac{0.95}{m}.\n\\]\n\n---\n\n### Step 2: Variance of the treatment effect in this stepped-wedge design\n\nDesign: 6 clusters, 4 periods, 2 clusters switch each period → sequences:\n- 2 clusters: \\([0,1,1,1]\\)\n- 2 clusters: \\([0,0,1,1]\\)\n- 2 clusters: \\([0,0,0,1]\\)\n\nUsing GLS for the cluster-period means with fixed period effects and a treatment indicator, the resulting variance of the treatment effect estimator simplifies to:\n\n\\[\n\\mathrm{Var}(\\hat\\beta)= \\frac{3u}{\\,8-\\frac{0.3}{v}\\,}\n\\]\n(with \\(\\sigma^2=1\\)).\n\n---\n\n### Step 3: Power requirement ⇒ required SE\n\nTwo-sided \\(\\alpha=0.05\\), target power \\(=0.80\\).\n\nNormal approximation requirement:\n\\[\n\\frac{d}{\\mathrm{SE}} \\ge z_{0.975}+z_{0.80}=1.96+0.842=2.802\n\\]\nSo\n\\[\n\\mathrm{SE} \\le \\frac{0.5}{2.802}=0.1785,\n\\qquad\n\\mathrm{Var}(\\hat\\beta)\\le 0.1785^2=0.03185.\n\\]\n\n---\n\n### Step 4: Solve for the smallest integer \\(m\\)\n\nCheck \\(m=23\\):\n\n- \\(u=0.025+0.95/23=0.066304\\)\n- \\(v=0.125+0.95/23=0.166304\\)\n- \\(\\displaystyle \\mathrm{Var} = \\frac{3u}{8-0.3/v}\n= \\frac{0.198913}{8-1.804} = \\frac{0.198913}{6.196}=0.03210\\)  (too large)\n\nCheck \\(m=24\\):\n\n- \\(u=0.025+0.95/24=0.064583\\)\n- \\(v=0.125+0.95/24=0.164583\\)\n- \\(\\displaystyle \\mathrm{Var} = \\frac{3u}{8-0.3/v}\n= \\frac{0.19375}{8-1.823}=\\frac{0.19375}{6.177}=0.03137\\)  (meets target)\n\nThis gives \\(\\mathrm{SE}=\\sqrt{0.03137}=0.1771\\), and\n\\[\n\\frac{0.5}{0.1771}=2.823 \\Rightarrow \\text{power}\\approx \\Phi(2.823-1.96)=\\Phi(0.863)=0.806.\n\\]\n\n---\n\n## Final answer (rounded up)\n\n\\[\n\\boxed{m = 24 \\text{ subjects per cluster-period (minimum)}}\n\\]\n\n(That corresponds to total \\(6\\times 4\\times 24 = 576\\) subjects.)",
    "latency_ms": 171040,
    "tokens": {
      "input": 134,
      "output": 8721,
      "total": 8855
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:23:49.662Z"
  },
  "t3-cross-001": {
    "task_id": "t3-cross-001",
    "tier": 3,
    "question": "I'm planning a 2x2 crossover trial where the between-subject effect size is d = 0.5 and the within-subject correlation is 0.7. How many subjects total do I need for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a standard **2×2 crossover** analyzed as a **within-subject (paired) treatment difference**, the treatment effect is tested like a paired \\(t\\)-test on each subject’s within-person difference.\n\n### 1) Convert the “between-subject” standardized effect to the paired standardized effect\nLet the between-subject SD be \\(\\sigma\\). Given Cohen’s \\(d= \\Delta/\\sigma = 0.5\\).\n\nFor a paired difference, the SD of the within-subject difference is\n\\[\n\\sigma_D=\\sqrt{2\\sigma^2(1-\\rho)}=\\sigma\\sqrt{2(1-\\rho)}.\n\\]\nSo the paired standardized effect is\n\\[\nd_{\\text{paired}}=\\frac{\\Delta}{\\sigma_D}\n=\\frac{d\\sigma}{\\sigma\\sqrt{2(1-\\rho)}}\n=\\frac{d}{\\sqrt{2(1-\\rho)}}.\n\\]\nWith \\(\\rho=0.7\\):\n\\[\nd_{\\text{paired}}=\\frac{0.5}{\\sqrt{2(1-0.7)}}=\\frac{0.5}{\\sqrt{0.6}}=\\frac{0.5}{0.7746}=0.6455.\n\\]\n\n### 2) Sample size for paired \\(t\\)-test (two-sided \\(\\alpha=0.05\\), power \\(=0.80\\))\nUsing a \\(t\\)-based critical value approximation:\n\\[\nn \\approx \\left(\\frac{t_{0.975,\\,n-1}+z_{0.80}}{d_{\\text{paired}}}\\right)^2.\n\\]\nCheck \\(n=21\\):\n- \\(df=20\\), \\(t_{0.975,20}\\approx 2.086\\)\n- \\(z_{0.80}=0.842\\)\n\n\\[\nn \\approx \\left(\\frac{2.086+0.842}{0.6455}\\right)^2\n= \\left(\\frac{2.928}{0.6455}\\right)^2\n= (4.536)^2\n= 20.6 \\;\\Rightarrow\\; 21.\n\\]\n\nSo the **minimum** is about **21 subjects** total.\n\nBecause a 2×2 crossover typically assigns subjects equally to the two sequences, you’d usually round up to the next even number.\n\n## Final answer (total subjects)\n**22 subjects total** (i.e., **11 per sequence**) for ~80% power at \\(\\alpha=0.05\\) given \\(d=0.5\\) and within-subject correlation \\(0.7\\).",
    "latency_ms": 99750,
    "tokens": {
      "input": 117,
      "output": 1587,
      "total": 1704
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:25:30.916Z"
  },
  "t3-cross-002": {
    "task_id": "t3-cross-002",
    "tier": 3,
    "question": "I'm planning a simple 2-period crossover trial (AB/BA design) to compare two treatments. I expect a standardized within-subject effect of d = 0.3 (the treatment difference divided by within-subject SD). How many subjects total do I need for 90% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a standard 2×2 crossover (AB/BA) with no carryover and a common within-subject (residual) SD \\( \\sigma_w \\), the treatment effect is estimated from **within-subject differences**. The within-subject treatment difference for each subject has SD\n\n\\[\nSD(\\text{difference})=\\sqrt{2}\\,\\sigma_w\n\\]\n\nGiven the standardized within-subject effect size\n\n\\[\nd=\\frac{\\Delta}{\\sigma_w}=0.3,\n\\]\n\nthe noncentrality parameter for the (approximately) paired \\(t\\)-test is\n\n\\[\n\\text{ncp}=\\frac{\\Delta}{(\\sqrt{2}\\sigma_w/\\sqrt{n})}\n= d\\sqrt{\\frac{n}{2}}.\n\\]\n\nFor 2-sided \\(\\alpha=0.05\\) and power \\(1-\\beta=0.90\\), using the usual normal-based planning equation (very close to the \\(t\\) result at this \\(n\\)):\n\n\\[\nn \\approx 2\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{d}\\right)^2\n=2\\left(\\frac{1.96+1.2816}{0.3}\\right)^2\n=2(10.805)^2\n=233.48.\n\\]\n\nRound up: \\(n=234\\).  \nUsing a \\(t\\) critical value (slightly larger than 1.96) nudges this up by about 1 subject, giving \\(\\approx 235\\).\n\nBecause an AB/BA crossover is typically allocated **1:1** to the two sequences, we round up to the next even number.\n\n## Final answer (total subjects)\n\\[\n\\boxed{236\\text{ subjects total (118 per sequence)}}\n\\]\n\n(Assumes no carryover, complete data, and that \\(d=\\Delta/\\sigma_w\\) uses the within-subject residual SD.)",
    "latency_ms": 53548,
    "tokens": {
      "input": 128,
      "output": 1631,
      "total": 1759
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:26:25.969Z"
  },
  "t3-cross-003": {
    "task_id": "t3-cross-003",
    "tier": 3,
    "question": "I'm designing a 3-treatment Latin square crossover study. The primary comparison is treatment A vs control, with expected effect d = 0.4 and period-to-period correlation of 0.6. How many subjects for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a 3-period Latin square crossover, the A vs control comparison is essentially a **within-subject (paired) comparison** because every subject receives both A and control once. With period-to-period correlation \\(r\\), the variance of the within-subject difference is reduced:\n\n\\[\n\\mathrm{Var}(Y_A-Y_C)=2\\sigma^2(1-r)\n\\]\n\nLet \\(d=\\delta/\\sigma=0.4\\) be the standardized mean difference on the marginal SD scale. Then the required number of subjects for a two-sided test is approximately\n\n\\[\nn \\;=\\; \\frac{\\left(t_{1-\\alpha/2,\\;n-1}+t_{1-\\beta,\\;n-1}\\right)^2\\; 2(1-r)}{d^2}\n\\]\n\nPlug in \\(\\alpha=0.05\\), power \\(=0.80\\Rightarrow \\beta=0.20\\), \\(r=0.6\\), \\(d=0.4\\).\n\nUsing a quick iteration around \\(n\\approx 40\\):\n\n- \\(t_{0.975,41}\\approx 2.02\\)\n- \\(t_{0.80,41}\\approx 0.851\\)\n\n\\[\nn \\approx \\frac{(2.02+0.851)^2\\cdot 2(1-0.6)}{0.4^2}\n= \\frac{(2.871)^2\\cdot 0.8}{0.16}\n= \\frac{8.244\\cdot 0.8}{0.16}\n= \\frac{6.595}{0.16}\n= 41.22\n\\]\n\nRound up: \\(n=42\\). This also conveniently allows equal allocation to the 3 sequences (\\(42/3=14\\) per sequence).\n\n**Final answer: \\(\\boxed{42\\text{ subjects total (14 per sequence)}}\\).**",
    "latency_ms": 78564,
    "tokens": {
      "input": 118,
      "output": 2029,
      "total": 2147
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:27:46.039Z"
  },
  "t3-cross-004": {
    "task_id": "t3-cross-004",
    "tier": 3,
    "question": "We're designing a 4-period replicate crossover study (ABAB/BABA) for a bioequivalence trial. The treatment effect is d = 0.35 (where d = mean difference divided by within-subject SD). How many subjects total for 85% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For an ABAB/BABA 4‑period replicate crossover, each subject has **2 A** and **2 B** observations. If the within‑subject SD for a single observation is \\(\\sigma_w\\), then the subject‑level mean difference is\n\n\\[\nD_i=\\bar Y_{iA}-\\bar Y_{iB}, \\qquad \n\\bar Y_{iA}=\\frac{Y_{iA1}+Y_{iA2}}{2},\\ \\bar Y_{iB}=\\frac{Y_{iB1}+Y_{iB2}}{2}\n\\]\n\nAssuming independent within‑subject residuals with variance \\(\\sigma_w^2\\):\n\n\\[\n\\mathrm{Var}(\\bar Y_{iA})=\\frac{\\sigma_w^2}{2},\\quad\n\\mathrm{Var}(\\bar Y_{iB})=\\frac{\\sigma_w^2}{2}\n\\]\n\nSo\n\n\\[\n\\mathrm{Var}(D_i)=\\frac{\\sigma_w^2}{2}+\\frac{\\sigma_w^2}{2}=\\sigma_w^2\n\\]\n\nThus the SE of the overall mean difference \\(\\bar D\\) across \\(n\\) subjects is\n\n\\[\n\\mathrm{SE}(\\bar D)=\\frac{\\sigma_w}{\\sqrt{n}}\n\\]\n\nGiven the standardized effect size \\(d=\\Delta/\\sigma_w = 0.35\\), the (approximate) required \\(n\\) for a two‑sided \\(\\alpha=0.05\\) test and power \\(1-\\beta=0.85\\) is:\n\n\\[\nn \\approx \\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{d}\\right)^2\n= \\left(\\frac{1.96+1.036}{0.35}\\right)^2\n= \\left(\\frac{2.996}{0.35}\\right)^2\n= 73.3\n\\]\n\nRound up gives \\(n=74\\). Using a t‑critical value (finite df) nudges this to about **75**, and because ABAB/BABA uses **two sequences** (balanced allocation), we round up to the next even total.\n\n## **Final answer (total subjects): \\(\\boxed{76}\\)**  \n(= **38 per sequence**)",
    "latency_ms": 83561,
    "tokens": {
      "input": 124,
      "output": 1509,
      "total": 1633
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:29:11.107Z"
  },
  "t3-fact-001": {
    "task_id": "t3-fact-001",
    "tier": 3,
    "question": "We're running a 2×2 factorial trial where we want to detect a main effect of one factor with d = 0.4. We need 80% power at per-comparison alpha = 0.05 (no multiplicity adjustment). How many subjects per cell?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To test the **main effect** of one factor in a balanced **2×2 factorial**, the main-effect comparison is between the **two marginal means** (level 1 vs level 2 of that factor), pooling over the other factor.\n\nWith **n subjects per cell**, each level of the target factor has **2 cells**, so the effective two-sample comparison has:\n\n- per “group” (factor level): \\(m = 2n\\)\n\nDetectable standardized mean difference (Cohen’s d): \\(d = 0.4\\)\n\n### Step 1: Two-sample t-test approximation for required per-group size\nFor equal-sized groups, an approximate sample size per group is:\n\n\\[\nm \\approx \\frac{2\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{d^2}\n\\]\n\nWith \\(\\alpha=0.05\\) (two-sided) ⇒ \\(z_{1-\\alpha/2}=1.96\\), and power \\(=0.80\\) ⇒ \\(z_{1-\\beta}=0.84\\):\n\n\\[\nm \\approx \\frac{2\\,(1.96+0.84)^2}{0.4^2}\n= \\frac{2\\,(2.80)^2}{0.16}\n= \\frac{2\\cdot 7.84}{0.16}\n= \\frac{15.68}{0.16}\n= 98\n\\]\n\nA small t-critical-value correction pushes this to about **99 per marginal group**, and because \\(m=2n\\) must be even for equal cell sizes, we round to **\\(m=100\\)** per factor level.\n\n### Step 2: Convert to per-cell sample size\n\\[\nm = 2n = 100 \\quad \\Rightarrow \\quad n = 50\n\\]\n\n## Final answer (rounded up)\n**Subjects per cell: 50**  \n(Total N = \\(4 \\times 50 = 200\\))",
    "latency_ms": 65458,
    "tokens": {
      "input": 627,
      "output": 1216,
      "total": 1843
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:30:18.067Z"
  },
  "t3-fact-002": {
    "task_id": "t3-fact-002",
    "tier": 3,
    "question": "In our 2×2 factorial, the main effects are d = 0.5 but we're primarily interested in detecting the interaction (d = 0.3). How many per cell for 80% power on the interaction at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To test the **2×2 interaction** you’re effectively testing the **difference-in-differences** contrast:\n\n\\[\n\\Delta = (\\mu_{11}-\\mu_{10})-(\\mu_{01}-\\mu_{00})\n\\]\n\nWith **balanced n per cell** and common SD \\( \\sigma \\),\n\n\\[\nSE(\\hat\\Delta)=\\sigma\\sqrt{\\frac{1}{n}+\\frac{1}{n}+\\frac{1}{n}+\\frac{1}{n}}=\\frac{2\\sigma}{\\sqrt{n}}\n\\]\n\nIf the interaction effect is expressed as a standardized mean difference\n\n\\[\nd = \\frac{\\Delta}{\\sigma}=0.3\n\\]\n\nthen the noncentrality for the (approx.) z/t test is\n\n\\[\n\\text{ncp} = \\frac{\\Delta}{SE(\\hat\\Delta)}=\\frac{d\\sigma}{2\\sigma/\\sqrt{n}}=\\frac{d\\sqrt{n}}{2}\n\\]\n\nFor 80% power, two-sided \\(\\alpha=0.05\\):\n\n- \\(z_{1-\\alpha/2}=1.96\\)\n- \\(z_{1-\\beta}=z_{0.80}=0.842\\)\n\nRequire:\n\n\\[\n\\frac{d\\sqrt{n}}{2} = z_{1-\\alpha/2}+z_{1-\\beta}\n\\]\n\nSolve for \\(n\\):\n\n\\[\n\\sqrt{n}=\\frac{2(1.96+0.842)}{0.3}=\\frac{2(2.802)}{0.3}=18.68\n\\]\n\\[\nn=(18.68)^2=349.0\n\\]\n\n## Final answer\n**Required sample size: \\(\\boxed{349}\\) per cell** (balanced 2×2).  \nTotal \\(N = 4n = \\boxed{1396}\\).",
    "latency_ms": 42474,
    "tokens": {
      "input": 116,
      "output": 1342,
      "total": 1458
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:31:02.045Z"
  },
  "t3-fact-003": {
    "task_id": "t3-fact-003",
    "tier": 3,
    "question": "We have a 3×2 factorial design (3 doses × 2 formulations) and want to test the linear dose-response trend (effect size f = 0.25). How many total subjects for 80% power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To test the **linear dose-response trend** in a **3×2 between-subjects factorial** (6 cells), the trend test is a **1 df** contrast, so we use an **F test with df1 = 1** (equivalently a 2-sided *t* test).\n\n**Given**\n- Effect size: \\( f = 0.25 \\Rightarrow f^2 = 0.0625 \\)\n- \\(\\alpha = 0.05\\)\n- Power \\(= 0.80\\)\n- Number of cells \\(= 3\\times 2 = 6\\)\n- Denominator df: \\(df_2 = N - 6\\)\n- Noncentrality parameter (fixed-effects ANOVA contrast):  \n  \\[\n  \\lambda = f^2 N = 0.0625\\,N\n  \\]\n  For \\(df_1=1\\), this corresponds to a noncentral-*t* with\n  \\[\n  \\delta = \\sqrt{\\lambda} = f\\sqrt{N} = 0.25\\sqrt{N}.\n  \\]\n\n### Approximate solve\nFor a 2-sided test at \\(\\alpha=0.05\\), with large-ish \\(df_2\\), \\(t_{0.975,df_2}\\approx 1.98\\). Power 0.80 implies approximately:\n\\[\n\\delta \\approx t_{crit} + z_{0.80} \\approx 1.98 + 0.842 = 2.822\n\\]\nSo:\n\\[\n0.25\\sqrt{N} \\approx 2.822 \\;\\Rightarrow\\; \\sqrt{N}\\approx \\frac{2.822}{0.25}=11.288\n\\]\n\\[\nN \\approx 11.288^2 = 127.4\n\\]\nSo you need **about 128 total** subjects.\n\nBecause a 3×2 design is typically allocated **equally across 6 cells**, we round up to the next multiple of 6:\n- \\(128/6 = 21.33\\) per cell (unequal)\n- Next balanced total: \\(22 \\times 6 = 132\\)\n\n---\n\n## Final answer (rounded up, balanced allocation)\n**Total subjects needed: 132 (i.e., 22 per cell) for 80% power at α = 0.05.**",
    "latency_ms": 90112,
    "tokens": {
      "input": 115,
      "output": 1877,
      "total": 1992
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:32:33.661Z"
  },
  "t3-fact-004": {
    "task_id": "t3-fact-004",
    "tier": 3,
    "question": "We're running a 2^4 fractional factorial (resolution IV, 8 runs) with 3 replicates per run. The standardized main effect is delta = 1.5 (in sigma units). What power do we have for detecting main effects at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "For a 2-level factorial main effect (difference between the mean at “+” and the mean at “−”), with balanced data:\n\n- Total observations: \\(N = 8 \\text{ runs}\\times 3 \\text{ reps} = 24\\)\n- Pure error df from replication: \\(\\nu = 8(3-1)=16\\)\n\n### 1) Standard error of a main-effect estimate\nEach main effect compares \\(N/2=12\\) observations at “+” vs 12 at “−”, so\n\n\\[\nSE(\\hat{\\text{effect}})=\\sqrt{\\frac{\\sigma^2}{12}+\\frac{\\sigma^2}{12}}\n=\\sqrt{\\frac{\\sigma^2}{6}}\n=\\frac{\\sigma}{\\sqrt{6}}\n\\approx 0.4082\\,\\sigma\n\\]\n\n### 2) Noncentrality parameter\nGiven standardized main effect \\(\\delta = \\text{effect}/\\sigma = 1.5\\),\n\n\\[\n\\text{ncp}=\\frac{\\text{effect}}{SE}=\\frac{\\delta\\sigma}{\\sigma/\\sqrt{6}}=\\delta\\sqrt{6}\n=1.5\\sqrt{6}\\approx 3.674\n\\]\n\n### 3) Critical value and power\nTwo-sided \\(\\alpha=0.05\\) with \\(\\nu=16\\):\n\n\\[\nt_{0.975,16}\\approx 2.12\n\\]\n\nPower is:\n\n\\[\n\\text{Power}=P\\left(\\left|T\\right|>2.12\\right),\n\\quad T\\sim t_{\\nu=16}(\\text{ncp}=3.674)\n\\]\n\nNumerically this is approximately:\n\n\\[\n\\boxed{\\text{Power} \\approx 0.93}\n\\]\n\n**Final answer (prominent): \\(\\boxed{\\text{Power} \\approx 93\\%}\\)** for detecting a main effect of \\(\\delta=1.5\\) at \\(\\alpha=0.05\\) (two-sided), using pure-error df \\(=16\\).",
    "latency_ms": 147283,
    "tokens": {
      "input": 552,
      "output": 2342,
      "total": 2894
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:35:02.449Z"
  },
  "t3-simr-001": {
    "task_id": "t3-simr-001",
    "tier": 3,
    "question": "I'm planning a mixed-effects study with 5 measurements per subject. The treatment effect is 0.5 units, random intercept SD is 0.8, and residual SD is 1.0. How many subjects per group do I need for 80% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a random-intercept mixed model with 2 equal-sized treatment groups and \\(m=5\\) repeated measurements per subject:\n\n\\[\ny_{ij}=\\beta_0+\\beta_1\\text{(treat)}+b_i+\\varepsilon_{ij},\\quad \nb_i\\sim N(0,\\sigma_b^2),\\ \\varepsilon_{ij}\\sim N(0,\\sigma_e^2)\n\\]\n\nGiven:\n\n- Treatment effect to detect: \\(\\Delta=0.5\\)\n- Random intercept SD \\(=0.8 \\Rightarrow \\sigma_b^2=0.64\\)\n- Residual SD \\(=1.0 \\Rightarrow \\sigma_e^2=1.00\\)\n- Repeats per subject: \\(m=5\\)\n- Target power \\(=80\\%\\) (so \\(z_{1-\\beta}=0.84\\))\n- Two-sided \\(\\alpha=0.05\\) (so \\(z_{1-\\alpha/2}=1.96\\))\n\n### Step 1: Variance of a subject’s mean\nAveraging \\(m\\) measurements reduces only the residual part:\n\n\\[\n\\mathrm{Var}(\\bar y_i)=\\sigma_b^2 + \\frac{\\sigma_e^2}{m}\n=0.64+\\frac{1.00}{5}=0.64+0.20=0.84\n\\]\n\n### Step 2: Variance of the difference in group means\nWith \\(n\\) subjects per group:\n\n\\[\n\\mathrm{Var}(\\hat\\Delta)=\\frac{2(\\sigma_b^2+\\sigma_e^2/m)}{n}\n=\\frac{2(0.84)}{n}\n\\]\n\n### Step 3: Solve for \\(n\\) using normal approximation\n\\[\nn \\;=\\; \\frac{2(\\sigma_b^2+\\sigma_e^2/m)\\left(z_{1-\\alpha/2}+z_{1-\\beta}\\right)^2}{\\Delta^2}\n\\]\n\nPlug in:\n\n\\[\nn=\\frac{2(0.84)(1.96+0.84)^2}{0.5^2}\n=\\frac{1.68\\cdot (2.80)^2}{0.25}\n=\\frac{1.68\\cdot 7.84}{0.25}\n=\\frac{13.1712}{0.25}\n=52.6848\n\\]\n\nRound up:\n\n\\[\nn = 53 \\text{ subjects per group}\n\\]\n\n## Final answer (rounded up)\n**You need \\(\\boxed{53}\\) subjects per group (total \\(\\boxed{106}\\) subjects) for 80% power.**",
    "latency_ms": 36528,
    "tokens": {
      "input": 117,
      "output": 1060,
      "total": 1177
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:35:40.482Z"
  },
  "t3-simr-002": {
    "task_id": "t3-simr-002",
    "tier": 3,
    "question": "I'm powering a study with a binary outcome analyzed using a mixed-effects logistic model. Each subject provides 3 observations, the treatment OR is 2.0, baseline probability is 30%, and the random intercept SD is 0.5. How many subjects per group for 80% power?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assumptions used (needed for a numeric answer): two-sided \\(\\alpha=0.05\\), equal allocation, random-intercept-only correlation (exchangeable within subject), and the stated OR and baseline probability are **conditional** (subject-specific) values.\n\n## 1) Convert OR + baseline probability to treated probability\nBaseline (control) probability: \\(p_0=0.30\\)  \nTreatment OR: \\(\\text{OR}=2.0\\)\n\n\\[\np_1=\\frac{\\text{OR}\\cdot p_0}{1-p_0+\\text{OR}\\cdot p_0}\n=\\frac{2(0.30)}{0.70+2(0.30)}\n=\\frac{0.60}{1.30}=0.4615\n\\]\n\nSo \\(\\Delta = p_1-p_0 = 0.4615-0.30=0.1615\\).\n\n## 2) “Independent-observations” sample size per group (two-sample proportions approximation)\nLet \\(z_{1-\\alpha/2}=1.96\\), \\(z_{1-\\beta}=0.84\\) for 80% power.\n\n\\[\n\\bar p=\\frac{p_0+p_1}{2}=\\frac{0.30+0.4615}{2}=0.3808\n\\]\n\n\\[\nn_{\\text{ind}}=\n\\frac{\\left[z_{1-\\alpha/2}\\sqrt{2\\bar p(1-\\bar p)}+z_{1-\\beta}\\sqrt{p_0(1-p_0)+p_1(1-p_1)}\\right]^2}{(p_1-p_0)^2}\n\\]\n\nCompute pieces:\n\n- \\(\\sqrt{2\\bar p(1-\\bar p)}=\\sqrt{2(0.3808)(0.6192)}=0.6863\\)\n- \\(\\sqrt{p_0(1-p_0)+p_1(1-p_1)}=\\sqrt{0.30(0.70)+0.4615(0.5385)}=0.6769\\)\n\n\\[\nn_{\\text{ind}}=\\frac{(1.96\\cdot 0.6863+0.84\\cdot 0.6769)^2}{0.1615^2}\n=\\frac{(1.344+0.569)^2}{0.02608}\n=\\frac{3.658}{0.02608}=140.3\n\\]\n\nSo about **141 independent observations per group**.\n\n## 3) Inflate for within-subject correlation from the random intercept\nRandom intercept SD \\(=0.5 \\Rightarrow \\sigma_b^2=0.25\\).\n\nFor a logistic random-intercept model, an ICC approximation is:\n\\[\n\\text{ICC}\\approx \\frac{\\sigma_b^2}{\\sigma_b^2+\\pi^2/3}\n=\\frac{0.25}{0.25+3.2899}=0.0706\n\\]\n\nWith \\(m=3\\) observations/subject, the design effect:\n\\[\n\\text{DE}=1+(m-1)\\text{ICC}=1+2(0.0706)=1.1412\n\\]\n\nEffective independent observations per group:\n\\[\nn_{\\text{eff}}=\\frac{n_{\\text{subj}}\\cdot m}{\\text{DE}}\n\\]\n\nSet \\(n_{\\text{eff}} \\approx 141\\):\n\\[\nn_{\\text{subj}}=\\frac{141\\cdot \\text{DE}}{m}\n=\\frac{141\\cdot 1.1412}{3}=53.6\n\\]\n\nRound up:\n\n## **Final answer: 54 subjects per group (≈108 total subjects).**",
    "latency_ms": 71168,
    "tokens": {
      "input": 122,
      "output": 1666,
      "total": 1788
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:36:53.151Z"
  },
  "t3-simr-003": {
    "task_id": "t3-simr-003",
    "tier": 3,
    "question": "I have a random slopes model with 6 time points (0-5). I want to detect a slope difference of 0.1 per time unit between groups. Random intercept SD = 1.0, random slope SD = 0.2, residual SD = 1.0. With 40 subjects per group, what power do I achieve at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Model and target effect\nRandom-intercept/random-slope LMM with times \\(t=0,1,2,3,4,5\\) (6 measurements/subject), 2 groups, \\(n=40\\) per group.\n\nDetect fixed **slope difference** (group×time interaction):\n\\[\n\\Delta = 0.1 \\text{ per time unit}\n\\]\nVariance components (assume random intercept–slope covariance \\(=0\\), since not given):\n- random intercept SD \\(=1.0 \\Rightarrow \\sigma_{0}^{2}=1\\)\n- random slope SD \\(=0.2 \\Rightarrow \\sigma_{1}^{2}=0.04\\)\n- residual SD \\(=1.0 \\Rightarrow \\sigma_e^{2}=1\\)\n\nTwo-sided \\(\\alpha=0.05\\).\n\n---\n\n## 1) Compute the SE of the fixed slope in one group\n\nFor one subject, with fixed-effect design \\(X=[\\mathbf{1},\\,\\mathbf{t}]\\) and \\(\\mathbf{t}=(0,1,2,3,4,5)^\\top\\),\n\n\\[\nV = \\sigma_e^2 I + \\sigma_0^2 \\mathbf{1}\\mathbf{1}^\\top + \\sigma_1^2 \\mathbf{t}\\mathbf{t}^\\top\n= I + \\mathbf{1}\\mathbf{1}^\\top + 0.04\\,\\mathbf{t}\\mathbf{t}^\\top\n\\]\n\nLet \\(U=[\\mathbf{1},\\mathbf{t}]\\) and \\(C=\\mathrm{diag}(1,0.04)\\). Then \\(V=I+UCU^\\top\\), and using Woodbury:\n\\[\nV^{-1} = I - U\\left(C^{-1}+U^\\top U\\right)^{-1}U^\\top\n\\]\n\nCompute needed cross-products:\n\\[\nU^\\top U =\n\\begin{pmatrix}\n\\sum 1 & \\sum t\\\\\n\\sum t & \\sum t^2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 & 15\\\\\n15 & 55\n\\end{pmatrix}\n\\]\n\\[\nC^{-1}=\\mathrm{diag}(1,25)\n\\Rightarrow\nC^{-1}+U^\\top U=\n\\begin{pmatrix}\n7 & 15\\\\\n15 & 80\n\\end{pmatrix}\n\\]\nInverse:\n\\[\n\\left(C^{-1}+U^\\top U\\right)^{-1}\n=\\frac{1}{335}\n\\begin{pmatrix}\n80 & -15\\\\\n-15 & 7\n\\end{pmatrix}\n\\]\n\nThe per-subject fixed-effect information is:\n\\[\nM = X^\\top V^{-1} X = U^\\top V^{-1}U\n=\n\\begin{pmatrix}\n51/67 & 75/67\\\\\n75/67 & 800/67\n\\end{pmatrix}\n\\]\n\nWith \\(n=40\\) subjects in that group:\n\\[\n\\mathrm{Var}(\\hat\\beta_{\\text{slope}})=\\left(nM\\right)^{-1}\n=\\frac{1}{n}M^{-1}\n\\]\nThe \\((2,2)\\) element of \\(M^{-1}\\) is:\n\\[\n(M^{-1})_{22}=\\frac{3417}{35175}=0.097159\n\\]\nSo:\n\\[\n\\mathrm{Var}(\\hat\\beta_{\\text{slope}})=\\frac{0.097159}{40}=0.00242898\n\\]\n\\[\nSE(\\hat\\beta_{\\text{slope}})=\\sqrt{0.00242898}=0.04928\n\\]\n\n---\n\n## 2) SE for the *difference* in slopes between two independent groups\nWith equal \\(n\\) and independent groups:\n\\[\n\\mathrm{Var}(\\widehat{\\Delta}) = 2\\,\\mathrm{Var}(\\hat\\beta_{\\text{slope}})\n=2(0.00242898)=0.00485796\n\\]\n\\[\nSE(\\widehat{\\Delta})=\\sqrt{0.00485796}=0.06969\n\\]\n\nNoncentrality (Wald z approximation):\n\\[\n\\lambda=\\frac{\\Delta}{SE}=\\frac{0.1}{0.06969}=1.435\n\\]\n\n---\n\n## 3) Power at two-sided \\(\\alpha=0.05\\)\nCritical value \\(z_{0.975}=1.96\\). Power:\n\\[\n\\text{Power} = P(|Z|>1.96 \\mid Z\\sim N(\\lambda,1))\n= \\Phi(-1.96-\\lambda)+1-\\Phi(1.96-\\lambda)\n\\]\n\\[\n= \\Phi(-3.395) + 1-\\Phi(0.525)\n\\approx 0.00034 + (1-0.700)\n\\approx 0.300\n\\]\n\n---\n\n# **Final answer (prominent): Power ≈ 0.30 (30%)**\nWith **40 subjects per group**, 6 time points (0–5), and the given variance components, the achieved power to detect a **slope difference of 0.1** at **α=0.05 (two-sided)** is approximately **30%**.",
    "latency_ms": 134620,
    "tokens": {
      "input": 142,
      "output": 5229,
      "total": 5371
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:39:09.278Z"
  },
  "t3-simr-004": {
    "task_id": "t3-simr-004",
    "tier": 3,
    "question": "We have a three-level educational study: students in classrooms in schools. Per arm: 10 schools, 4 classrooms per school, 25 students per classroom. Effect size d = 0.3, school variance = 0.05, classroom variance = 0.10, residual = 0.85. What power does this design achieve at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Assume a **2-arm cluster-randomized trial with randomization at the school level** (since you gave “per arm: 10 schools …”).\n\n### Given\n- Per arm: \\(J=10\\) schools  \n- Per school: \\(m=4\\) classrooms  \n- Per classroom: \\(n=25\\) students  \n- Total per school: \\(mn=100\\)\n- Standardized effect size: \\(d=\\Delta/\\sigma = 0.3\\) with \\(\\sigma^2=1\\)\n- Variance components (sum to 1):\n  - School: \\(\\sigma^2_S = 0.05\\)\n  - Classroom within school: \\(\\sigma^2_C = 0.10\\)\n  - Residual: \\(\\sigma^2_E = 0.85\\)\n- Two-sided \\(\\alpha=0.05\\)\n\n### 1) Variance of a **school mean**\nFor a school mean averaging over all \\(mn\\) students:\n\\[\n\\mathrm{Var}(\\bar Y_{\\text{school}})=\n\\sigma^2_S + \\frac{\\sigma^2_C}{m} + \\frac{\\sigma^2_E}{mn}\n\\]\n\\[\n=0.05 + \\frac{0.10}{4} + \\frac{0.85}{100}\n=0.05+0.025+0.0085\n=0.0835\n\\]\n\n### 2) SE of the treatment effect (difference in arm means)\nWith \\(J=10\\) schools per arm:\n\\[\n\\mathrm{SE}(\\hat\\Delta)=\\sqrt{0.0835\\left(\\frac{1}{10}+\\frac{1}{10}\\right)}\n=\\sqrt{0.0835\\cdot 0.2}\n=\\sqrt{0.0167}\n=0.1292\n\\]\n\n### 3) Noncentrality parameter\n\\[\n\\Delta = d \\cdot \\sigma = 0.3\n\\]\n\\[\n\\text{ncp}=\\frac{\\Delta}{\\mathrm{SE}}=\\frac{0.3}{0.1292}=2.323\n\\]\n\n### 4) Power (two-sided test)\nUse a cluster-level \\(t\\) test with\n\\[\n\\text{df}=2J-2=18,\\quad t_{0.975,18}\\approx 2.101\n\\]\nApproximate power via normal approximation to the noncentral \\(t\\):\n\\[\n\\text{Power} \\approx P\\left(Z>t_{\\text{crit}}-\\text{ncp}\\right)\n= P\\left(Z>2.101-2.323\\right)\n= P(Z>-0.222)\n\\approx 0.588\n\\]\n\n## Final answer\n**Power \\(\\approx 0.59\\) (about 59%) at \\(\\alpha=0.05\\)** for **10 schools per arm**, with 4 classes/school and 25 students/class, given the stated variance components and \\(d=0.3\\).\n\n(If randomization were at the **classroom** level instead of school level, power would be substantially higher; tell me if that’s the intended design.)",
    "latency_ms": 87715,
    "tokens": {
      "input": 141,
      "output": 2170,
      "total": 2311
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:40:38.500Z"
  },
  "t3-simr-005": {
    "task_id": "t3-simr-005",
    "tier": 3,
    "question": "I'm analyzing clustered survival data with gamma frailty. We have 20 clusters per arm, 15 subjects per cluster, frailty variance = 0.5. We want to detect HR = 0.7 over 2-year follow-up (baseline 2-year survival = 60%). Using a standard Cox model without frailty, what is the marginal power at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Total sample size and (marginal) 2-year event probabilities\n- Clusters/arm = 20  \n- Subjects/cluster = 15  → subjects/arm \\(=20\\times 15=300\\)  \n- Total \\(N=600\\)\n\nGiven **marginal** 2-year survival in control \\(S_C(2)=0.60\\), the 2-year event probability in control is\n\\[\n\\Pr(\\text{event by 2y} \\mid C)=1-0.60=0.40.\n\\]\n\nUnder a **gamma frailty** model with variance \\(\\theta=0.5\\), the marginal survival is\n\\[\nS(t)=\\left(1+\\theta \\Lambda(t)\\right)^{-1/\\theta}.\n\\]\nCalibrate the (conditional) cumulative baseline hazard at 2y, \\(\\Lambda_0(2)\\), so that the *marginal* control survival is 0.60:\n\\[\n0.60=(1+0.5\\,\\Lambda_0(2))^{-2}\n\\Rightarrow 1+0.5\\Lambda_0(2)=0.60^{-1/2}=1.290\n\\Rightarrow \\Lambda_0(2)=0.580.\n\\]\n\nWith **conditional HR** \\(\\psi=0.7\\), the treatment cumulative hazard is \\(\\psi\\Lambda_0(2)=0.406\\), so treatment marginal survival at 2y is\n\\[\nS_T(2)=\\left(1+0.5\\times 0.406\\right)^{-2}=(1.203)^{-2}=0.690,\n\\]\nthus\n\\[\n\\Pr(\\text{event by 2y} \\mid T)=1-0.690=0.310.\n\\]\n\nExpected total events:\n\\[\nD = 300(0.40)+300(0.310)=120+93.0=213.0.\n\\]\n\n---\n\n### Step 2: Account for clustering via an ICC for the 2-year event indicator\nFor two subjects in the same cluster (shared frailty), let \\(Z=I(T\\le 2)\\). Using the Laplace transform of the gamma frailty,\n\\[\nS=\\Pr(Z=0)=(1+\\theta\\Lambda)^{-1/\\theta},\\quad\nS_2=\\Pr(Z_1=0,Z_2=0)=(1+2\\theta\\Lambda)^{-1/\\theta}.\n\\]\nThen\n\\[\nE[Z_1Z_2]=1-2S+S_2,\\quad\n\\rho=\\frac{\\text{Cov}(Z_1,Z_2)}{\\text{Var}(Z)}.\n\\]\n\n**Control arm** (\\(\\Lambda=0.580\\)):\n- \\(S=0.60\\)\n- \\(S_2=(1+2\\cdot 0.5\\cdot 0.580)^{-2}=(1.58)^{-2}=0.4006\\)\n- \\(p=0.40\\)\n- \\(E[Z_1Z_2]=1-2(0.60)+0.4006=0.2006\\)\n- \\(\\text{Cov}=0.2006-0.40^2=0.0406\\)\n- \\(\\text{Var}=0.40(0.60)=0.24\\)\n- \\(\\rho_C=0.0406/0.24=0.169\\)\n\n**Treatment arm** (\\(\\Lambda=0.406\\)):\n- \\(S=0.690\\)\n- \\(S_2=(1+2\\cdot 0.5\\cdot 0.406)^{-2}=(1.406)^{-2}=0.506\\)\n- \\(p=0.310\\)\n- \\(E[Z_1Z_2]=1-2(0.690)+0.506=0.126\\)\n- \\(\\text{Cov}=0.126-0.310^2=0.0299\\)\n- \\(\\text{Var}=0.310(0.690)=0.2139\\)\n- \\(\\rho_T=0.0299/0.2139=0.140\\)\n\nEvent-weighted average ICC:\n\\[\n\\bar\\rho=\\frac{120\\rho_C+93\\rho_T}{213}\n=\\frac{120(0.169)+93(0.140)}{213}\\approx 0.156.\n\\]\n\nDesign effect (cluster size \\(m=15\\)):\n\\[\nDE = 1+(m-1)\\bar\\rho = 1+14(0.156)=3.18.\n\\]\n\nEffective number of events:\n\\[\nD_{\\text{eff}}=\\frac{D}{DE}=\\frac{213.0}{3.18}=67.0.\n\\]\n\n---\n\n### Step 3: Marginal (attenuated) effect size under gamma frailty, then Schoenfeld power\nUnder gamma frailty, the **marginal hazard ratio is attenuated and time-varying**:\n\\[\nHR_{\\text{marg}}(t)=\\psi\\frac{1+\\theta\\Lambda_0(t)}{1+\\theta\\psi\\Lambda_0(t)}.\n\\]\nApproximate \\(\\Lambda_0(t)\\) as linear in time (constant baseline hazard) so \\(\\Lambda_0(2)=0.580 \\Rightarrow \\Lambda_0(t)=0.29t\\). A reasonable “information time” is around when about half the control events have occurred; here that’s around \\(t\\approx 0.8\\) y, giving \\(\\Lambda_0(0.8)\\approx 0.232\\). Then\n\\[\nHR_{\\text{marg}}(0.8)\\approx 0.7\\frac{1+0.5(0.232)}{1+0.5(0.7)(0.232)}\n=0.7\\frac{1.116}{1.081}=0.723,\n\\]\nso \\(|\\log(HR_{\\text{marg}})|=|\\log(0.723)|=0.325.\\)\n\nUsing Schoenfeld/logrank approximation with equal allocation \\(p=0.5\\):\n\\[\n\\text{NCP}=\\sqrt{D_{\\text{eff}}\\,p(1-p)}\\,|\\log(HR_{\\text{marg}})|\n=\\sqrt{67.0\\times 0.25}\\times 0.325\n=4.09\\times 0.325\n=1.33.\n\\]\nTwo-sided \\(\\alpha=0.05\\Rightarrow z_{0.975}=1.96\\). Power:\n\\[\n\\text{Power}\\approx \\Phi(\\text{NCP}-1.96)=\\Phi(1.33-1.96)=\\Phi(-0.63)=0.26.\n\\]\n\n---\n\n## **Final answer (marginal power at \\(\\alpha=0.05\\))**\n\\[\n\\boxed{\\text{Power} \\approx 0.26 \\text{ (about }26\\%\\text{)}}\n\\]\n\n(Approximation based on Schoenfeld events, cluster design effect from frailty-induced ICC, and marginal HR attenuation under gamma frailty.)",
    "latency_ms": 147268,
    "tokens": {
      "input": 143,
      "output": 6743,
      "total": 6886
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:43:07.271Z"
  },
  "t3-simr-006": {
    "task_id": "t3-simr-006",
    "tier": 3,
    "question": "I'm analyzing a multivariate mixed model with 2 correlated outcomes (r = 0.6). Treatment effect is 0.4 on the primary outcome and 0.3 on secondary. Each subject has 4 timepoints, random intercept SD = 0.5, residual SD = 1.0. How many subjects per group for 80% power on the primary outcome?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Reduce the 4 repeated measures to the information they contribute (random-intercept model)\n\nFor one outcome with a random intercept \\(b_i \\sim N(0,\\sigma_b^2)\\) and residuals \\(e_{it}\\sim N(0,\\sigma_e^2)\\), each measurement is\n\n\\[\nY_{it} = \\mu + \\text{(treatment effect)} + b_i + e_{it}.\n\\]\n\nWith 4 timepoints per subject, the subject-specific mean \\(\\bar Y_i\\) has variance\n\n\\[\n\\mathrm{Var}(\\bar Y_i)=\\sigma_b^2 + \\frac{\\sigma_e^2}{m},\n\\quad m=4.\n\\]\n\nGiven:\n- random intercept SD \\(=0.5 \\Rightarrow \\sigma_b^2=0.25\\)\n- residual SD \\(=1.0 \\Rightarrow \\sigma_e^2=1.00\\)\n\n\\[\n\\mathrm{Var}(\\bar Y_i)=0.25+\\frac{1.00}{4}=0.25+0.25=0.50.\n\\]\n\nSo the SD of the subject mean is:\n\n\\[\n\\sigma_{\\bar Y}=\\sqrt{0.50}=0.7071.\n\\]\n\n---\n\n### Step 2: Two-group comparison on the primary outcome\n\nPrimary treatment effect (difference in means) is \\(\\delta=0.4\\).\n\nUsing a two-sided \\(\\alpha=0.05\\) and power \\(1-\\beta=0.80\\):\n\n\\[\nn \\;=\\; \\frac{2\\,\\sigma_{\\bar Y}^2\\,(z_{1-\\alpha/2}+z_{1-\\beta})^2}{\\delta^2}.\n\\]\n\nCritical values:\n- \\(z_{1-\\alpha/2}=z_{0.975}=1.96\\)\n- \\(z_{1-\\beta}=z_{0.80}=0.842\\)\n\nPlug in:\n\n\\[\nn=\\frac{2(0.50)(1.96+0.842)^2}{0.4^2}\n= \\frac{1.0 \\times (2.802)^2}{0.16}\n= \\frac{7.85}{0.16}\n= 49.06.\n\\]\n\nRound up:\n\n\\[\nn = 50 \\text{ subjects per group.}\n\\]\n\n---\n\n### Note on the secondary outcome correlation\nEven though the two outcomes are correlated (\\(r=0.6\\)), **if you are powering for the primary outcome’s treatment effect without imposing constraints that link the two outcomes’ effects**, the secondary outcome generally does **not** reduce the standard error of the primary outcome’s treatment effect estimate (it mainly induces correlation between the two effect estimates).\n\n---\n\n## **Final answer: 50 subjects per group (for 80% power on the primary outcome).**",
    "latency_ms": 67975,
    "tokens": {
      "input": 141,
      "output": 1668,
      "total": 1809
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:44:16.751Z"
  },
  "t3-simr-007": {
    "task_id": "t3-simr-007",
    "tier": 3,
    "question": "In our cluster trial, we have 15 clusters per arm with 20 subjects each, measured at 4 time points (0,1,2,3). The treatment effect on the slope is 0.15 per time unit. Random intercept SD = 0.5, random slope SD = 0.1, residual SD = 1.0. What power for the time × treatment interaction at alpha = 0.05?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Work on the cluster-time means\nAt each time point you have **20 subjects/cluster**, so the variance of a **cluster mean** is\n\n\\[\n\\sigma^2_{\\bar y}=\\frac{\\sigma_e^2}{20}=\\frac{1^2}{20}=0.05\n\\]\n\n### Step 2: Variance of a within-cluster slope estimate (4 time points: 0,1,2,3)\nFor times \\(t=\\{0,1,2,3\\}\\), the usual\n\\[\nS_{xx}=\\sum (t-\\bar t)^2\n\\]\nhas \\(\\bar t=1.5\\) and\n\n\\[\nS_{xx}=(0-1.5)^2+(1-1.5)^2+(2-1.5)^2+(3-1.5)^2\n=2.25+0.25+0.25+2.25=5\n\\]\n\nSo the sampling variance of the estimated slope within a cluster (from the 4 cluster means) is\n\n\\[\n\\mathrm{Var}(\\hat b_{\\text{slope}} \\mid b_{\\text{true}})=\\frac{\\sigma^2_{\\bar y}}{S_{xx}}\n=\\frac{0.05}{5}=0.01\n\\]\n\n### Step 3: Add the random slope variation between clusters\nRandom slope SD \\(=0.1 \\Rightarrow \\sigma^2_{b1}=0.1^2=0.01\\).\n\nThus the variance of an observed cluster slope estimate is approximately\n\n\\[\n\\mathrm{Var}(\\hat b_{\\text{slope}})=0.01+0.01=0.02\n\\]\n\n*(Note: the random intercept SD = 0.5 does not contribute to slope variance for this balanced equally-spaced time design.)*\n\n### Step 4: SE for the treatment × time interaction (difference in mean slopes)\nEach arm has \\(m=15\\) clusters, so\n\n\\[\n\\mathrm{Var}(\\bar b_{\\text{slope, arm}})=\\frac{0.02}{15}=0.001333\n\\]\n\nDifference in slopes (treatment − control):\n\n\\[\n\\mathrm{Var}(\\widehat{\\Delta b}) = 2 \\times 0.001333 = 0.002667\n\\]\n\\[\nSE(\\widehat{\\Delta b})=\\sqrt{0.002667}=0.05164\n\\]\n\n### Step 5: Power for effect = 0.15 at α = 0.05 (two-sided)\n\\[\nZ = \\frac{0.15}{0.05164}=2.905\n\\]\n\nUsing a normal critical value \\(z_{0.975}=1.96\\),\n\n\\[\n\\text{Power} \\approx P(|N(2.905,1)|>1.96)\n= 1-\\Phi(1.96-2.905)\n=1-\\Phi(-0.945)=\\Phi(0.945)=0.827\n\\]\n\n---\n\n## **Final answer: Power ≈ 0.83 (82.7%)** at α = 0.05 (two-sided).\n\n(If you instead use a small-sample \\(t\\) critical value with df ≈ \\(2m-2=28\\), power is slightly lower, about **0.80**.)",
    "latency_ms": 175731,
    "tokens": {
      "input": 152,
      "output": 7651,
      "total": 7803
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:47:13.983Z"
  },
  "t4-binary-001": {
    "task_id": "t4-binary-001",
    "tier": 4,
    "question": "We're developing a prediction model for liver cirrhosis using logistic regression. We have 24 candidate predictors and expect about 17% of patients will have cirrhosis. Based on similar models, we anticipate a Cox-Snell R² of about 0.288. What's the minimum sample size needed for model development?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using the Riley et al. (2019) / **pmsampsize** approach for logistic regression model development, one key requirement is a **global shrinkage factor** \\(S\\) (typically \\(S \\ge 0.90\\)) to limit overfitting.\n\n### Inputs\n- Candidate predictor parameters: \\(p = 24\\)\n- Outcome proportion (cirrhosis): \\(p_y = 0.17\\)\n- Anticipated Cox–Snell \\(R^2\\): \\(R^2_{CS} = 0.288\\)\n- Target shrinkage: \\(S = 0.90\\)\n\n### Criterion: shrinkage \\(S \\ge 0.90\\)\nFor logistic regression,\n\\[\n\\chi^2 = -n \\ln(1 - R^2_{CS})\n\\]\nand the approximate global shrinkage is\n\\[\nS \\approx \\frac{\\chi^2 - p}{\\chi^2} = 1 - \\frac{p}{\\chi^2}\n\\]\nRequire \\(S \\ge 0.90\\):\n\\[\n1 - \\frac{p}{\\chi^2} \\ge 0.90 \\;\\Rightarrow\\; \\frac{p}{\\chi^2} \\le 0.10 \\;\\Rightarrow\\; \\chi^2 \\ge 10p\n\\]\nSo,\n\\[\n-n \\ln(1 - R^2_{CS}) \\ge 10p\n\\Rightarrow\nn \\ge \\frac{10p}{-\\ln(1 - R^2_{CS})}\n\\]\n\nCompute:\n\\[\n-\\ln(1 - 0.288) = -\\ln(0.712) = 0.33933\n\\]\n\\[\nn \\ge \\frac{10 \\times 24}{0.33933} = \\frac{240}{0.33933} = 707.2\n\\]\n\nRound up:\n\n## **Minimum required sample size: \\(\\boxed{708}\\)**\n\n(That corresponds to expected events \\(=0.17\\times 708 \\approx 120\\) cirrhosis cases.)",
    "latency_ms": 152313,
    "tokens": {
      "input": 128,
      "output": 2708,
      "total": 2836
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:49:47.800Z"
  },
  "t4-binary-002": {
    "task_id": "t4-binary-002",
    "tier": 4,
    "question": "We're building a prediction model for hospital readmission with 24 candidate predictors. Outcome prevalence is about 17.4%, and we anticipate the model will have a Nagelkerke R-squared of 0.36. What sample size is needed using Riley criteria?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al. (pmsampsize) criteria for a **binary logistic prediction model** with:\n\n- Candidate predictors \\(p = 24\\)  \n- Outcome prevalence \\(\\pi = 0.174\\)  \n- Anticipated **Nagelkerke** \\(R^2_N = 0.36\\)\n\nwe first convert \\(R^2_N\\) to **Cox–Snell** \\(R^2_{CS}\\), then check the 3 Riley minimum-sample-size criteria and take the maximum.\n\n---\n\n## 1) Convert Nagelkerke \\(R^2\\) to Cox–Snell \\(R^2\\)\n\nCompute the maximum Cox–Snell \\(R^2\\) given prevalence \\(\\pi\\):\n\n\\[\n\\frac{LL_0}{n}=\\pi\\ln(\\pi)+(1-\\pi)\\ln(1-\\pi)\n\\]\n\n\\[\n\\ln(0.174)=-1.747,\\quad \\ln(0.826)=-0.191\n\\]\n\\[\n\\frac{LL_0}{n}=0.174(-1.747)+0.826(-0.191)=-0.4616\n\\]\n\n\\[\nR^2_{CS,\\max}=1-\\exp\\left(\\frac{2LL_0}{n}\\right)=1-\\exp(2\\times -0.4616)\n=1-\\exp(-0.9232)=1-0.397=0.603\n\\]\n\nConvert:\n\n\\[\nR^2_{CS}=R^2_N \\times R^2_{CS,\\max}=0.36\\times 0.603=0.217\n\\]\n\n---\n\n## 2) Criterion 1: Shrinkage factor \\(S \\ge 0.90\\)\n\nRiley’s shrinkage-based requirement implies \\(LR \\ge 10p\\), and with\n\\[\nR^2_{CS}=1-\\exp(-LR/n)\\Rightarrow LR=-n\\ln(1-R^2_{CS})\n\\]\n\nSo\n\\[\nn \\ge \\frac{10p}{-\\ln(1-R^2_{CS})}\n\\]\n\n\\[\n-\\ln(1-0.217)=-\\ln(0.783)=0.2447\n\\]\n\\[\nn_1 \\ge \\frac{240}{0.2447}=981.0 \\Rightarrow 982\n\\]\n\n---\n\n## 3) Criterion 2: Small optimism in \\(R^2\\) (default \\(\\le 0.05\\) on Nagelkerke scale)\n\nUsing the standard Riley/pmsampsize approximation:\n\n\\[\n\\text{Optimism}(R^2_N)=\\frac{(1-R^2_{CS})\\left(e^{p/n}-1\\right)}{R^2_{CS,\\max}} \\le 0.05\n\\]\n\nSolve:\n\n\\[\ne^{24/n}-1 \\le 0.05\\cdot\\frac{0.603}{0.783}=0.03852\n\\]\n\\[\n\\frac{24}{n}\\le \\ln(1.03852)=0.03780 \\Rightarrow n_2\\ge \\frac{24}{0.03780}=635.4\\Rightarrow 636\n\\]\n\n---\n\n## 4) Criterion 3: Precise estimation of the overall outcome risk (not limiting here)\n\nWith a typical ±0.05 absolute precision for prevalence:\n\\[\nn_3 \\approx \\frac{1.96^2\\,\\pi(1-\\pi)}{0.05^2}\n= \\frac{3.8416\\times 0.174\\times 0.826}{0.0025}=220.9\\Rightarrow 221\n\\]\n\n---\n\n## Required sample size (take the maximum)\n\n\\[\nn=\\max(n_1,n_2,n_3)=\\max(982,636,221)=\\boxed{982}\n\\]\n\nThis corresponds to expected:\n- Events: \\(982\\times 0.174 = 171\\)  \n- Non-events: \\(982-171 = 811\\)\n\n---\n\n# Final answer: **\\(\\boxed{n = 982}\\)** total patients (≈ **171 readmissions**).",
    "latency_ms": 134903,
    "tokens": {
      "input": 116,
      "output": 3540,
      "total": 3656
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:52:04.207Z"
  },
  "t4-binary-003": {
    "task_id": "t4-binary-003",
    "tier": 4,
    "question": "I'm developing a prediction model for postoperative complications, which occur in only 5% of patients. We plan to include 12 predictors and expect a C-statistic of 0.80. Using Riley criteria with default shrinkage (0.90), what sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s **minimum sample size** criterion based on a **global shrinkage factor** \\(S=0.90\\) for a logistic prediction model:\n\n### 1) Convert anticipated C-statistic to an anticipated Cox–Snell \\(R^2_{CS}\\)\n\nGiven:\n- Outcome prevalence \\(p_o = 0.05\\)\n- Predictors (parameters) \\(p = 12\\)\n- Anticipated C-statistic \\(C = 0.80\\)\n\n**Step 1: Convert \\(C\\) to the discrimination slope \\(D\\)** (Riley/pmsampsize approach):\n\\[\nD=\\sqrt{2}\\,\\Phi^{-1}(C)\n\\]\n\\[\n\\Phi^{-1}(0.80)=0.8416 \\;\\Rightarrow\\; D=1.4142\\times 0.8416=1.1902\n\\]\n\n**Step 2: Compute \\(R^2_D\\)**\n\\[\nR^2_D=\\frac{D^2}{D^2+\\pi^2/3}\n\\]\n\\[\nD^2=1.1902^2=1.4167,\\quad \\pi^2/3=3.2899\n\\]\n\\[\nR^2_D=\\frac{1.4167}{1.4167+3.2899}=0.3010\n\\]\n\n**Step 3: Compute maximum possible Cox–Snell \\(R^2_{CS,\\max}\\)** for outcome proportion \\(p_o\\):\n\\[\nR^2_{CS,\\max}=1-\\exp\\Big(2\\,[p_o\\ln(p_o)+(1-p_o)\\ln(1-p_o)]\\Big)\n\\]\n\\[\n2\\,[0.05\\ln(0.05)+0.95\\ln(0.95)] = -0.3970\n\\]\n\\[\nR^2_{CS,\\max}=1-e^{-0.3970}=0.3276\n\\]\n\n**Step 4: Anticipated \\(R^2_{CS}\\)**\n\\[\nR^2_{CS}=R^2_D \\times R^2_{CS,\\max}=0.3010\\times 0.3276=0.0986\n\\]\n\n---\n\n### 2) Apply Riley shrinkage criterion \\(S \\ge 0.90\\)\n\nFor shrinkage:\n\\[\nS = 1 - \\frac{p}{\\chi^2} \\ge 0.90\n\\;\\Rightarrow\\;\n\\chi^2 \\ge \\frac{p}{1-S} = \\frac{12}{0.10}=120\n\\]\n\nLink \\(\\chi^2\\) to \\(R^2_{CS}\\):\n\\[\nR^2_{CS}=1-\\exp\\left(-\\frac{\\chi^2}{n}\\right)\n\\;\\Rightarrow\\;\n\\chi^2 = -n\\ln(1-R^2_{CS})\n\\]\n\nSo required \\(n\\):\n\\[\n-n\\ln(1-R^2_{CS}) \\ge 120\n\\;\\Rightarrow\\;\nn \\ge \\frac{120}{-\\ln(1-0.0986)}\n\\]\n\\[\n-\\ln(0.9014)=0.1038\n\\]\n\\[\nn \\ge \\frac{120}{0.1038}=1155.7\n\\]\n\nRound up:\n\n## Required sample size: **n = 1156 patients**\n\nThis corresponds to expected events:\n\\[\n0.05\\times 1156 \\approx 57.8 \\Rightarrow \\textbf{about 58 events}\n\\]",
    "latency_ms": 255963,
    "tokens": {
      "input": 118,
      "output": 4452,
      "total": 4570
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:56:21.675Z"
  },
  "t4-binary-004": {
    "task_id": "t4-binary-004",
    "tier": 4,
    "question": "We're developing a high-dimensional prediction model with 25 predictors (after variable selection), outcome prevalence 12%, and expected C-statistic of 0.82. We want to be more conservative with a shrinkage target of 0.95. What sample size is required?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Step 1: Shrinkage requirement → required model LR statistic\nFor global shrinkage target \\(S\\), the (approximate) required likelihood ratio (LR) statistic is\n\n\\[\nS \\approx 1-\\frac{p}{LR}\\quad \\Rightarrow\\quad LR=\\frac{p}{1-S}\n\\]\n\nWith \\(p=25\\) predictors and \\(S=0.95\\):\n\n\\[\nLR=\\frac{25}{1-0.95}=\\frac{25}{0.05}=500\n\\]\n\n---\n\n### Step 2: Convert expected C-statistic to a Cox–Snell \\(R^2\\) (via normal LP approximation)\n\nUsing the common approximation that the linear predictor (LP) is approximately normal and that\n\n\\[\nC = \\Phi\\!\\left(\\frac{\\sigma}{\\sqrt{2}}\\right)\\quad \\Rightarrow\\quad \n\\sigma = \\sqrt{2}\\,\\Phi^{-1}(C)\n\\]\n\nFor \\(C=0.82\\), \\(\\Phi^{-1}(0.82)\\approx 0.915\\), so:\n\n\\[\n\\sigma \\approx \\sqrt{2}\\times 0.915 = 1.294,\\qquad \\sigma^2\\approx 1.674\n\\]\n\nLet prevalence \\(\\pi=0.12\\). Then:\n\n\\[\n\\text{logit}(\\pi)=\\ln\\left(\\frac{0.12}{0.88}\\right)\\approx -1.991\n\\]\n\\[\n\\mu_0=\\text{logit}(\\pi)-\\frac{\\sigma^2}{2}=-1.991-0.837=-2.828\n\\]\n\\[\n\\mu_1=\\mu_0+\\sigma^2=-2.828+1.674=-1.154\n\\]\n\nApproximate the expected per-observation LR contribution (via expected log-likelihood improvement over the null). Using numerical integration (Gauss–Hermite) gives:\n\n- \\(E[\\log(1+e^{LP})] \\approx 0.1487\\)\n- \\(E[y\\cdot LP]=\\pi\\,\\mu_1=0.12(-1.154)=-0.1385\\)\n\nSo:\n\n\\[\n\\frac{LL_{\\text{model}}}{N} \\approx -0.1385-0.1487=-0.2872\n\\]\n\nNull log-likelihood per observation:\n\n\\[\n\\frac{LL_{\\text{null}}}{N}=\\pi\\ln\\pi+(1-\\pi)\\ln(1-\\pi)\n=0.12\\ln(0.12)+0.88\\ln(0.88)\\approx -0.3669\n\\]\n\nImprovement per observation:\n\n\\[\n\\Delta = \\frac{LL_{\\text{model}}-LL_{\\text{null}}}{N}\n= -0.2872 - (-0.3669)=0.0797\n\\]\n\nThus LR per observation:\n\n\\[\n\\frac{LR}{N}=2\\Delta = 2(0.0797)=0.1594\n\\]\n\n---\n\n### Step 3: Solve for required sample size\nWe need total \\(LR=500\\), and expected \\(LR/N=0.1594\\), so:\n\n\\[\nN = \\frac{500}{0.1594}=3136.5 \\;\\Rightarrow\\; \\boxed{3137}\n\\]\n\n---\n\n## Final answer (rounded up)\n\\[\n\\boxed{N \\approx 3137 \\text{ participants total}}\n\\]\n\n(Which implies about \\(0.12\\times 3137 \\approx 377\\) events.)",
    "latency_ms": 197616,
    "tokens": {
      "input": 117,
      "output": 8717,
      "total": 8834
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T04:59:40.798Z"
  },
  "t4-binary-005": {
    "task_id": "t4-binary-005",
    "tier": 4,
    "question": "We're developing a prediction model for a very rare outcome (3% prevalence) with 8 candidate predictors. We expect a C-statistic of 0.85. What minimum sample size is needed using Riley criteria?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Riley et al.’s sample size criteria for developing a **binary** prediction model require an **anticipated Cox–Snell \\(R^2\\) (\\(R^2_{CS}\\))**, plus the number of candidate predictor parameters \\(p\\), outcome prevalence, and targets for overfitting and precision. A **C-statistic alone is not sufficient** to uniquely determine \\(R^2_{CS}\\), so below I use a common *approximation* to translate \\(C\\) to \\(R^2\\) (and show the resulting Riley calculations).\n\n## Given\n- Outcome prevalence: \\(\\pi = 0.03\\)\n- Candidate predictor parameters: \\(p = 8\\)\n- Anticipated C-statistic: \\(C = 0.85\\)\n- Riley defaults typically used:\n  - Target shrinkage: \\(S = 0.90\\)\n  - Target optimism in Nagelkerke \\(R^2\\): \\(\\Delta R^2_N \\le 0.05\\)\n  - Precision of outcome prevalence (intercept): I use **±0.01** absolute (more meaningful than ±0.05 when prevalence is 3%)\n\n---\n\n## Step 1) Approximate \\(R^2_{CS}\\) from \\(C\\)\nConvert \\(C\\) to a standardized separation (Cohen’s \\(d\\)) via the binormal relationship:\n\\[\nz=\\Phi^{-1}(C)=\\Phi^{-1}(0.85)=1.036,\\quad d=\\sqrt{2}\\,z=1.466\n\\]\nApproximate a “variance explained” on an \\(R^2\\)-like scale:\n\\[\nR^2_N \\approx \\frac{d^2}{d^2+4}=\\frac{2.149}{2.149+4}=0.349\n\\]\n\nCompute the **maximum** Cox–Snell \\(R^2\\) given prevalence \\(\\pi\\):\n\\[\n\\max(R^2_{CS}) = 1-\\exp\\left(2\\left[\\pi\\ln\\pi + (1-\\pi)\\ln(1-\\pi)\\right]\\right)\n\\]\n\\[\n\\pi\\ln\\pi + (1-\\pi)\\ln(1-\\pi)=0.03\\ln(0.03)+0.97\\ln(0.97)=-0.1347\n\\]\n\\[\n\\max(R^2_{CS})=1-\\exp(2\\times -0.1347)=1-\\exp(-0.2694)=0.236\n\\]\n\nThen\n\\[\nR^2_{CS} \\approx R^2_N\\times \\max(R^2_{CS})=0.349\\times 0.236=0.0826\n\\]\n\n---\n\n## Step 2) Riley criterion 1: shrinkage \\(S\\ge 0.90\\)\nUsing \\(LR \\approx -n\\ln(1-R^2_{CS})\\) and \\(S = 1 - p/LR\\), require \\(LR \\ge p/(1-S)\\):\n\\[\nn \\ge \\frac{p}{(1-S)\\,[-\\ln(1-R^2_{CS})]}\n\\]\n\\[\n-\\ln(1-0.0826)=-\\ln(0.9174)=0.0863\n\\]\n\\[\nn_1 \\ge \\frac{8}{0.1\\times 0.0863}=926.7 \\Rightarrow 927\n\\]\n\n---\n\n## Step 3) Riley criterion 2: optimism in \\(R^2_N\\) ≤ 0.05\nDifference on Cox–Snell scale:\n\\[\n\\Delta R^2_{CS} \\le 0.05\\times \\max(R^2_{CS}) = 0.05\\times 0.236 = 0.01182\n\\]\nUsing Riley’s approximation \\(R^2_{CS,\\text{app}}-R^2_{CS,\\text{adj}} \\approx (1-R^2_{CS})(e^{p/n}-1)\\):\n\\[\n(1-R^2_{CS})(e^{p/n}-1) \\le 0.01182\n\\]\n\\[\n0.9174(e^{8/n}-1)\\le 0.01182 \\Rightarrow e^{8/n}\\le 1.01288\n\\]\n\\[\n8/n \\le \\ln(1.01288)=0.01280 \\Rightarrow n_2 \\ge 625\n\\]\n\n---\n\n## Step 4) Riley criterion 3: precision of overall outcome risk (intercept)\nFor absolute margin \\(m=0.01\\) on the outcome proportion:\n\\[\nn_3 \\ge \\frac{(1.96)^2\\pi(1-\\pi)}{m^2}\n= \\frac{3.8416\\times 0.03\\times 0.97}{0.01^2}\n=1116.0 \\Rightarrow 1116\n\\]\n\n---\n\n# Minimum sample size (max of criteria)\n\\[\nn_{\\min}=\\max(n_1,n_2,n_3)=\\max(927,\\;625,\\;1116)=\\boxed{1116}\n\\]\n\n**Final answer: \\(\\boxed{n = 1116}\\)** total participants (expected events \\(=0.03\\times 1116\\approx 34\\)).\n\n*Note:* If you instead used a looser intercept-precision target (e.g., ±0.05), criterion 3 would be tiny and the governing criterion would be shrinkage, giving about **927**. The biggest driver here is what precision you require for estimating a 3% baseline risk.",
    "latency_ms": 211765,
    "tokens": {
      "input": 105,
      "output": 7777,
      "total": 7882
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:03:14.071Z"
  },
  "t4-surv-001": {
    "task_id": "t4-surv-001",
    "tier": 4,
    "question": "We are developing a survival prediction model for time to recurrent venous thromboembolism (VTE). We have 30 candidate predictors. The overall event rate is 0.065, and we anticipate R²_CS of 0.051. The timepoint of interest is 2 years with mean follow-up of 2.07 years. What minimum sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s minimum sample size approach for time-to-event (survival) prediction models (via the Cox-Snell \\(R^2\\)):\n\n### Inputs\n- Candidate predictors: \\(p = 30\\)\n- Anticipated \\(R^2_{CS} = 0.051\\)\n- Overall event proportion over mean follow-up: 0.065\n- Mean follow-up \\(= 2.07\\) years; timepoint of interest \\(t=2\\) years\n\n### Step 1: (Optional) approximate event proportion by 2 years\nAssuming an exponential event process, estimate the hazard rate:\n\\[\n\\lambda = \\frac{-\\ln(1-0.065)}{2.07}=\\frac{-\\ln(0.935)}{2.07}=\\frac{0.0672}{2.07}=0.03246 \\text{ per year}\n\\]\nThen 2-year event risk:\n\\[\nP(\\text{event by 2y}) = 1-e^{-\\lambda\\cdot 2} = 1-e^{-0.06492}=0.0628\n\\]\n\n### Step 2: Sample size to target global shrinkage \\(S \\ge 0.90\\)\nFor a prediction model, the required likelihood ratio statistic is:\n\\[\nLR \\ge \\frac{p}{1-S} = \\frac{30}{0.1} = 300\n\\]\nAnd Cox-Snell \\(R^2\\) relates to \\(LR\\) via:\n\\[\nR^2_{CS} = 1-\\exp\\left(-\\frac{LR}{n}\\right)\n\\Rightarrow LR = -n\\ln(1-R^2_{CS})\n\\]\nSo:\n\\[\nn \\ge \\frac{LR}{-\\ln(1-R^2_{CS})}\n= \\frac{300}{-\\ln(1-0.051)}\n= \\frac{300}{-\\ln(0.949)}\n\\]\nCompute:\n\\[\n-\\ln(0.949) \\approx 0.052345\n\\]\n\\[\nn \\ge \\frac{300}{0.052345} = 5732.7\n\\]\n\nRound up:\n\n## Minimum required sample size: **5733 participants**\n\n(With the estimated 2-year event risk \\(\\approx 0.0628\\), this corresponds to about \\(5733\\times 0.0628 \\approx 360\\) events by 2 years.)",
    "latency_ms": 155146,
    "tokens": {
      "input": 139,
      "output": 3055,
      "total": 3194
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:05:50.723Z"
  },
  "t4-surv-002": {
    "task_id": "t4-surv-002",
    "tier": 4,
    "question": "I'm developing a cancer prognosis model with 6 predictors. The 5-year mortality is 40%, expected Cox-Snell R² is 0.25, and mean follow-up is 3 years. What minimum sample size is needed for model development?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using the Riley et al. minimum sample size approach for a time-to-event (Cox) prediction model (default targeting **global shrinkage ≈ 0.90** and **±0.05 absolute precision** for the baseline 5-year survival estimate):\n\n### Given\n- Predictors (parameters): \\(p=6\\)  \n- 5-year mortality: 40%  \\(\\Rightarrow S(5)=0.60\\)  \n- Expected Cox–Snell \\(R^2_{CS}=0.25\\)  \n- Mean follow-up: 3 years  \n\n---\n\n## 1) Convert 5-year mortality to an event hazard (assume exponential)\n\\[\n\\lambda_e = -\\frac{\\ln(S(5))}{5} = -\\frac{\\ln(0.60)}{5} = \\frac{0.5108}{5}=0.1022\\ \\text{per year}\n\\]\n\n## 2) Use mean follow-up to infer censoring (assume exponential censoring)\nMean follow-up \\(\\approx 1/(\\lambda_e+\\lambda_c)=3\\):\n\\[\n\\lambda_e+\\lambda_c = \\frac{1}{3}=0.3333 \\Rightarrow \\lambda_c = 0.3333-0.1022=0.2311\n\\]\n\nExpected proportion with an **observed event by 5 years**:\n\\[\n\\pi_{5} = \\frac{\\lambda_e}{\\lambda_e+\\lambda_c}\\left(1-e^{-(\\lambda_e+\\lambda_c)5}\\right)\n\\]\n\\[\n\\pi_{5} = \\frac{0.1022}{0.3333}\\left(1-e^{-0.3333\\cdot 5}\\right)\n=0.3066(1-e^{-1.6667})\n=0.3066(1-0.1889)=0.248\n\\]\n\nSo about **24.8%** are expected to have an observed death by 5 years.\n\n---\n\n## 3) Criterion A: sample size for shrinkage (≈0.90)\nFor Cox–Snell \\(R^2\\), the likelihood ratio statistic is:\n\\[\nLR = -N\\ln(1-R^2_{CS})\n\\]\nShrinkage approximation:\n\\[\nS \\approx 1-\\frac{p}{LR}\n\\]\nSet \\(S=0.90\\):\n\\[\n0.90 = 1-\\frac{p}{-N\\ln(1-R^2_{CS})}\n\\Rightarrow N=\\frac{10p}{-\\ln(1-R^2_{CS})}\n\\]\n\\[\nN_1=\\frac{10\\cdot 6}{-\\ln(0.75)}=\\frac{60}{0.28768}=208.6 \\Rightarrow 209\n\\]\n\n---\n\n## 4) Criterion B: precision of baseline 5-year survival (±0.05)\nBaseline cumulative hazard at 5 years:\n\\[\nH(5)=-\\ln(S(5))=0.5108\n\\]\n\nApproximate SE for \\(S(5)\\) via delta method using \\( \\text{Var}(\\log H)\\approx 1/E \\) gives:\n\\[\nSE\\{S(5)\\} \\approx \\frac{S(5)\\,H(5)}{\\sqrt{E}}\n\\]\nRequire 95% margin \\(\\le 0.05\\):\n\\[\n1.96\\cdot \\frac{S(5)\\,H(5)}{\\sqrt{E}} \\le 0.05\n\\Rightarrow\nE \\ge \\left(\\frac{1.96\\cdot 0.60\\cdot 0.5108}{0.05}\\right)^2\n\\]\n\\[\nE \\ge (12.01)^2=144.3 \\Rightarrow 145\\ \\text{events}\n\\]\n\nConvert events to total sample size using \\(\\pi_5=0.248\\):\n\\[\nN_3 = \\frac{E}{\\pi_5}=\\frac{145}{0.248}=584.7 \\Rightarrow 585\n\\]\n\n---\n\n# **Minimum required sample size (rounded up): \\(\\boxed{585}\\)**\n\nThis corresponds to about **145 observed deaths by 5 years** (given the mortality and follow-up assumptions).",
    "latency_ms": 229573,
    "tokens": {
      "input": 113,
      "output": 6522,
      "total": 6635
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:09:41.800Z"
  },
  "t4-surv-003": {
    "task_id": "t4-surv-003",
    "tier": 4,
    "question": "I'm developing a cardiovascular risk prediction model with 12 predictors. The 10-year event rate is about 15%, and based on existing models I expect a Cox-Snell R² of 0.15. Mean follow-up will be about 6 years. What sample size do I need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using the **Riley et al. (pmsampsize) framework for time-to-event prediction models** (Cox model), a common set of defaults is:\n\n- Target global **shrinkage** \\(S=0.90\\)  \n- Adequate precision for estimating the **baseline (overall) 10-year risk** (often taken as an absolute CI width of about **0.05** on the risk scale)\n\nGiven:\n- Predictors (parameters) \\(p = 12\\)\n- Anticipated Cox–Snell \\(R^2_{CS} = 0.15\\)\n- 10-year event risk \\(=0.15\\)\n- Mean follow-up \\(\\bar{t} = 6\\) years\n\n---\n\n## 1) Convert 10-year risk to an event hazard, then account for censoring via mean follow-up\n\nAssume an exponential event time distribution (standard for this calculation):\n\n\\[\n\\lambda = -\\frac{\\ln(1-0.15)}{10} = -\\frac{\\ln(0.85)}{10} = 0.0162519 \\text{ per year}\n\\]\n\nAssume independent exponential censoring such that mean follow-up is 6 years:\n\n\\[\n\\lambda+\\mu = \\frac{1}{6}=0.1666667 \\Rightarrow \\mu = 0.1666667-0.0162519=0.1504148\n\\]\n\nExpected **observed** event proportion by 10 years (event happens before censoring and before 10 years):\n\n\\[\n\\pi_{10}=\\frac{\\lambda}{\\lambda+\\mu}\\left(1-e^{-(\\lambda+\\mu)10}\\right)\n\\]\n\n\\[\n\\frac{\\lambda}{\\lambda+\\mu}= \\frac{0.0162519}{0.1666667}=0.0975113\n\\]\n\\[\n1-e^{-0.1666667\\cdot 10}=1-e^{-1.6666667}=1-0.1888756=0.8111244\n\\]\n\\[\n\\pi_{10}=0.0975113 \\times 0.8111244 = 0.079103\n\\]\n\nSo only about **7.91%** of participants are expected to have an *observed* event by 10 years, given mean follow-up of 6 years.\n\n---\n\n## 2) Sample size from shrinkage requirement \\(S=0.90\\)\n\nFor Cox/Snell \\(R^2\\), the required sample size for shrinkage \\(S\\) is:\n\n\\[\nn \\ge \\frac{p}{(1-S)\\,[-\\ln(1-R^2_{CS})]}\n\\]\n\nCompute:\n\n\\[\n-\\ln(1-R^2_{CS})=-\\ln(0.85)=0.1625189\n\\]\n\\[\nn \\ge \\frac{12}{0.1\\times 0.1625189}=\\frac{12}{0.0162519}=738.4 \\Rightarrow 739\n\\]\n\nSo shrinkage alone suggests **739** participants.\n\n---\n\n## 3) Precision of baseline 10-year risk (typically the binding constraint here)\n\nTo estimate the baseline survival (thus baseline 10-year risk) with reasonable precision, you typically need on the order of **~121 observed events by 10 years** for an absolute risk CI width around **0.05** when the true 10-year risk is 0.15 (using a log–log survival CI approximation).\n\nThus required \\(n\\):\n\n\\[\nn \\ge \\frac{D_{10}}{\\pi_{10}} = \\frac{121}{0.079103}=1530.2 \\Rightarrow 1531\n\\]\n\nThis also automatically satisfies shrinkage (since 1531 > 739).\n\n---\n\n# **Final required sample size: \\(\\boxed{1531}\\) participants**\n\n(Implies ~\\(1531\\times 0.079 \\approx 121\\) observed events by 10 years, given mean follow-up 6 years and a 10-year event risk of 15%.)",
    "latency_ms": 269499,
    "tokens": {
      "input": 121,
      "output": 7557,
      "total": 7678
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:14:12.806Z"
  },
  "t4-surv-004": {
    "task_id": "t4-surv-004",
    "tier": 4,
    "question": "We're developing a survival prediction model where some patients experience a competing event. The primary event rate is 20%, we have 10 predictors, and expected R²_CS = 0.18. Timepoint of interest is 5 years with mean follow-up of 3 years. What's the Riley sample size for the primary event model?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s **time-to-event (Cox) prediction model** sample size criteria (as implemented in *pmsampsize*), with default targets **shrinkage \\(S=0.90\\)** and **expected optimism in \\(R^2_{CS}\\) \\(\\le 0.05\\)**:\n\nGiven  \n- predictors \\(p=10\\)  \n- anticipated \\(R^2_{CS}=0.18\\)\n\nCompute\n\\[\n\\ln\\!\\left(\\frac{1}{1-R^2_{CS}}\\right)= -\\ln(1-0.18)= -\\ln(0.82)=0.1984509\n\\]\n\n### Criterion 1 (shrinkage \\(S=0.90\\))\n\\[\nN_1=\\frac{p}{(1-S)\\;\\ln\\!\\left(\\frac{1}{1-R^2_{CS}}\\right)}\n=\\frac{10}{0.10\\times 0.1984509}\n=503.90 \\Rightarrow 504\n\\]\n\n### Criterion 2 (optimism in \\(R^2_{CS}\\) ≤ 0.05)\n\\[\nN_2=\\frac{p}{0.05\\;\\ln\\!\\left(\\frac{1}{1-R^2_{CS}}\\right)}\n=\\frac{10}{0.05\\times 0.1984509}\n=1007.8 \\Rightarrow 1008\n\\]\n\nCriterion 2 is larger, so it governs.\n\n### (Optional) translate to expected primary events with mean follow-up 3 years\nUsing an exponential approximation from 5-year primary event risk 0.20:\n\\[\n\\lambda = -\\ln(0.8)/5 = 0.04463\n\\]\nExpected primary event proportion by 3 years:\n\\[\nF(3)=1-e^{-\\lambda\\cdot 3}=1-e^{-0.1339}=0.1253\n\\]\nExpected primary events:\n\\[\n1008\\times 0.1253 \\approx 126\n\\]\n\n## Final answer (Riley sample size)\n**Required total sample size ≈ \\(\\boxed{1008}\\) patients** (rounding up).  \n(Implies ~**126** expected primary events over mean 3-year follow-up under the exponential approximation.)",
    "latency_ms": 169680,
    "tokens": {
      "input": 129,
      "output": 8688,
      "total": 8817
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:17:03.997Z"
  },
  "t4-surv-005": {
    "task_id": "t4-surv-005",
    "tier": 4,
    "question": "I'm developing a survival model for a condition with high event rate (40%) over 3 years. We have 15 predictors and expect R²_CS = 0.20. Mean follow-up is 2.5 years. What minimum sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s **minimum sample size for a time-to-event (Cox) prediction model** framework (as implemented in tools like `pmsampsize`), take the maximum required \\(N\\) across criteria. The dominant criterion here is typically the **global shrinkage** requirement.\n\n## Inputs\n- Predictors (parameters): \\(p = 15\\)\n- Anticipated Cox–Snell \\(R^2_{CS} = 0.20\\)\n- 3-year event proportion: \\(1-S(3)=0.40 \\Rightarrow S(3)=0.60\\)\n- Mean follow-up: \\(\\bar{t}=2.5\\) years\n- Target shrinkage (default): \\(S = 0.90\\)\n\n---\n\n## 1) Sample size from shrinkage criterion (key step)\nFor desired shrinkage \\(S\\), Riley’s approach uses:\n\\[\nN \\;\\ge\\; \\frac{p}{(1-S)\\,[-\\ln(1-R^2_{CS})]}\n\\]\n\nCompute terms:\n\\[\n-\\ln(1-R^2_{CS}) = -\\ln(0.80)=0.223143\n\\]\n\\[\nN \\ge \\frac{15}{0.10 \\times 0.223143} = \\frac{15}{0.0223143}=672.6\n\\]\n\nRound up:\n\\[\nN_{\\text{min}} = 673\n\\]\n\n---\n\n## 2) Expected number of events (using event rate + mean follow-up)\nAssume approximately exponential hazards to translate the 3-year risk to a rate:\n\n\\[\nS(3)=e^{-\\lambda 3}=0.60 \\Rightarrow \\lambda = -\\ln(0.60)/3 = 0.1703 \\text{ per year}\n\\]\n\nExpected event proportion over mean follow-up \\(\\bar{t}=2.5\\):\n\\[\nP(\\text{event by }2.5)=1-e^{-\\lambda \\cdot 2.5}\n=1-e^{-0.1703\\cdot 2.5}\n=1-e^{-0.4258}\n=0.346\n\\]\n\nExpected events at \\(N=673\\):\n\\[\nD \\approx 673 \\times 0.346 = 233 \\text{ events (approx.)}\n\\]\n\n---\n\n# **Final answer (minimum required sample size): \\(\\boxed{673}\\) participants**\n(With this, you’d expect roughly \\(\\approx \\boxed{233}\\) events given the mean follow-up of 2.5 years.)",
    "latency_ms": 178622,
    "tokens": {
      "input": 115,
      "output": 4907,
      "total": 5022
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:20:04.125Z"
  },
  "t4-cont-001": {
    "task_id": "t4-cont-001",
    "tier": 4,
    "question": "We are developing a linear regression model to predict fat-free mass in children from 25 candidate predictors. We anticipate an R² of 0.2. The intercept is 1.9 and the standard deviation of the outcome is 0.6. What minimum sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s **minimum sample size criteria for developing a continuous-outcome linear regression prediction model**, a key requirement is to limit overfitting by ensuring a **global shrinkage factor** of at least \\(S=0.90\\).\n\n### Inputs\n- Candidate predictor parameters: \\(p = 25\\)\n- Anticipated model fit: \\(R^2 = 0.20\\)\n- (Intercept and outcome SD are not needed for the shrinkage-driven requirement; they matter for other criteria such as intercept precision, which is typically smaller than the shrinkage requirement here.)\n\n---\n\n## 1) Shrinkage criterion (dominant)\nFor linear regression, the model likelihood ratio statistic is approximately\n\n\\[\n\\chi^2 = n \\ln\\!\\left(\\frac{1}{1-R^2}\\right)\n\\]\n\nThe global shrinkage factor is\n\n\\[\nS = 1 - \\frac{p}{\\chi^2}\n\\]\n\nRequire \\(S \\ge 0.90\\):\n\n\\[\n1 - \\frac{p}{\\chi^2} \\ge 0.90\n\\quad\\Rightarrow\\quad\n\\frac{p}{\\chi^2} \\le 0.10\n\\quad\\Rightarrow\\quad\n\\chi^2 \\ge 10p\n\\]\n\nSo:\n\n\\[\nn \\ln\\!\\left(\\frac{1}{1-R^2}\\right) \\ge 10p\n\\]\n\nCompute the log term:\n\n\\[\n\\ln\\!\\left(\\frac{1}{1-0.2}\\right)=\\ln(1.25)=0.2231436\n\\]\n\nNow solve for \\(n\\):\n\n\\[\nn \\ge \\frac{10p}{\\ln(1/(1-R^2))}\n= \\frac{10\\cdot 25}{0.2231436}\n= \\frac{250}{0.2231436}\n= 1120.9\n\\]\n\nRound up:\n\n\\[\nn_{\\min} = 1121\n\\]\n\n---\n\n## (Check) R² optimism / adjusted R² criterion (not limiting here)\nA common bound is \\(R^2 - R^2_{\\text{adj}} \\le 0.05\\), where\n\n\\[\nR^2 - R^2_{\\text{adj}} = \\frac{p(1-R^2)}{n-p-1}\n\\]\n\nSolve:\n\n\\[\n\\frac{25(0.8)}{n-26} \\le 0.05\n\\Rightarrow \\frac{20}{n-26}\\le 0.05\n\\Rightarrow n \\ge 426\n\\]\n\nThis is much smaller than 1121, so shrinkage dominates.\n\n---\n\n# Final answer (minimum sample size)\n## **\\( \\boxed{1121} \\)** children (minimum sample size)",
    "latency_ms": 127263,
    "tokens": {
      "input": 120,
      "output": 3426,
      "total": 3546
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:22:12.892Z"
  },
  "t4-cont-002": {
    "task_id": "t4-cont-002",
    "tier": 4,
    "question": "I'm building a prediction model for blood pressure with 5 predictors. I expect the model to explain about 30% of the variance. The population mean is around 120 mmHg with SD of 15 mmHg. What sample size do I need using Riley criteria?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s criteria for **sample size for developing a linear regression prediction model** (continuous outcome), you take the **largest** sample size required across 3 conditions:\n\nLet  \n\n- number of predictors \\(p=5\\) (excluding intercept)  \n- anticipated model fit \\(R^2 = 0.30\\)  \n- target global shrinkage \\(S \\ge 0.90\\)  \n- target optimism in \\(R^2\\): \\(R^2_{\\text{apparent}}-R^2_{\\text{adjusted}} \\le 0.05\\)  \n- target precision for residual SD: 95% CI for \\(\\sigma\\) within \\(\\pm 10\\%\\) (i.e., 0.9 to 1.1 of true \\(\\sigma\\))\n\n(Your outcome mean 120 and SD 15 mmHg are *not needed* for these Riley calculations; \\(R^2\\) drives the required \\(n\\).)\n\n---\n\n## 1) Shrinkage criterion (\\(S\\ge 0.90\\))\n\nFor linear regression, with likelihood ratio statistic \\(\\chi^2 \\approx n\\ln\\!\\left(\\frac{1}{1-R^2}\\right)\\) and shrinkage \\(S \\approx 1 - \\frac{p}{\\chi^2}\\),\n\n\\[\nn \\ge \\frac{p/(1-S)}{\\ln\\left(\\frac{1}{1-R^2}\\right)}\n\\]\n\nCompute:\n\n- \\(\\ln\\left(\\frac{1}{1-R^2}\\right)=\\ln\\left(\\frac{1}{0.7}\\right)=\\ln(1.4286)=0.3567\\)\n- \\(p/(1-S)=5/0.1=50\\)\n\n\\[\nn_1 \\ge \\frac{50}{0.3567}=140.2 \\Rightarrow 141\n\\]\n\n---\n\n## 2) Small optimism in \\(R^2\\) (≤ 0.05)\n\nFor linear regression,\n\n\\[\nR^2 - R^2_{\\text{adj}} = \\frac{p(1-R^2)}{n-p-1} \\le 0.05\n\\]\n\nSolve:\n\n\\[\nn \\ge \\frac{p(1-R^2)}{0.05} + p + 1\n\\]\n\n\\[\nn_2 \\ge \\frac{5(0.7)}{0.05}+6 = 70+6 = 76\n\\]\n\n---\n\n## 3) Precision of residual SD \\(\\sigma\\) (95% CI within ±10%)\n\nWith \\(df = n - p - 1\\), the 95% CI for \\(\\sigma\\) implies (approximately, using chi-square quantiles) requiring:\n\n\\[\n\\sqrt{\\frac{df}{\\chi^2_{0.025,df}}} \\le 1.1\n\\quad\\text{and}\\quad\n\\sqrt{\\frac{df}{\\chi^2_{0.975,df}}} \\ge 0.9\n\\]\n\nThis is satisfied at about \\(df \\approx 232\\) (the upper-limit condition drives it). Thus:\n\n\\[\nn_3 \\ge df + p + 1 = 232 + 5 + 1 = 238\n\\]\n\n---\n\n# Final (Riley) required sample size\n\n\\[\nn = \\max(n_1,n_2,n_3)=\\max(141,76,238)=238\n\\]\n\n## **Answer: You need at least \\(\\boxed{238}\\) participants.**",
    "latency_ms": 158907,
    "tokens": {
      "input": 116,
      "output": 3910,
      "total": 4026
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:24:53.306Z"
  },
  "t4-cont-003": {
    "task_id": "t4-cont-003",
    "tier": 4,
    "question": "We're developing a continuous outcome prediction model with 15 predictors but expect only modest explanatory power (R² = 0.15). The outcome is standardized (mean 0, SD 1). We want to be more conservative with a shrinkage target of 0.95 (less than 5% shrinkage). What sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Use the usual global **uniform shrinkage factor** approximation (van Houwelingen/le Cessie; also used in Riley et al.):\n\n1) For a continuous (Gaussian) outcome model, the model likelihood ratio statistic can be written in terms of the anticipated \\(R^2\\) as  \n\\[\n\\chi^2 \\approx n \\,\\ln\\!\\left(\\frac{1}{1-R^2}\\right).\n\\]\n\n2) The global shrinkage factor is approximated by  \n\\[\nS \\approx \\frac{\\chi^2 - p}{\\chi^2} = 1 - \\frac{p}{\\chi^2},\n\\]\nwhere \\(p\\) is the number of predictors (parameters excluding intercept; here \\(p=15\\)).\n\nSet target shrinkage \\(S=0.95\\):\n\\[\n0.95 = 1 - \\frac{p}{\\chi^2}\n\\;\\Rightarrow\\;\n\\frac{p}{\\chi^2}=0.05\n\\;\\Rightarrow\\;\n\\chi^2=\\frac{p}{0.05}=20p=20(15)=300.\n\\]\n\n3) Solve for \\(n\\) using \\(R^2=0.15\\):\n\\[\nn = \\frac{\\chi^2}{\\ln\\!\\left(\\frac{1}{1-R^2}\\right)}\n= \\frac{300}{\\ln\\!\\left(\\frac{1}{0.85}\\right)}\n= \\frac{300}{\\ln(1.1764706)}\n= \\frac{300}{0.1625189}\n= 1846.6.\n\\]\n\nRound up:\n\n## Required sample size: **\\(n = 1847\\)**",
    "latency_ms": 47571,
    "tokens": {
      "input": 132,
      "output": 980,
      "total": 1112
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:25:42.383Z"
  },
  "t4-cont-004": {
    "task_id": "t4-cont-004",
    "tier": 4,
    "question": "After LASSO variable selection, we have 30 predictors for a continuous outcome model with expected R² = 0.35. Using standardized outcome (mean 0, SD 1) and standard Riley criteria (shrinkage 0.90), what sample size is required?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Using Riley et al.’s **shrinkage criterion** for a continuous outcome linear regression model:\n\n### Step 1: Riley shrinkage requirement\nFor global shrinkage \\(S\\),\n\n\\[\nS \\;=\\; 1 - \\frac{p}{\\chi^2}\n\\]\n\nRequire \\(S \\ge 0.90 \\Rightarrow 1 - \\frac{p}{\\chi^2} \\ge 0.90 \\Rightarrow \\chi^2 \\ge 10p\\).\n\nFor linear regression, the model \\(\\chi^2\\) can be approximated from the anticipated \\(R^2\\) as:\n\n\\[\n\\chi^2 \\approx n \\,\\ln\\!\\left(\\frac{1}{1-R^2}\\right)\n\\]\n\nSo the shrinkage requirement becomes:\n\n\\[\nn \\,\\ln\\!\\left(\\frac{1}{1-R^2}\\right) \\ge 10p\n\\quad\\Rightarrow\\quad\nn \\ge \\frac{10p}{\\ln\\!\\left(\\frac{1}{1-R^2}\\right)}\n\\]\n\n### Step 2: Plug in values\n- \\(p = 30\\)\n- \\(R^2 = 0.35\\)\n\n\\[\n\\ln\\!\\left(\\frac{1}{1-0.35}\\right)\n=\n\\ln\\!\\left(\\frac{1}{0.65}\\right)\n=\n\\ln(1.53846)\n=\n0.43078\n\\]\n\n\\[\nn \\ge \\frac{10(30)}{0.43078}\n=\n\\frac{300}{0.43078}\n=\n696.5\n\\]\n\nRound up:\n\n\\[\nn = 697\n\\]\n\n## Final answer (required sample size)\n**\\( \\boxed{697} \\)** participants.",
    "latency_ms": 95312,
    "tokens": {
      "input": 120,
      "output": 921,
      "total": 1041
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:27:19.200Z"
  },
  "t4-valid-001": {
    "task_id": "t4-valid-001",
    "tier": 4,
    "question": "We're validating a DVT diagnostic model in a new population. The linear predictor follows a normal distribution with mean = -1.75 and SD = 2.16. Expected C-statistic is 0.82, outcome prevalence is 22%. We want precision: C-stat CI width ≤ 0.10, calibration slope CI width ≤ 0.20, O/E ratio CI width ≤ 0.20. What validation sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To meet **all three precision targets**, calculate the required validation \\(N\\) for each target and take the **maximum**.\n\nGiven:\n\n- LP \\(X \\sim N(\\mu=-1.75,\\;\\sigma=2.16)\\)\n- Expected \\(C=0.82\\)\n- Outcome prevalence \\(p=0.22\\) \\(\\Rightarrow n_1=pN\\) events, \\(n_0=(1-p)N\\) non-events\n- 95% CI width targets  \n  - \\(C\\)-stat width \\(\\le 0.10\\)  \\(\\Rightarrow\\) half-width \\(d_C=0.05\\)  \n  - calibration slope width \\(\\le 0.20\\) \\(\\Rightarrow\\) half-width \\(d_S=0.10\\)  \n  - O/E width \\(\\le 0.20\\) \\(\\Rightarrow\\) half-width \\(d_{OE}=0.10\\)\n\nFor a 95% CI, the general requirement is:\n\\[\n1.96\\times SE \\le d \\quad\\Rightarrow\\quad SE \\le \\frac{d}{1.96}\n\\]\n\n---\n\n## 1) Required \\(N\\) for \\(C\\)-statistic precision\n\nUse Hanley–McNeil variance:\n\n\\[\n\\mathrm{Var}(C)=\\frac{C(1-C)+(n_1-1)(Q_1-C^2)+(n_0-1)(Q_2-C^2)}{n_1n_0}\n\\]\nwhere\n\\[\nQ_1=\\frac{C}{2-C},\\qquad Q_2=\\frac{2C^2}{1+C}\n\\]\n\nWith \\(C=0.82\\):\n\n- \\(C^2=0.6724\\)\n- \\(Q_1=0.82/1.18=0.694915\\)  \\(\\Rightarrow Q_1-C^2=0.022515\\)\n- \\(Q_2=2(0.6724)/1.82=0.738901\\) \\(\\Rightarrow Q_2-C^2=0.066501\\)\n\nTarget half-width \\(d_C=0.05\\):\n\\[\nSE_C \\le \\frac{0.05}{1.96}=0.02551\n\\]\n\nSolving (with \\(n_1=0.22N,\\;n_0=0.78N\\)) gives approximately:\n\\[\nN_C \\approx 509\n\\]\n\n---\n\n## 2) Required \\(N\\) for O/E ratio precision\n\nAssume expected O/E \\(\\approx 1\\). Using the common large-sample log CI:\n\\[\n\\log(\\mathrm{O/E}) \\approx \\pm 1.96\\sqrt{\\frac{1}{O}}\n\\]\nso on the ratio scale:\n\\[\n\\text{CI width} = e^{a}-e^{-a} = 2\\sinh(a),\\quad a=\\frac{1.96}{\\sqrt{O}}\n\\]\n\nRequire width \\(\\le 0.20\\):\n\\[\n2\\sinh(a)\\le 0.20 \\Rightarrow \\sinh(a)\\le 0.10 \\Rightarrow a\\le \\mathrm{asinh}(0.10)=0.099834\n\\]\n\\[\n\\frac{1.96}{\\sqrt{O}} \\le 0.099834 \\Rightarrow O \\ge \\left(\\frac{1.96}{0.099834}\\right)^2 = 385.3\n\\]\n\nWith prevalence \\(p=0.22\\), \\(O\\approx pN\\):\n\\[\nN_{OE} \\ge \\frac{385.3}{0.22}=1751.5 \\Rightarrow N_{OE}=1752\n\\]\n\n---\n\n## 3) Required \\(N\\) for calibration slope precision\n\nCalibration slope \\(\\beta\\) is estimated from:\n\\[\n\\text{logit}(Y)=\\alpha + \\beta X\n\\]\n\nWe need \\( \\text{CI width} \\le 0.20 \\Rightarrow d_S=0.10\\):\n\\[\nSE_\\beta \\le \\frac{0.10}{1.96}=0.05102\n\\]\n\nUsing Fisher information for logistic regression with intercept and slope, per-observation expected information elements:\n\\[\nI_{\\alpha\\alpha}=E[w],\\quad I_{\\alpha\\beta}=E[wX],\\quad I_{\\beta\\beta}=E[wX^2],\\;\\;\\; w=\\pi(1-\\pi),\\;\\pi=\\text{expit}(\\alpha+X)\n\\]\nand\n\\[\n\\mathrm{Var}(\\hat\\beta)=\\frac{I_{\\alpha\\alpha}}{N\\left(I_{\\alpha\\alpha}I_{\\beta\\beta}-I_{\\alpha\\beta}^2\\right)}\n\\]\n\nChoose \\(\\alpha\\) so that \\(E[\\pi]=0.22\\) (matches the stated prevalence). Numerically (via normal-mixture approximation/integration), this corresponds to \\(\\alpha\\approx -0.37\\).\n\nThen the expected information (per observation) is approximately:\n- \\(I_{\\alpha\\alpha}\\approx 0.1071\\)\n- \\(I_{\\alpha\\beta}\\approx -0.0441\\)\n- \\(I_{\\beta\\beta}\\approx 0.2161\\)\n\nSo:\n\\[\nD=I_{\\alpha\\alpha}I_{\\beta\\beta}-I_{\\alpha\\beta}^2\n=0.1071(0.2161)-(0.0441)^2\\approx 0.02121\n\\]\n\\[\n\\mathrm{Var}(\\hat\\beta)\\approx \\frac{0.1071}{N(0.02121)}=\\frac{5.051}{N}\n\\Rightarrow SE_\\beta \\approx \\frac{\\sqrt{5.051}}{\\sqrt{N}}=\\frac{2.248}{\\sqrt{N}}\n\\]\n\nImpose \\(SE_\\beta \\le 0.05102\\):\n\\[\n\\frac{2.248}{\\sqrt{N}} \\le 0.05102 \\Rightarrow \\sqrt{N}\\ge 44.07 \\Rightarrow N \\ge 1942\n\\]\n\nThus:\n\\[\nN_{\\text{slope}} = 1942\n\\]\n\n---\n\n# Final required validation sample size\n\nTake the maximum of the three requirements:\n- \\(N_C \\approx 509\\)\n- \\(N_{OE} = 1752\\)\n- \\(N_{\\text{slope}} = 1942\\)\n\n## **Required sample size: \\(\\boxed{N = 1942}\\)**\n\nThis corresponds to expected numbers:\n- Events: \\(0.22\\times 1942 \\approx \\boxed{427}\\)\n- Non-events: \\(\\boxed{1515}\\)",
    "latency_ms": 539591,
    "tokens": {
      "input": 154,
      "output": 12098,
      "total": 12252
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:36:20.297Z"
  },
  "t4-valid-002": {
    "task_id": "t4-valid-002",
    "tier": 4,
    "question": "We're externally validating a stroke risk model. Expected C-statistic is 0.75 in the new population, prevalence is 15%. The linear predictor follows a normal distribution (mean = -2.0, SD = 1.44). We want: C-stat CI width ≤ 0.08, calibration slope CI width ≤ 0.30, O/E CI width ≤ 0.30. What sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Goal\nFind the minimum validation sample size \\(n\\) such that (all at 95% CI):\n\n- AUC (C-statistic) CI width \\(\\le 0.08\\)  \\(\\Rightarrow\\) half-width \\(=0.04\\)\n- Calibration slope CI width \\(\\le 0.30\\) \\(\\Rightarrow\\) half-width \\(=0.15\\)\n- O/E ratio CI width \\(\\le 0.30\\)\n\nGiven:\n- Expected AUC \\(A=0.75\\)\n- Outcome prevalence \\(\\pi = 0.15\\)\n- Linear predictor \\(X \\sim N(\\mu=-2.0,\\ \\sigma=1.44)\\)\n\nFor any measure, CI width \\(\\le W\\) implies\n\\[\nSE \\le \\frac{W/2}{1.96}.\n\\]\n\nSo:\n- AUC target: \\(SE_{AUC} \\le 0.04/1.96 = 0.02041\\)\n- Slope target: \\(SE_{\\beta} \\le 0.15/1.96 = 0.07653\\)\n\n---\n\n## 1) Sample size for AUC precision\nUse Hanley–McNeil variance approximation:\n\n\\[\n\\mathrm{Var}(A)=\\frac{A(1-A) + (n_1-1)(Q_1-A^2)+(n_0-1)(Q_2-A^2)}{n_1n_0},\n\\]\nwhere\n\\[\nQ_1=\\frac{A}{2-A},\\quad Q_2=\\frac{2A^2}{1+A},\n\\]\nand \\(n_1 = n\\pi,\\ n_0=n(1-\\pi)\\).\n\nFor \\(A=0.75\\):\n- \\(Q_1=0.75/1.25=0.60\\)\n- \\(Q_2=2(0.75^2)/1.75=0.642857\\)\n- \\(A^2=0.5625\\)\n\nThus:\n- \\(Q_1-A^2=0.0375\\)\n- \\(Q_2-A^2=0.080357\\)\n- \\(A(1-A)=0.1875\\)\n\nSolving for \\(SE=\\sqrt{\\mathrm{Var}(A)} \\le 0.02041\\) gives approximately:\n\\[\nn \\approx 1392.\n\\]\n\n(Checked: at \\(n=1412\\), with \\(n_1=212\\), \\(n_0=1200\\), the resulting AUC CI width is \\(\\approx 0.0795\\), meeting the 0.08 requirement.)\n\n---\n\n## 2) Sample size for calibration slope precision\nIn validation, calibration slope \\(\\beta\\) is estimated from logistic regression:\n\\[\n\\text{logit}(P(Y=1\\mid X))=\\alpha + \\beta X,\n\\]\nwith \\(\\beta\\approx 1\\) assumed for planning, and \\(\\alpha\\) chosen so that the marginal prevalence matches \\(\\pi=0.15\\).\n\nUsing a discretized normal approximation (decile midpoints for \\(Z\\sim N(0,1)\\); weights 0.1 each), solving for \\(\\alpha\\) to achieve mean risk 0.15 gave \\(\\alpha \\approx -0.30\\).\n\nThen compute Fisher-information expectations with \\(p=\\text{expit}(\\alpha+X)\\) and \\(w=p(1-p)\\):\n\\[\nE[w] \\approx 0.10396,\\quad E[wx]\\approx -0.10506,\\quad E[wx^2]\\approx 0.22727.\n\\]\nDeterminant:\n\\[\nD=E[w]E[wx^2]-(E[wx])^2 \\approx 0.01258.\n\\]\nFor logistic regression with intercept, the large-sample variance of \\(\\hat\\beta\\) is:\n\\[\n\\mathrm{Var}(\\hat\\beta)=\\frac{E[w]}{n\\,D}\\approx \\frac{0.10396}{n(0.01258)}=\\frac{8.263}{n}.\n\\]\nSo\n\\[\nSE_\\beta=\\sqrt{\\frac{8.263}{n}} \\le 0.07653\n\\Rightarrow\nn \\ge \\frac{8.263}{0.07653^2}\n= \\frac{8.263}{0.005856}\n\\approx 1411.6.\n\\]\nThus:\n\\[\nn \\ge 1412.\n\\]\n\n---\n\n## 3) Sample size for O/E precision\nAssuming the model’s overall expected risk matches prevalence, \\(E = n\\pi\\) and \\(O\\sim\\text{Bin}(n,\\pi)\\).\n\nDelta method on \\(\\log(O/E)\\):\n\\[\n\\mathrm{Var}(\\log(O/E)) \\approx \\frac{1-\\pi}{n\\pi}.\n\\]\nSo\n\\[\nSE_{\\log(O/E)} \\approx \\sqrt{\\frac{1-\\pi}{n\\pi}}.\n\\]\n\nRequiring ratio CI width \\(\\le 0.30\\) around 1 using the exact log-scale CI width:\n\\[\n\\text{width} = \\exp(1.96SE)-\\exp(-1.96SE)=2\\sinh(1.96SE)\\le 0.30\n\\]\nimplies \\(SE \\lesssim 0.07625\\), hence:\n\\[\nn \\gtrsim \\frac{1-\\pi}{\\pi \\cdot 0.07625^2}\n= \\frac{0.85}{0.15\\cdot 0.005814}\n\\approx 975.\n\\]\nSo O/E is **not** the limiting criterion.\n\n---\n\n# Final required sample size (meeting all 3 criteria)\n\\[\n\\boxed{n = 1412}\n\\]\n\nThis corresponds to approximately:\n- Events: \\(0.15\\times 1412 = 211.8 \\Rightarrow \\boxed{212\\ \\text{events}}\\)\n- Non-events: \\(1412-212=\\boxed{1200\\ \\text{non-events}}\\)",
    "latency_ms": 239652,
    "tokens": {
      "input": 150,
      "output": 8292,
      "total": 8442
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:40:21.455Z"
  },
  "t4-valid-003": {
    "task_id": "t4-valid-003",
    "tier": 4,
    "question": "We're validating a high-discrimination model (expected C = 0.90) with prevalence 30%. The LP is normally distributed (mean = -0.5, SD = 4.0). We need very precise C-statistic (CI width ≤ 0.05), plus calibration slope and O/E CI widths ≤ 0.20. What validation sample size is required?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Goal\nFind the **minimum external validation sample size \\(N\\)** such that (at 95% confidence):\n\n- **AUC (C-statistic)** CI width \\(\\le 0.05\\)\n- **Calibration slope** CI width \\(\\le 0.20\\)\n- **O/E ratio** CI width \\(\\le 0.20\\)\n\nGiven:\n- Expected \\(C = 0.90\\)\n- Prevalence \\(\\pi = 0.30\\)  \\(\\Rightarrow\\) expected events \\(n_1=0.30N\\), non-events \\(n_0=0.70N\\)\n- Linear predictor \\(LP \\sim N(\\mu=-0.5,\\ \\sigma=4.0)\\)\n\nWe compute the required \\(N\\) for each criterion and take the maximum.\n\n---\n\n## 1) Precision for C-statistic (AUC)\nDesired 95% CI width \\(\\le 0.05\\) means half-width \\(=0.025\\), so we need:\n\\[\nSE(C) \\le \\frac{0.025}{1.96}=0.012755\n\\]\n\nUsing the **Hanley–McNeil** approximation:\n\\[\nSE^2(C)=\\frac{C(1-C)+(n_1-1)(Q_1-C^2)+(n_0-1)(Q_2-C^2)}{n_1n_0}\n\\]\nwhere\n\\[\nQ_1=\\frac{C}{2-C},\\quad Q_2=\\frac{2C^2}{1+C}\n\\]\n\nFor \\(C=0.90\\):\n- \\(Q_1=0.9/1.1=0.81818\\)\n- \\(Q_2=2(0.9^2)/(1.9)=0.85263\\)\n- \\(C^2=0.81\\)\n\nWith \\(n_1=0.3N,\\ n_0=0.7N\\), solving \\(SE(C)\\le 0.012755\\) gives:\n\\[\nN \\approx 947\n\\]\n\n**So AUC precision requires:** \\(\\boxed{N \\ge 947}\\).\n\n---\n\n## 2) Precision for O/E ratio\nAssume true \\(O/E \\approx 1\\). A common large-sample CI for \\(\\log(O/E)\\) uses:\n\\[\nSE\\{\\log(O/E)\\}\\approx \\frac{1}{\\sqrt{O}}\n\\]\nso a 95% CI is:\n\\[\n\\log(O/E) \\pm \\frac{1.96}{\\sqrt{O}}\n\\]\nWhen \\(O/E=1\\), the CI on the ratio scale is:\n\\[\n\\left[\\exp\\!\\left(-\\frac{1.96}{\\sqrt{O}}\\right),\\ \\exp\\!\\left(\\frac{1.96}{\\sqrt{O}}\\right)\\right]\n\\]\nWidth requirement:\n\\[\n\\exp\\!\\left(\\frac{1.96}{\\sqrt{O}}\\right)-\\exp\\!\\left(-\\frac{1.96}{\\sqrt{O}}\\right)\\le 0.20\n\\]\nThis implies approximately \\(O \\gtrsim 386\\) events.\n\nThus:\n\\[\n0.30N \\ge 386 \\Rightarrow N \\ge 1286.7 \\Rightarrow 1287\n\\]\n\n**So O/E precision requires:** \\(\\boxed{N \\ge 1287}\\).\n\n---\n\n## 3) Precision for calibration slope\nWe estimate slope \\(\\beta\\) from the calibration model:\n\\[\n\\text{logit}(Y)=\\alpha+\\beta \\cdot LP\n\\]\nTarget CI width \\(\\le 0.20\\) means:\n\\[\nSE(\\hat\\beta) \\le \\frac{0.10}{1.96}=0.05102\n\\]\n\nTo evaluate Fisher information, we need the true \\(p(x)=\\Pr(Y=1\\mid LP=x)\\).\nWe choose \\(\\alpha\\) so that the marginal event rate is 0.30:\n\\[\nE[\\text{expit}(\\alpha+LP)] = 0.30\n\\]\nWith \\(LP\\sim N(-0.5,16)\\), solving this gives approximately:\n\\[\n\\alpha \\approx -1.78\n\\]\n(checked numerically to yield \\(\\approx 0.30\\) prevalence).\n\nLet \\(x=LP\\), \\(p=\\text{expit}(\\alpha+x)\\), and \\(w=p(1-p)\\).\nDefine:\n\\[\na=E[w],\\quad b=E[xw],\\quad c=E[x^2w]\n\\]\nThen (for a model with intercept and slope), the large-sample variance is:\n\\[\n\\mathrm{Var}(\\hat\\beta)=\\frac{a}{N(ac-b^2)}\n\\]\n\nNumerical integration under \\(LP\\sim N(-0.5,16)\\) with \\(\\alpha\\approx -1.78\\) gives:\n- \\(a \\approx 0.07956\\)\n- \\(b \\approx 0.11269\\)\n- \\(c \\approx 0.36529\\)\n\nSo:\n\\[\nac-b^2 \\approx (0.07956)(0.36529)-(0.11269)^2 \\approx 0.01636\n\\]\n\\[\n\\mathrm{Var}(\\hat\\beta)\\approx \\frac{0.07956}{N(0.01636)} \\approx \\frac{4.862}{N}\n\\]\n\\[\nSE(\\hat\\beta)\\approx \\frac{\\sqrt{4.862}}{\\sqrt{N}}=\\frac{2.204}{\\sqrt{N}}\n\\]\n\nRequire:\n\\[\n\\frac{2.204}{\\sqrt{N}} \\le 0.05102\n\\Rightarrow \\sqrt{N}\\ge 43.2\n\\Rightarrow N \\ge 1866\n\\]\n\n**So calibration slope precision requires:** \\(\\boxed{N \\ge 1866}\\).\n\n---\n\n# Final required validation sample size\nTake the maximum of the three requirements:\n- AUC: \\(N\\ge 947\\)\n- O/E: \\(N\\ge 1287\\)\n- Slope: \\(N\\ge 1866\\)\n\n\\[\n\\boxed{N_{\\text{required}} = 1866}\n\\]\n\nThis corresponds to approximately:\n- Expected events: \\(0.30\\times 1866 \\approx 560\\)\n- Expected non-events: \\(1306\\)",
    "latency_ms": 840108,
    "tokens": {
      "input": 140,
      "output": 12001,
      "total": 12141
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:54:23.071Z"
  },
  "t4-valid-004": {
    "task_id": "t4-valid-004",
    "tier": 4,
    "question": "Our validation study has predicted probabilities following a Beta(2, 5) distribution. Expected C = 0.75, prevalence = 15%. We want: C-stat CI width ≤ 0.10, calibration slope CI width ≤ 0.20, O/E CI width ≤ 0.20. What sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Goal\nFind the minimum **total validation sample size \\(N\\)** such that (95% Wald-type CIs):\n\n- **C-statistic (AUC)** CI width \\(\\le 0.10\\)\n- **Calibration slope** CI width \\(\\le 0.20\\)\n- **O/E ratio** CI width \\(\\le 0.20\\) (interpreted on the **log(O/E)** scale, i.e., ratio CI approximately \\(\\exp(\\pm 0.10)\\), giving width \\(\\approx 0.20\\) on the ratio scale around 1)\n\nGiven:\n- Predicted risks \\(p \\sim \\text{Beta}(2,5)\\)\n- Expected \\(C = 0.75\\)\n- Event prevalence \\(\\pi = 0.15\\)\n\nWe compute the required \\(N\\) for each criterion and take the maximum.\n\n---\n\n## 1) C-statistic precision\nTarget CI width \\(\\le 0.10\\) ⇒ half-width \\(=0.05\\)\n\nFor a 95% CI:  \n\\[\n0.05 = 1.96 \\cdot SE \\;\\Rightarrow\\; SE \\le \\frac{0.05}{1.96}=0.02551\n\\]\n\nUsing the Hanley–McNeil large-sample variance approximation for AUC with:\n- \\(n_1 = \\pi N = 0.15N\\) cases\n- \\(n_0 = (1-\\pi)N = 0.85N\\) controls\n- \\(AUC=0.75\\)\n- \\(Q_1=\\frac{AUC}{2-AUC}=0.6\\)\n- \\(Q_2=\\frac{2AUC^2}{1+AUC}=0.642857\\)\n\nSolving \\(2\\cdot 1.96\\cdot SE \\le 0.10\\) gives:\n\\[\nN \\approx 891.4 \\;\\Rightarrow\\; N_C = 892\n\\]\n\n---\n\n## 2) O/E ratio precision\nFor O/E, a standard Poisson-based CI for \\(\\log(O/E)\\) uses:\n\\[\nSE\\{\\log(O/E)\\} \\approx \\frac{1}{\\sqrt{O}}\n\\]\nso 95% CI width on log-scale is:\n\\[\n2\\cdot 1.96 \\cdot \\frac{1}{\\sqrt{O}} \\le 0.20\n\\]\n\\[\n\\sqrt{O} \\ge \\frac{2\\cdot 1.96}{0.20}=19.6 \\;\\Rightarrow\\; O \\ge 384.16\n\\]\nRound up to events \\(O=385\\). With prevalence 0.15:\n\\[\nN_{OE} = \\left\\lceil \\frac{385}{0.15} \\right\\rceil = \\lceil 2566.67\\rceil = 2567\n\\]\n\n---\n\n## 3) Calibration slope precision\nTarget slope CI width \\(\\le 0.20\\) ⇒ half-width \\(=0.10\\)\n\n\\[\n0.10 = 1.96\\cdot SE \\;\\Rightarrow\\; SE \\le \\frac{0.10}{1.96}=0.05102\n\\]\n\nLet \\(x=\\text{logit}(p)\\). Calibration slope is obtained from a logistic regression of outcome on \\(x\\) (with intercept). Under correct slope (=1) but prevalence \\(\\pi=0.15\\), the “true” event probability is:\n\\[\np_{\\text{true}}=\\text{expit}(a+x)=\\frac{k\\,p}{1-p+k\\,p},\\quad k=e^a\n\\]\nWe choose \\(k\\) such that \\(E(p_{\\text{true}})=0.15\\) with \\(p\\sim\\text{Beta}(2,5)\\). Solving gives approximately:\n\\[\nk \\approx 0.4 \\quad (a=\\log 0.4\\approx -0.916)\n\\]\n\nThe per-subject Fisher information for the slope (with intercept included) is:\n\\[\nI_{\\beta}/N = E\\!\\left[w(x-\\bar x_w)^2\\right]\n\\]\nwhere \\(w=p_{\\text{true}}(1-p_{\\text{true}})\\). With \\(k=0.4\\) and \\(p\\sim\\text{Beta}(2,5)\\), this evaluates to approximately:\n\\[\nE\\!\\left[w(x-\\bar x_w)^2\\right] \\approx 0.068435\n\\]\nThus:\n\\[\nSE(\\hat\\beta) \\approx \\sqrt{\\frac{1}{N\\cdot 0.068435}}\n\\]\nRequire \\(SE \\le 0.05102\\):\n\\[\nN \\ge \\frac{1}{0.068435\\cdot (0.05102)^2}\n     = \\frac{1}{0.068435\\cdot 0.002603}\n     \\approx 5612.6\n\\]\n\\[\nN_{\\text{slope}}=5613\n\\]\n\n---\n\n# **Required sample size**\nTake the maximum of the three requirements:\n- \\(N_C = 892\\)\n- \\(N_{OE} = 2567\\)\n- \\(N_{\\text{slope}} = 5613\\)\n\n## **Final answer: \\(\\boxed{N = 5613}\\)**\nThis corresponds to expected:\n- Events: \\(0.15\\times 5613 \\approx 842\\)\n- Non-events: \\(5613-842 = 4771\\)",
    "latency_ms": 278350,
    "tokens": {
      "input": 131,
      "output": 12285,
      "total": 12416
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T05:59:02.930Z"
  },
  "t4-valid-005": {
    "task_id": "t4-valid-005",
    "tier": 4,
    "question": "We're validating the same DVT model as before (LP normal with mean = -1.75, SD = 2.16, C = 0.82, prevalence = 22%) but with more relaxed precision targets: C-stat CI width ≤ 0.10, calibration slope CI width ≤ 0.30, O/E CI width ≤ 0.30. What's the reduced sample size needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "### Given\n- LP ~ Normal(mean \\(m=-1.75\\), SD \\(s=2.16\\))\n- Target C-statistic \\(=0.82\\)\n- Outcome prevalence \\(\\pi = 0.22\\)\n- 95% CI width targets (total width):\n  - AUC width \\(\\le 0.10\\)\n  - calibration slope width \\(\\le 0.30\\)\n  - O/E width \\(\\le 0.30\\)\n\nWe compute the required \\(N\\) for each target and take the maximum.\n\n---\n\n## 1) C-statistic (AUC) CI width ≤ 0.10\nTarget half-width \\(=0.05\\), so required\n\\[\nSE(\\widehat{AUC}) \\le \\frac{0.05}{1.96}=0.02551\n\\]\n\nUse Hanley–McNeil large-sample variance:\n\\[\n\\mathrm{Var}(A)=\\frac{A(1-A) + (n_1-1)(Q_1-A^2) + (n_0-1)(Q_2-A^2)}{n_1 n_0}\n\\]\nwhere \\(n_1=\\pi N\\), \\(n_0=(1-\\pi)N\\),\n\\[\nQ_1=\\frac{A}{2-A},\\quad Q_2=\\frac{2A^2}{1+A}\n\\]\nWith \\(A=0.82\\):\n- \\(Q_1=0.82/1.18=0.6949\\)\n- \\(Q_2=2(0.82^2)/(1.82)=0.7390\\)\n- \\(A^2=0.6724\\)\n- \\(Q_1-A^2=0.0225\\)\n- \\(Q_2-A^2=0.0666\\)\n\nSolving for \\(SE \\le 0.02551\\) gives **\\(N \\approx 512\\)** (this meets the AUC width target).\n\n**So AUC requires:** \\(\\boxed{N \\ge 512}\\)\n\n---\n\n## 2) O/E ratio CI width ≤ 0.30\nIf the model is correct on average, expected events \\(E = N\\pi\\) and observed events \\(O \\sim \\text{Bin}(N,\\pi)\\) with \\(O\\approx N\\pi\\).\n\nApproximate 95% CI for \\(\\log(O/E)\\):  \n\\[\n\\log(O/E)\\pm 1.96/\\sqrt{O}\n\\]\nSo the CI for \\(O/E\\) is \\(\\left[\\exp(-h),\\exp(h)\\right]\\) where \\(h=1.96/\\sqrt{O}\\).  \nWidth on ratio scale:\n\\[\n\\text{width}=\\exp(h)-\\exp(-h)=2\\sinh(h)\\le 0.30\n\\]\nThis is essentially satisfied by \\(h\\approx 0.15\\), so:\n\\[\n\\frac{1.96}{\\sqrt{O}} \\le 0.15 \\Rightarrow O \\ge \\left(\\frac{1.96}{0.15}\\right)^2=170.7\n\\]\nThus need \\(\\ge 171\\) events, so:\n\\[\nN \\ge \\frac{171}{0.22}=777.3 \\Rightarrow 778\n\\]\n\n**So O/E requires:** \\(\\boxed{N \\ge 778}\\)\n\n---\n\n## 3) Calibration slope CI width ≤ 0.30\nTarget half-width \\(=0.15\\), so required\n\\[\nSE(\\hat\\beta)\\le \\frac{0.15}{1.96}=0.07653\n\\]\n\nEstimate slope via logistic regression with intercept + LP. With intercept in the model:\n\\[\n\\mathrm{Var}(\\hat\\beta)\\approx \\frac{1}{\\sum w_i(x_i-\\bar x_w)^2},\\quad w_i=p_i(1-p_i)\n\\]\nSo \\(SE(\\hat\\beta)\\approx 1/\\sqrt{N\\cdot S}\\), where\n\\[\nS = E\\!\\left[wX^2\\right]-\\frac{\\left(E[wX]\\right)^2}{E[w]}\n\\]\nwith \\(X=LP\\).\n\nTo ensure prevalence 0.22 under the true model \\(\\text{logit}(p)=a+X\\), we set \\(a\\) so that \\(E[p]=0.22\\). Using a standard logistic–probit approximation gives \\(a\\approx -0.37\\).\n\nNumerically integrating over \\(X\\sim N(-1.75,2.16^2)\\) yields:\n- \\(E[w]\\approx 0.1065\\)\n- \\(E[wX]\\approx -0.0461\\)\n- \\(E[wX^2]\\approx 0.2133\\)\n\nThus:\n\\[\nS = 0.2133 - \\frac{(-0.0461)^2}{0.1065} \\approx 0.1933\n\\]\nSo:\n\\[\nSE(\\hat\\beta)\\approx \\frac{1}{\\sqrt{N\\cdot 0.1933}} \\le 0.07653\n\\]\n\\[\nN\\cdot 0.1933 \\ge \\frac{1}{0.07653^2}=170.7\n\\Rightarrow N \\ge \\frac{170.7}{0.1933}=883.5\n\\Rightarrow 884\n\\]\n\n**So calibration slope requires:** \\(\\boxed{N \\ge 884}\\)\n\n---\n\n# Final (reduced) sample size needed\nTake the maximum of the three requirements:\n- AUC: 512\n- O/E: 778\n- slope: **884**\n\n## ✅ Required sample size: **\\(\\boxed{N = 884}\\)**  \nThis corresponds to about \\(\\;0.22\\times 884 \\approx \\boxed{195 \\text{ events}}\\).",
    "latency_ms": 262706,
    "tokens": {
      "input": 145,
      "output": 9874,
      "total": 10019
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T06:03:27.142Z"
  },
  "t4-valid-006": {
    "task_id": "t4-valid-006",
    "tier": 4,
    "question": "We're validating a cancer screening model and also want to assess clinical utility. Expected C = 0.80, prevalence = 10%, LP is normal (mean = -3.0, SD = 2.25). We need standard precision (C-stat CI ≤ 0.10, cal slope CI ≤ 0.20, O/E CI ≤ 0.20). Additionally, at a 10% treatment threshold, expected sensitivity is 0.80, specificity is 0.70, and we want standardized net benefit CI width ≤ 0.20. What sample size is needed?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "Let \\(N\\) be total sample size, event prevalence \\(\\pi=0.10\\) so\n\\[\nn_1=N\\pi=0.1N,\\qquad n_0=N(1-\\pi)=0.9N.\n\\]\nA 95% CI width constraint “\\(\\le w\\)” corresponds (Wald) to\n\\[\n2\\times 1.96\\times SE \\le w \\;\\Rightarrow\\; SE \\le \\frac{w}{3.92}.\n\\]\n\n---\n\n## 1) C-statistic (AUC) precision: CI width \\(\\le 0.10\\)\n\nTarget \\(SE_{\\text{AUC}} \\le 0.10/3.92=0.02551\\).\n\nUsing Hanley–McNeil variance (with \\(A=0.80\\)):\n\\[\nQ_1=\\frac{A}{2-A}=\\frac{0.8}{1.2}=0.6667,\\quad \nQ_2=\\frac{2A^2}{1+A}=\\frac{1.28}{1.8}=0.7111.\n\\]\n\\[\n\\mathrm{Var}(A)\\approx \\frac{A(1-A)+(n_1-1)(Q_1-A^2)+(n_0-1)(Q_2-A^2)}{n_1n_0}.\n\\]\nWith \\(n_1=0.1N,\\;n_0=0.9N\\), this simplifies to approximately\n\\[\n\\mathrm{Var}(A)\\approx \\frac{0.740741}{N}+\\frac{0.691358}{N^2}.\n\\]\nSolve \\(\\sqrt{\\mathrm{Var}(A)}\\le 0.02551\\) \\(\\Rightarrow N \\approx 1140\\).\n\n**Requirement:** \\(N_{\\text{AUC}} \\approx 1140\\).\n\n---\n\n## 2) Calibration slope precision: CI width \\(\\le 0.20\\)\n\nTarget \\(SE_{\\beta}\\le 0.20/3.92=0.05102\\).\n\nCalibration slope is estimated from logistic recalibration: \\(Y \\sim \\alpha + \\beta \\, LP\\).\nIts large-sample variance is\n\\[\n\\mathrm{Var}(\\hat\\beta)=\\frac{1}{N\\,D},\\quad\nD = \\mathbb{E}\\!\\left[w(LP-\\mu_w)^2\\right],\\;\\; w=p(1-p),\\;\\; p=\\text{expit}(\\alpha+LP).\n\\]\nWe use the given \\(LP\\sim N(-3.0,\\,2.25^2)\\) and set \\(\\alpha\\) so that \\(\\mathbb{E}[p]=\\pi=0.10\\) (approx \\(\\alpha\\approx -0.38\\)). Numerical approximation gives\n\\[\nD \\approx 0.1508.\n\\]\nThen\n\\[\nSE_\\beta=\\sqrt{\\frac{1}{ND}} \\le 0.05102\n\\;\\Rightarrow\\;\nN \\ge \\frac{1}{0.1508\\times 0.05102^2}\n= \\frac{1}{0.1508\\times 0.002603}\n\\approx 2551.\n\\]\n\n**Requirement:** \\(N_{\\text{slope}} \\approx 2551\\).\n\n---\n\n## 3) O/E precision: CI width \\(\\le 0.20\\)\n\nAssume expected \\(O/E\\approx 1\\). A standard approximate 95% CI for \\(O/E\\) uses\n\\[\n\\log(O/E)\\pm 1.96/\\sqrt{O}\\quad \\Rightarrow\\quad\n(O/E)\\times \\exp(\\pm 1.96/\\sqrt{O}).\n\\]\nSo the (ratio-scale) CI width is\n\\[\nW=\\exp(x)-\\exp(-x)=2\\sinh(x),\\quad x=\\frac{1.96}{\\sqrt{O}}.\n\\]\nRequire \\(W\\le 0.20\\):\n\\[\n2\\sinh(x)\\le 0.20 \\Rightarrow \\sinh(x)\\le 0.10 \\Rightarrow x\\le \\operatorname{asinh}(0.10)=0.09983.\n\\]\nThus\n\\[\n\\frac{1.96}{\\sqrt{O}}\\le 0.09983 \\Rightarrow \\sqrt{O}\\ge 19.63 \\Rightarrow O\\ge 385.3.\n\\]\nSo need \\(O\\ge 386\\) events. With \\(\\pi=0.10\\):\n\\[\n0.1N \\ge 386 \\Rightarrow N \\ge 3860.\n\\]\n\n**Requirement:** \\(N_{\\text{O/E}} = 3860\\).\n\n---\n\n## 4) Standardized net benefit precision at \\(p_t=10\\%\\): CI width \\(\\le 0.20\\)\n\nGiven sensitivity \\(Se=0.80\\), specificity \\(Sp=0.70\\), \\(\\pi=0.10\\), and threshold \\(p_t=0.10\\).\n\nStandardized net benefit (standardized by prevalence) is\n\\[\nsNB = \\frac{NB}{\\pi} = Se - \\frac{1-\\pi}{\\pi}(1-Sp)\\frac{p_t}{1-p_t}.\n\\]\nHere \\(p_t=\\pi\\), so the multiplier equals 1, and\n\\[\nsNB = Se + Sp - 1 = 0.80+0.70-1=0.50.\n\\]\nTreating \\(Se\\) and \\(Sp\\) as independent binomial estimators:\n\\[\n\\mathrm{Var}(sNB)=\\frac{Se(1-Se)}{n_1}+\\frac{Sp(1-Sp)}{n_0}\n=\\frac{0.16}{0.1N}+\\frac{0.21}{0.9N}\n=\\frac{1.83333}{N}.\n\\]\nRequire \\(SE\\le 0.20/3.92=0.05102\\):\n\\[\n\\sqrt{\\frac{1.83333}{N}} \\le 0.05102 \\Rightarrow N \\ge \\frac{1.83333}{0.002603}\\approx 705.\n\\]\n\n**Requirement:** \\(N_{\\text{sNB}} \\approx 705\\).\n\n---\n\n# Final required sample size (meet all criteria)\n\nTake the maximum:\n\\[\nN = \\max(1140,\\;2551,\\;3860,\\;705)=3860.\n\\]\n\n## **Answer: \\(N = 3860\\) total participants**\nThat corresponds to approximately **386 events** and **3474 non-events** (at 10% prevalence).",
    "latency_ms": 546038,
    "tokens": {
      "input": 184,
      "output": 9562,
      "total": 9746
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T06:12:34.685Z"
  },
  "t4-valid-007": {
    "task_id": "t4-valid-007",
    "tier": 4,
    "question": "I'm validating an existing prediction model in a new population. The original C-statistic was 0.80, but I expect it to be 0.75 in this new population. Outcome prevalence is 12%. I want the 95% CI width for the C-statistic to be at most 0.10. What sample size do I need?",
    "model": "gpt-5.2-pro",
    "model_display": "GPT-5.2 Pro",
    "provider": "openai",
    "response_text": "To get a 95% CI **total width ≤ 0.10** for an expected C-statistic (AUC) of **0.75** with outcome prevalence **12%**, we can use the large-sample variance approximation for the AUC (Hanley–McNeil), and then solve for \\(N\\) such that\n\n\\[\n\\text{half-width}=1.96\\times SE(\\widehat{AUC}) \\le 0.05\n\\quad\\Rightarrow\\quad\nSE(\\widehat{AUC}) \\le \\frac{0.05}{1.96}=0.0255102.\n\\]\n\n### 1) AUC variance (Hanley–McNeil)\nLet \\(A=0.75\\), \\(n_1=\\) #cases, \\(n_0=\\) #controls.\n\n\\[\n\\operatorname{Var}(\\widehat{A})=\n\\frac{A(1-A) + (n_1-1)(Q_1-A^2) + (n_0-1)(Q_2-A^2)}{n_1 n_0}\n\\]\nwhere\n\\[\nQ_1=\\frac{A}{2-A},\\qquad Q_2=\\frac{2A^2}{1+A}.\n\\]\n\nCompute:\n- \\(A^2=0.5625\\)\n- \\(Q_1=0.75/(2-0.75)=0.75/1.25=0.6\\)  so \\(Q_1-A^2=0.0375\\)\n- \\(Q_2=2(0.5625)/(1.75)=1.125/1.75=0.6428571\\) so \\(Q_2-A^2=0.0803571\\)\n- \\(A(1-A)=0.1875\\)\n\nWith prevalence \\(p=0.12\\): \\(n_1=pN=0.12N\\), \\(n_0=(1-p)N=0.88N\\).\n\nThis yields (after substitution and simplification):\n\\[\n\\operatorname{Var}(\\widehat{A}) \\approx \\frac{0.71226}{N} + \\frac{0.65950}{N^2}.\n\\]\n\n### 2) Solve for \\(N\\)\nRequire:\n\\[\n\\frac{0.71226}{N} + \\frac{0.65950}{N^2} \\le (0.0255102)^2 = 0.00065077.\n\\]\n\nSolving gives:\n\\[\nN \\approx 1095.6 \\;\\Rightarrow\\; \\boxed{N=1096}\\ (\\text{round up}).\n\\]\n\n### What this corresponds to in events/non-events\n- Expected cases: \\(0.12\\times 1096 = 131.5\\) → about **132 events**\n- Controls: \\(1096-132=964\\) non-events\n\n## **Final answer (prominent)**\n\\[\n\\boxed{\\textbf{You need approximately } N=1096 \\textbf{ participants total (about 132 events and 964 non-events).}}\n\\]\n\n(Uses a Wald-type 95% CI width target and the Hanley–McNeil AUC variance approximation.)",
    "latency_ms": 77286,
    "tokens": {
      "input": 134,
      "output": 2707,
      "total": 2841
    },
    "finish_reason": "completed",
    "model_used": "gpt-5.2-pro-2025-12-11",
    "timestamp": "2026-02-22T06:13:53.476Z"
  }
}