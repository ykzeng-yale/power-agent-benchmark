[
  {
    "id": "t1-ttest-001",
    "template": "two_sample_ttest",
    "difficulty": "basic",
    "question": "I want to compare mean blood pressure between treatment and control groups. I expect a medium effect size (Cohen's d = 0.5), want 80% power at alpha = 0.05 using a two-sided test. How many participants do I need per group?",
    "expected_template": "two_sample_ttest",
    "ground_truth": {
      "sample_size_per_group": 64,
      "total_sample_size": 128,
      "effect_size_d": 0.5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (Example 12: Two-Sample t-Test)",
    "reference_code": "library(pwr)\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = 'two.sample', alternative = 'two.sided')\n# n = 63.77 per group, round up to 64\n# Source: https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html",
    "tier": 1
  },
  {
    "id": "t1-ttest-002",
    "template": "two_sample_ttest",
    "difficulty": "basic",
    "question": "Testing if a new drug reduces cholesterol. Expected reduction is 15 mg/dL with SD of 30 mg/dL in both groups. I need 90% power with two-sided alpha = 0.05. What sample size per group?",
    "expected_template": "two_sample_ttest",
    "ground_truth": {
      "sample_size_per_group": 86,
      "total_sample_size": 172,
      "effect_size_d": 0.5,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nd <- 15/30  # effect size = difference/SD = 0.5\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.90, type = 'two.sample')\n# n = 85.03 per group, round up to 86",
    "tier": 1
  },
  {
    "id": "t1-ttest-003",
    "template": "two_sample_ttest",
    "difficulty": "intermediate",
    "question": "I'm comparing pain scores between a new treatment and standard care. Based on prior studies, we expect only a small improvement (d = 0.3), but we believe the new treatment will be better, so we're using a one-sided test. How many patients per arm do we need for 80% power at alpha = 0.05?",
    "expected_template": "two_sample_ttest",
    "ground_truth": {
      "sample_size_per_group": 139,
      "total_sample_size": 278,
      "effect_size_d": 0.3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.t.test(d = 0.3, sig.level = 0.05, power = 0.80, type = 'two.sample', alternative = 'greater')\n# n = 138.08 per group, round up to 139",
    "tier": 1
  },
  {
    "id": "t1-ttest-004",
    "template": "two_sample_ttest",
    "difficulty": "intermediate",
    "question": "We're running a trial comparing CBT therapy to a waitlist control for anxiety. Based on pilot data, we expect an 8-point difference on the anxiety scale, with a pooled SD of about 12 points. We want to be more rigorous with alpha = 0.01. How many participants per group for 90% power?",
    "expected_template": "two_sample_ttest",
    "ground_truth": {
      "sample_size_per_group": 69,
      "total_sample_size": 138,
      "effect_size_d": 0.667,
      "power": 0.9,
      "alpha": 0.01
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nd <- 8/12  # effect size = 0.667\npwr.t.test(d = 0.667, sig.level = 0.01, power = 0.90, type = 'two.sample')\n# n = 68.11 per group, round up to 69",
    "tier": 1
  },
  {
    "id": "t1-ttest-005",
    "template": "two_sample_ttest",
    "difficulty": "advanced",
    "question": "We're planning an RCT to see if continuous glucose monitors improve HbA1c compared to standard monitoring. We expect a 0.4% difference with SD of 1.0%. Since we're testing superiority (one-sided), we'll use alpha = 0.025. How many patients per group for 95% power?",
    "expected_template": "two_sample_ttest",
    "ground_truth": {
      "sample_size_per_group": 164,
      "total_sample_size": 328,
      "effect_size_d": 0.4,
      "power": 0.95,
      "alpha": 0.025
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nd <- 0.4/1.0\npwr.t.test(d = 0.4, sig.level = 0.025, power = 0.95, type = 'two.sample', alternative = 'greater')\n# n = 163.12 per group, round up to 164",
    "tier": 1
  },
  {
    "id": "t1-ttest-006",
    "template": "two_sample_ttest",
    "difficulty": "advanced",
    "question": "A pharmaceutical company needs to detect a large effect (d = 0.8) in a confirmatory phase III trial. They require 99% power with alpha = 0.001 (two-sided) for regulatory purposes. What's the sample size per treatment arm?",
    "expected_template": "two_sample_ttest",
    "ground_truth": {
      "sample_size_per_group": 102,
      "total_sample_size": 204,
      "effect_size_d": 0.8,
      "power": 0.99,
      "alpha": 0.001
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.t.test(d = 0.8, sig.level = 0.001, power = 0.99, type = 'two.sample')\n# n = 101.11 per group, round up to 102",
    "tier": 1
  },
  {
    "id": "t1-paired-001",
    "template": "paired_ttest",
    "difficulty": "basic",
    "question": "I'm measuring patients' depression scores before and after 8 weeks of treatment. Expected improvement is 5 points with SD of changes = 10 (d = 0.5). With 80% power and alpha = 0.05, how many patients do I need?",
    "expected_template": "paired_ttest",
    "ground_truth": {
      "sample_size": 34,
      "effect_size_d": 0.5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = 'paired')\n# n = 33.37, round up to 34",
    "tier": 1
  },
  {
    "id": "t1-paired-002",
    "template": "paired_ttest",
    "difficulty": "basic",
    "question": "I'm running a weight loss intervention and will measure participants before and after. We expect about 3 kg average loss with individual variation (SD of the changes) around 5 kg. How many participants do I need for 90% power at alpha = 0.05?",
    "expected_template": "paired_ttest",
    "ground_truth": {
      "sample_size": 32,
      "effect_size_d": 0.6,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nd <- 3/5  # effect size = 0.6\npwr.t.test(d = 0.6, sig.level = 0.05, power = 0.90, type = 'paired')\n# n = 31.15, round up to 32",
    "tier": 1
  },
  {
    "id": "t1-paired-003",
    "template": "paired_ttest",
    "difficulty": "intermediate",
    "question": "We're doing a crossover bioequivalence study comparing two drug formulations. The expected difference in AUC between formulations is 50 units, with within-subject SD of 100. How many subjects do we need for 85% power at alpha = 0.05?",
    "expected_template": "paired_ttest",
    "ground_truth": {
      "sample_size": 38,
      "effect_size_d": 0.5,
      "power": 0.85,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nd <- 50/100  # effect size = 0.5\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.85, type = 'paired')\n# n = 37.99, round up to 38",
    "tier": 1
  },
  {
    "id": "t1-paired-004",
    "template": "paired_ttest",
    "difficulty": "advanced",
    "question": "In our bioequivalence crossover study, we need to be able to detect even small differences (d = 0.25) with high confidence - 95% power at the stricter alpha = 0.01. How many subjects?",
    "expected_template": "paired_ttest",
    "ground_truth": {
      "sample_size": 289,
      "effect_size_d": 0.25,
      "power": 0.95,
      "alpha": 0.01
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.t.test(d = 0.25, sig.level = 0.01, power = 0.95, type = 'paired')\n# n = 288.31, round up to 289",
    "tier": 1
  },
  {
    "id": "t1-anova-001",
    "template": "one_way_anova",
    "difficulty": "basic",
    "question": "We're comparing 3 treatment groups for pain relief and expect a medium effect size (Cohen's f = 0.25). How many subjects per group for 80% power at alpha = 0.05?",
    "expected_template": "one_way_anova",
    "ground_truth": {
      "sample_size_per_group": 53,
      "total_sample_size": 159,
      "effect_size_f": 0.25,
      "power": 0.8,
      "alpha": 0.05,
      "groups": 3
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.80)\n# n = 52.40 per group, round up to 53",
    "tier": 1
  },
  {
    "id": "t1-anova-002",
    "template": "one_way_anova",
    "difficulty": "basic",
    "question": "We're testing 4 different dosage levels of a medication and expect to see an effect size of f = 0.30 between groups. How many patients per dosage group for 80% power at alpha = 0.05?",
    "expected_template": "one_way_anova",
    "ground_truth": {
      "sample_size_per_group": 32,
      "total_sample_size": 128,
      "effect_size_f": 0.3,
      "power": 0.8,
      "alpha": 0.05,
      "groups": 4
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.anova.test(k = 4, f = 0.30, sig.level = 0.05, power = 0.80)\n# n = 31.28 per group, round up to 32",
    "tier": 1
  },
  {
    "id": "t1-anova-003",
    "template": "one_way_anova",
    "difficulty": "intermediate",
    "question": "We're comparing 5 different exercise interventions for VO2max improvement. We expect a small-medium effect (f = 0.20). How many subjects per group for 90% power at alpha = 0.05?",
    "expected_template": "one_way_anova",
    "ground_truth": {
      "sample_size_per_group": 78,
      "total_sample_size": 390,
      "effect_size_f": 0.2,
      "power": 0.9,
      "alpha": 0.05,
      "groups": 5
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.anova.test(k = 5, f = 0.20, sig.level = 0.05, power = 0.90)\n# n = 77.77 per group, round up to 78",
    "tier": 1
  },
  {
    "id": "t1-anova-004",
    "template": "one_way_anova",
    "difficulty": "intermediate",
    "question": "We're comparing three different diets for LDL cholesterol reduction. Based on the literature, we expect reductions of about 10, 20, and 25 mg/dL for the three diets, with a common SD of 20 mg/dL. How many participants per diet group for 80% power at alpha = 0.05?",
    "expected_template": "one_way_anova",
    "ground_truth": {
      "sample_size_per_group": 35,
      "total_sample_size": 105,
      "effect_size_f": 0.312,
      "power": 0.8,
      "alpha": 0.05,
      "groups": 3
    },
    "tolerance": {
      "sample_size": 2,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\n# Calculate f from group means\nmeans <- c(-10, -20, -25)\ngrand_mean <- mean(means)\nbetween_var <- sum((means - grand_mean)^2) / 3\nf <- sqrt(between_var) / 20  # f = 0.312\npwr.anova.test(k = 3, f = 0.312, sig.level = 0.05, power = 0.80)\n# n = 34.15 per group, round up to 35",
    "tier": 1
  },
  {
    "id": "t1-anova-005",
    "template": "one_way_anova",
    "difficulty": "advanced",
    "question": "We're conducting a large multi-center trial across 6 treatment centers and expect a large effect (f = 0.40). Due to regulatory requirements, we need 95% power at a stringent alpha = 0.01. How many subjects per center?",
    "expected_template": "one_way_anova",
    "ground_truth": {
      "sample_size_per_group": 29,
      "total_sample_size": 174,
      "effect_size_f": 0.4,
      "power": 0.95,
      "alpha": 0.01,
      "groups": 6
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.anova.test(k = 6, f = 0.40, sig.level = 0.01, power = 0.95)\n# n = 28.23 per group, round up to 29",
    "tier": 1
  },
  {
    "id": "t1-prop-001",
    "template": "two_proportions",
    "difficulty": "basic",
    "question": "We're testing a new treatment where we expect the response rate to improve from 40% (control) to 60% (treatment). How many patients per group for 80% power at alpha = 0.05 (two-sided)?",
    "expected_template": "two_proportions",
    "ground_truth": {
      "sample_size_per_group": 97,
      "total_sample_size": 194,
      "p1": 0.4,
      "p2": 0.6,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nh <- ES.h(0.60, 0.40)  # h = 0.4028\npwr.2p.test(h = h, sig.level = 0.05, power = 0.80)\n# n = 96.70 per group, round up to 97",
    "tier": 1
  },
  {
    "id": "t1-prop-002",
    "template": "two_proportions",
    "difficulty": "basic",
    "question": "Our current drug has a 30% adverse event rate. We're hoping a new formulation will reduce this to 20%. How many patients per arm do we need to detect this difference with 80% power at alpha = 0.05?",
    "expected_template": "two_proportions",
    "ground_truth": {
      "sample_size_per_group": 292,
      "total_sample_size": 584,
      "p1": 0.3,
      "p2": 0.2,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nh <- ES.h(0.30, 0.20)  # h = 0.2319\npwr.2p.test(h = h, sig.level = 0.05, power = 0.80)\n# n = 291.20 per group, round up to 292",
    "tier": 1
  },
  {
    "id": "t1-prop-003",
    "template": "two_proportions",
    "difficulty": "intermediate",
    "question": "We're planning a vaccine efficacy trial. The placebo group is expected to have a 10% infection rate, and we hope the vaccine reduces this to 5%. Since we're testing efficacy (one-sided), we'll use alpha = 0.025. How many per group for 90% power?",
    "expected_template": "two_proportions",
    "ground_truth": {
      "sample_size_per_group": 582,
      "total_sample_size": 1164,
      "p1": 0.1,
      "p2": 0.05,
      "power": 0.9,
      "alpha": 0.025
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "power.prop.test(p1 = 0.10, p2 = 0.05, power = 0.90, sig.level = 0.025, alternative = 'one.sided')\n# n = 581.02 per group, round up to 582",
    "tier": 1
  },
  {
    "id": "t1-prop-004",
    "template": "two_proportions",
    "difficulty": "intermediate",
    "question": "We're comparing a new surgical technique to the standard approach. The standard technique has about 85% success rate, and we expect the new technique to achieve 95%. How many patients per group for 80% power at alpha = 0.05?",
    "expected_template": "two_proportions",
    "ground_truth": {
      "sample_size_per_group": 133,
      "total_sample_size": 266,
      "p1": 0.85,
      "p2": 0.95,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\nh <- ES.h(0.95, 0.85)  # h = 0.3440\npwr.2p.test(h = h, sig.level = 0.05, power = 0.80)\n# n = 132.20 per group, round up to 133",
    "tier": 1
  },
  {
    "id": "t1-prop-005",
    "template": "two_proportions",
    "difficulty": "advanced",
    "question": "We're planning a non-inferiority trial for a new treatment. The standard treatment has about 70% response rate, and we expect the new treatment to be similar. We need to rule out that the new treatment is more than 10% worse - so if the true rate is 60% or lower, we should detect it. How many patients per group for 90% power with one-sided alpha = 0.025?",
    "expected_template": "two_proportions",
    "ground_truth": {
      "sample_size_per_group": 477,
      "total_sample_size": 954,
      "p1": 0.7,
      "p2": 0.6,
      "power": 0.9,
      "alpha": 0.025
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "power.prop.test(p1 = 0.70, p2 = 0.60, power = 0.90, sig.level = 0.025, alternative = 'one.sided')\n# n = 476.68 per group, round up to 477",
    "tier": 1
  },
  {
    "id": "t1-chi-001",
    "template": "chi_square",
    "difficulty": "basic",
    "question": "I'm testing independence in a 2\u00d72 table and want 80% power to detect a medium effect (w = 0.3) at alpha = 0.05. What total sample size do I need?",
    "expected_template": "chi_square_test",
    "ground_truth": {
      "total_sample_size": 88,
      "effect_size_w": 0.3,
      "power": 0.8,
      "alpha": 0.05,
      "df": 1
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.chisq.test(w = 0.3, df = 1, sig.level = 0.05, power = 0.80)\n# N = 87.21, round up to 88",
    "tier": 1
  },
  {
    "id": "t1-chi-002",
    "template": "chi_square",
    "difficulty": "basic",
    "question": "I'm analyzing a 3\u00d72 contingency table (3 treatment groups, 2 outcome categories) and want to detect an effect size of w = 0.25 with 80% power at alpha = 0.05. What total N is needed?",
    "expected_template": "chi_square_test",
    "ground_truth": {
      "total_sample_size": 155,
      "effect_size_w": 0.25,
      "power": 0.8,
      "alpha": 0.05,
      "df": 2
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.chisq.test(w = 0.25, df = 2, sig.level = 0.05, power = 0.80)\n# N = 154.30, round up to 155",
    "tier": 1
  },
  {
    "id": "t1-chi-003",
    "template": "chi_square",
    "difficulty": "intermediate",
    "question": "I'm testing association in a 3\u00d74 contingency table. I want to detect an effect size of w = 0.3 with 80% power at alpha = 0.05. What total sample size do I need?",
    "expected_template": "chi_square_test",
    "ground_truth": {
      "total_sample_size": 152,
      "effect_size_w": 0.3,
      "power": 0.8,
      "alpha": 0.05,
      "df": 6
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\n# df = (3-1)*(4-1) = 6\npwr.chisq.test(w = 0.3, df = 6, sig.level = 0.05, power = 0.80)\n# N = 151.40, round up to 152",
    "tier": 1
  },
  {
    "id": "t1-chi-004",
    "template": "chi_square",
    "difficulty": "intermediate",
    "question": "We're analyzing genotype-phenotype association in a 4\u00d74 table and need to detect a small effect (w = 0.2) with 90% power at a stringent alpha = 0.01. What total sample size?",
    "expected_template": "chi_square_test",
    "ground_truth": {
      "total_sample_size": 654,
      "effect_size_w": 0.2,
      "power": 0.9,
      "alpha": 0.01,
      "df": 9
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\n# df = (4-1)*(4-1) = 9\npwr.chisq.test(w = 0.2, df = 9, sig.level = 0.01, power = 0.90)\n# N = 653.06, round up to 654",
    "tier": 1
  },
  {
    "id": "t1-chi-005",
    "template": "chi_square",
    "difficulty": "advanced",
    "question": "I'm running a goodness-of-fit test with 5 categories, comparing observed vs expected distributions. I want to detect an effect size w = 0.35 with 85% power at alpha = 0.05. What sample size?",
    "expected_template": "chi_square_test",
    "ground_truth": {
      "total_sample_size": 110,
      "effect_size_w": 0.35,
      "power": 0.85,
      "alpha": 0.05,
      "df": 4
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\n# df = 5 - 1 = 4 for goodness-of-fit\npwr.chisq.test(w = 0.35, df = 4, sig.level = 0.05, power = 0.85)\n# N = 109.36, round up to 110",
    "tier": 1
  },
  {
    "id": "t1-corr-001",
    "template": "correlation",
    "difficulty": "basic",
    "question": "I want to investigate whether BMI is correlated with blood pressure in our patient population. Based on the literature, I expect a moderate correlation of about r = 0.3. How many patients do I need for 80% power at alpha = 0.05?",
    "expected_template": "correlation",
    "ground_truth": {
      "sample_size": 85,
      "correlation_r": 0.3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.r.test(r = 0.3, sig.level = 0.05, power = 0.80)\n# n = 84.07, round up to 85",
    "tier": 1
  },
  {
    "id": "t1-corr-002",
    "template": "correlation",
    "difficulty": "basic",
    "question": "We're studying whether age is correlated with cognitive decline scores. We expect a modest correlation of r = 0.25. How many participants for 90% power at alpha = 0.05?",
    "expected_template": "correlation",
    "ground_truth": {
      "sample_size": 164,
      "correlation_r": 0.25,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.r.test(r = 0.25, sig.level = 0.05, power = 0.90)\n# n = 163.05, round up to 164",
    "tier": 1
  },
  {
    "id": "t1-corr-003",
    "template": "correlation",
    "difficulty": "intermediate",
    "question": "I'm investigating whether a novel biomarker correlates with patient outcomes. The expected correlation is small (r = 0.15), but clinically meaningful. How many patients do I need for 80% power at alpha = 0.05?",
    "expected_template": "correlation",
    "ground_truth": {
      "sample_size": 346,
      "correlation_r": 0.15,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.r.test(r = 0.15, sig.level = 0.05, power = 0.80)\n# n = 345.68, round up to 346",
    "tier": 1
  },
  {
    "id": "t1-corr-004",
    "template": "correlation",
    "difficulty": "intermediate",
    "question": "We expect a strong correlation (r = 0.5) between exercise frequency and VO2max. We want high confidence - 95% power with alpha = 0.01. What sample size do we need?",
    "expected_template": "correlation",
    "ground_truth": {
      "sample_size": 62,
      "correlation_r": 0.5,
      "power": 0.95,
      "alpha": 0.01
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.r.test(r = 0.5, sig.level = 0.01, power = 0.95)\n# n = 61.35, round up to 62",
    "tier": 1
  },
  {
    "id": "t1-corr-005",
    "template": "correlation",
    "difficulty": "advanced",
    "question": "I hypothesize that medication adherence is positively correlated with health outcomes (r = 0.2). Since I'm predicting a positive direction, I'll use a one-sided test at alpha = 0.025. How many patients for 85% power?",
    "expected_template": "correlation",
    "ground_truth": {
      "sample_size": 221,
      "correlation_r": 0.2,
      "power": 0.85,
      "alpha": 0.025
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\npwr.r.test(r = 0.2, sig.level = 0.025, power = 0.85, alternative = 'greater')\n# n = 220.28, round up to 221",
    "tier": 1
  },
  {
    "id": "t2-linreg-001",
    "template": "linear_regression",
    "difficulty": "basic",
    "question": "I'm planning a study where I'll use multiple regression with 5 predictors to predict patient outcomes. I want to be able to detect if these predictors together explain at least 10% of the variance in the outcome. How many subjects do I need for 80% power with alpha = 0.05?",
    "expected_template": "linear_regression",
    "ground_truth": {
      "sample_size": 122,
      "r2": 0.1,
      "predictors": 5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (pwr.f2.test)",
    "reference_code": "library(pwr)\nf2 <- 0.10 / (1 - 0.10)  # f2 = 0.111\npwr.f2.test(u = 5, f2 = f2, sig.level = 0.05, power = 0.80)\n# v = 115.1, n = v + u + 1 = 122",
    "tier": 2
  },
  {
    "id": "t2-linreg-002",
    "template": "linear_regression",
    "difficulty": "basic",
    "question": "I want to test whether a single biomarker predicts disease severity. If the biomarker explains about 15% of the variance in severity scores, how many patients do I need to detect this with 90% power at alpha = 0.05?",
    "expected_template": "linear_regression",
    "ground_truth": {
      "sample_size": 62,
      "r2": 0.15,
      "predictors": 1,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (pwr.f2.test)",
    "reference_code": "library(pwr)\nf2 <- 0.15 / (1 - 0.15)  # f2 = 0.176\npwr.f2.test(u = 1, f2 = f2, sig.level = 0.05, power = 0.90)\n# v = 59.5, n = v + u + 1 = 62",
    "tier": 2
  },
  {
    "id": "t2-linreg-003",
    "template": "linear_regression",
    "difficulty": "intermediate",
    "question": "I have a regression model with 4 predictors that explains 20% of the variance in patient recovery time. I want to test whether adding 3 new genetic markers improves the model to explain 30% of the variance. How many subjects do I need to detect this incremental improvement with 80% power at alpha = 0.05?",
    "expected_template": "linear_regression",
    "ground_truth": {
      "sample_size": 85,
      "r2_full": 0.3,
      "r2_reduced": 0.2,
      "predictors_tested": 3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (pwr.f2.test)",
    "reference_code": "library(pwr)\n# f2 for incremental R\u00b2 test\nf2 <- (0.30 - 0.20) / (1 - 0.30)  # f2 = 0.143\nresult <- pwr.f2.test(u = 3, f2 = f2, sig.level = 0.05, power = 0.80)\n# v = 76.3, p_total = 4 + 3 = 7, n = v + p_total + 1 = 76.3 + 7 + 1 = 85",
    "tier": 2
  },
  {
    "id": "t2-linreg-004",
    "template": "linear_regression",
    "difficulty": "intermediate",
    "question": "We're building a prediction model with 10 predictors. We want to detect a medium effect size (Cohen's f\u00b2 = 0.15) with 85% power at a more stringent alpha = 0.01. What sample size do we need?",
    "expected_template": "linear_regression",
    "ground_truth": {
      "sample_size": 174,
      "f2": 0.15,
      "predictors": 10,
      "power": 0.85,
      "alpha": 0.01
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (pwr.f2.test)",
    "reference_code": "library(pwr)\npwr.f2.test(u = 10, f2 = 0.15, sig.level = 0.01, power = 0.85)\n# v = 162.5, n = v + u + 1 = 174",
    "tier": 2
  },
  {
    "id": "t2-linreg-005",
    "template": "linear_regression",
    "difficulty": "advanced",
    "question": "In my regression model with 8 predictors, I want to test whether one specific predictor contributes meaningfully. I expect this predictor to explain an additional 5% of variance (partial f\u00b2 \u2248 0.053). How many subjects for 90% power at alpha = 0.05?",
    "expected_template": "linear_regression",
    "ground_truth": {
      "sample_size": 208,
      "f2": 0.053,
      "predictors": 8,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (pwr.f2.test)",
    "reference_code": "library(pwr)\n# partial f\u00b2 = partial R\u00b2 / (1 - full R\u00b2)\n# If partial R\u00b2 = 0.05, with full R\u00b2 \u2248 0.20, f\u00b2 \u2248 0.053\npwr.f2.test(u = 1, f2 = 0.053, sig.level = 0.05, power = 0.90)\n# v = 198.3, n = v + 8 + 1 = 208",
    "tier": 2
  },
  {
    "id": "t2-linreg-006",
    "template": "linear_regression",
    "difficulty": "advanced",
    "question": "We're conducting a large epidemiological study with 15 risk factors as predictors. We expect only a small effect (R\u00b2 around 5%) since these are observational data. What sample size do we need for 95% power with alpha = 0.01?",
    "expected_template": "linear_regression",
    "ground_truth": {
      "sample_size": 688,
      "r2": 0.05,
      "predictors": 15,
      "power": 0.95,
      "alpha": 0.01
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette (pwr.f2.test)",
    "reference_code": "library(pwr)\nf2 <- 0.05 / (1 - 0.05)  # f2 = 0.0526\npwr.f2.test(u = 15, f2 = f2, sig.level = 0.01, power = 0.95)\n# v = 672, n = v + u + 1 = 688",
    "tier": 2
  },
  {
    "id": "t2-logreg-001",
    "template": "logistic_regression",
    "difficulty": "basic",
    "question": "I'm analyzing whether a binary exposure (present in 50% of patients) predicts disease. The outcome rate is 20% in unexposed patients, and I expect an odds ratio of 2.0 for exposed vs unexposed. How many patients total for 80% power at alpha = 0.05?",
    "expected_template": "logistic_regression",
    "ground_truth": {
      "sample_size": 347,
      "odds_ratio": 2,
      "p1": 0.2,
      "p2": 0.333,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "pwrss package CRAN documentation",
    "reference_code": "library(pwrss)\n# p2 = OR * p1 / (1 + (OR-1)*p1) = 2*0.2/(1+0.2) = 0.333\npwrss.z.logreg(p0 = 0.20, p1 = 0.333, r2.other.x = 0, power = 0.80, alpha = 0.05, dist = 'binomial')\n# n = 347 (Demidenko variance-corrected method)",
    "tier": 2
  },
  {
    "id": "t2-logreg-002",
    "template": "logistic_regression",
    "difficulty": "basic",
    "question": "In our case-control study, I want to detect whether a continuous exposure is associated with disease. I expect an odds ratio of 1.5 per standard deviation increase in the exposure. The baseline disease risk is about 10%. How many subjects for 80% power at alpha = 0.05?",
    "expected_template": "logistic_regression",
    "ground_truth": {
      "sample_size": 522,
      "odds_ratio": 1.5,
      "baseline_risk": 0.1,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "pwrss (R-validated 2026-02-01)",
    "reference_code": "library(pwrss)\npwrss.z.logreg(p0 = 0.10, p1 = 0.143, r2.other.x = 0, power = 0.80, alpha = 0.05, dist = 'normal')\n# n \u2248 565",
    "reference_code_note": "pwrss.z.logreg(p0=0.10, p1=0.143, r2.other.x=0, dist='normal') gives N=522",
    "tier": 2
  },
  {
    "id": "t2-logreg-003",
    "template": "logistic_regression",
    "difficulty": "intermediate",
    "question": "I'm building a multivariable logistic regression to predict readmission risk with 5 covariates. My main predictor is a continuous biomarker (normally distributed). I expect an odds ratio of 1.8 per SD increase in this biomarker, while adjusting for confounders that together explain about 10% of the variance in the predictor. Baseline readmission rate is 15%. How many patients for 80% power at alpha = 0.05?",
    "expected_template": "logistic_regression",
    "ground_truth": {
      "sample_size": 204,
      "odds_ratio": 1.8,
      "baseline_risk": 0.15,
      "r2_other": 0.1,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "pwrss package CRAN documentation",
    "reference_code": "library(pwrss)\n# p1 = OR*p0/(1+(OR-1)*p0) = 1.8*0.15/(1+0.8*0.15) = 0.241\npwrss.z.logreg(p0 = 0.15, p1 = 0.241, r2.other.x = 0.10, power = 0.80, alpha = 0.05)\n# n = 204 (Demidenko variance-corrected method)",
    "tier": 2
  },
  {
    "id": "t2-logreg-004",
    "template": "logistic_regression",
    "difficulty": "intermediate",
    "question": "I'm studying a protective factor with an expected odds ratio of 0.6 (40% reduction in odds). The exposure is binary with 50% prevalence, and baseline disease rate in unexposed is 25%. What sample size do I need for 90% power at alpha = 0.05?",
    "expected_template": "logistic_regression",
    "ground_truth": {
      "sample_size": 999,
      "odds_ratio": 0.6,
      "baseline_risk": 0.25,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 15,
      "power": 0.03
    },
    "source": "pwrss (R-validated 2026-01-28)",
    "reference_code": "library(pwrss)\n# p1 = 0.6*0.25/(1+(0.6-1)*0.25) = 0.167\npwrss.z.logreg(p0=0.25, p1=0.167, r2.other.x=0, power=0.90, alpha=0.05, dist='binomial')\n# n = 999 (Demidenko variance-corrected, binomial distribution)",
    "tier": 2
  },
  {
    "id": "t2-logreg-005",
    "template": "logistic_regression",
    "difficulty": "advanced",
    "question": "We're studying a rare outcome (5% baseline risk) and a binary exposure that's present in only 30% of the population. We expect an OR of 2.5 for exposed vs unexposed. What total sample size is needed for 80% power at alpha = 0.05?",
    "expected_template": "logistic_regression",
    "ground_truth": {
      "sample_size": 610,
      "odds_ratio": 2.5,
      "baseline_risk": 0.05,
      "predictor_prevalence": 0.3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 15,
      "power": 0.03
    },
    "source": "pwrss package with 30% predictor prevalence (R-validated 2026-02-05)",
    "reference_code": "library(pwrss)\n# p1 = 2.5*0.05/(1+1.5*0.05) = 0.116\n# Must specify 30% predictor prevalence (not default 50%)\npwrss.z.logreg(p0 = 0.05, p1 = 0.116, r2.other.x = 0, power = 0.80, alpha = 0.05,\n               dist = list(dist='binomial', size=1, prob=0.3))\n# n = 610 (with 30% predictor prevalence)",
    "tier": 2
  },
  {
    "id": "t2-logreg-006",
    "template": "logistic_regression",
    "difficulty": "advanced",
    "question": "I'm testing for an interaction between two binary risk factors in a logistic regression. Each factor has 50% prevalence and they're independent. Baseline risk (neither factor present) is 20%, main effect OR for each is 1.5, and the interaction OR is 2.0. What sample size do I need to detect this interaction with 80% power at alpha = 0.05?",
    "expected_template": "logistic_regression",
    "ground_truth": {
      "sample_size": 319,
      "interaction_or": 2,
      "main_or": 1.5,
      "baseline_risk": 0.2,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 16,
      "power": 0.03
    },
    "source": "pwrss (R-validated 2026-02-01)",
    "reference_code": "library(pwrss)\nb0 <- log(0.20/0.80); b1 <- log(1.5); b2 <- log(1.5); b3 <- log(2.0)\np00 <- 0.20; p10 <- 1/(1+exp(-(b0+b1))); p01 <- 1/(1+exp(-(b0+b2)))\np11 <- 1/(1+exp(-(b0+b1+b2+b3)))\np_no_int <- p10*p01/p00  # = 0.372\npwrss.z.logreg(p0=p_no_int, p1=p11, r2.other.x=0.15, power=0.80, alpha=0.05, dist='binomial')\n# n = 370",
    "reference_code_note": "pwrss.z.logreg with p0=0.36, p1=0.529, r2.other.x=0.15 gives N=319",
    "tier": 2
  },
  {
    "id": "t2-mixed-001",
    "template": "mixed_effects_lmm",
    "difficulty": "basic",
    "question": "I'm planning a longitudinal study with 4 repeated measures per subject, comparing treatment vs control. I expect a medium effect size (d = 0.5) between groups, and the ICC for within-subject correlation is about 0.5. How many subjects per group for 80% power at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "subjects_per_group": 40,
      "total_subjects": 80,
      "measurements_per_subject": 4,
      "effect_size_d": 0.5,
      "icc": 0.5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 2,
      "power": 0.03
    },
    "source": "R simulation verification (2026-01-26)",
    "reference_code": "# n_ind = 64/group (pwr.t.test, d=0.5)\n# DE = 1 + (4-1)*0.5 = 2.5; multiplier = 4/2.5 = 1.6\n# n_repeated = ceil(64/1.6) = 40/group\n# simr simulation (500 sims): n=38-40 gives 80% power\n# Previous GT of 80 was WRONG (gave 98% power)",
    "tier": 2
  },
  {
    "id": "t2-mixed-002",
    "template": "mixed_effects_lmm",
    "difficulty": "basic",
    "question": "We're doing a simple pre-post study comparing treatment vs control, with measurements at baseline and 8 weeks. We expect a treatment effect of d = 0.4, and within-patient correlation is about ICC = 0.6. How many subjects per group for 80% power at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "subjects_per_group": 80,
      "total_subjects": 160,
      "measurements_per_subject": 2,
      "effect_size_d": 0.4,
      "icc": 0.6,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "R simulation verification (2026-01-26)",
    "reference_code": "# R lmer simulation (1000 sims): n=80/grp gives 80.3% power\n# n=100/grp gives 87.4%, n=60/grp gives 68.4%\n# Analytical DE formula: t-test n=100, RM efficiency=0.8, n=80/grp\n# Both analytical and simulation confirm GT=80",
    "tier": 2
  },
  {
    "id": "t2-mixed-003",
    "template": "mixed_effects_lmm",
    "difficulty": "intermediate",
    "question": "We're running a clinical trial with monthly visits over 6 months to detect a treatment effect of d = 0.35. The within-patient correlation (compound symmetry) is about ICC = 0.4. How many subjects per arm for 90% power at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "subjects_per_group": 87,
      "total_subjects": 174,
      "measurements_per_subject": 6,
      "effect_size_d": 0.35,
      "icc": 0.4,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 3,
      "power": 0.03
    },
    "source": "pwr package with design effect (R-validated 2026-01-29)",
    "reference_code": "# Simple t-test: n=173/grp, DE = (1+(6-1)*0.4)/6 = 0.5\n# With RM efficiency: n = 173 * 0.5 = 87 per group",
    "tier": 2
  },
  {
    "id": "t2-mixed-004",
    "template": "mixed_effects_lmm",
    "difficulty": "intermediate",
    "question": "Our trial has patients nested within 10 clinics per treatment arm. The treatment effect is d = 0.3 and the ICC between patients within clinics is 0.05. How many patients per clinic do we need for 80% power at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "clusters_per_arm": 10,
      "patients_per_cluster": 150,
      "total_patients": 3000,
      "effect_size_d": 0.3,
      "icc": 0.05,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 50,
      "power": 0.08
    },
    "source": "simr package CRAN vignette",
    "reference_code": "# Cluster design with ICC=0.05\n# Individual n needed: 176/arm for d=0.3, power=80%\n# Design effect at m patients: DE = 1 + (m-1)*0.05\n# At m=80: DE=4.95, effective n=800/4.95=162, power=76.7%\n# At m=150: DE=8.45, effective n=1500/8.45=178, power=80.5%\n# GT CORRECTED: 80 patients gives 76.7%, need ~150 for 80%",
    "tier": 2
  },
  {
    "id": "t2-mixed-005",
    "template": "mixed_effects_lmm",
    "difficulty": "advanced",
    "question": "We have a three-level study design: 3 measurements within patients, 15 patients within sites, and 5 sites per treatment arm. The variance is partitioned as: site variance = 0.02, patient variance = 0.50, residual = 0.48 (total = 1.0). We want to detect a treatment effect of d = 0.4. What power does this design achieve at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "given_sites_per_arm": 5,
      "given_patients_per_site": 15,
      "given_measurements_per_patient": 3,
      "given_total_patients": 150,
      "effect_size_d": 0.4,
      "site_icc": 0.02,
      "patient_icc": 0.5,
      "power": 0.58,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.08
    },
    "source": "simr powerSim nsim=500 KR (R-validated 2026-02-05)",
    "reference_code": "library(simr)\n# Three-level model: y ~ treatment + (1|site/patient)\n# sigma_site=sqrt(0.02), sigma_patient=sqrt(0.50), sigma_residual=sqrt(0.48)\n# fixef: intercept=0, treatment=0.4\n# simr nsim=500: power=58.4% (CI: 53.9%-62.8%)\n# With nsim=500: SE=0.022, results reliably in 0.53-0.63 range",
    "reference_code_note": "simr with correct variance assignment: site=0.02, patient_within_site=0.50. Power=57-58% across seeds. CAUTION: makeLmer reorders VarCorr - must verify variance assignment matches specification.",
    "tier": 2
  },
  {
    "id": "t2-mixed-006",
    "template": "mixed_effects_lmm",
    "difficulty": "advanced",
    "question": "I'm analyzing a growth curve study comparing two groups over 8 time points (0-7). I expect the groups to differ in their slopes by 0.1 units per time point. The random intercept SD is 1.0, random slope SD is 0.3, and residual SD is 1.0. What's the minimum sample size per group to achieve 80% power at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "subjects_per_group": 190,
      "total_subjects": 380,
      "time_points": 8,
      "slope_difference": 0.1,
      "residual_sd": 1,
      "random_intercept_sd": 1,
      "random_slope_sd": 0.3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 18,
      "power": 0.03
    },
    "source": "Monte Carlo simulation (R-validated 2026-02-01)",
    "reference_code": "# simr powerSim (KR test, nsim=500):\n# n=170: 74.4%, n=180: 78.8%, n=190: 81.4%, n=200: 83.8%\n# First n to exceed 80% is n=190/group\n# GT=190; tolerance \u00b119 accepts 171-209",
    "tier": 2
  },
  {
    "id": "t2-mixed-007",
    "template": "mixed_effects_lmm",
    "difficulty": "advanced",
    "question": "I'm planning a 2-period AB/BA crossover trial. The within-subject correlation is 0.7, and the treatment effect is d = 0.6 (standardized by total SD). How many subjects do I need for 85% power at alpha = 0.05?",
    "expected_template": "crossover_2x2",
    "ground_truth": {
      "subjects": 17,
      "periods": 2,
      "effect_size_d": 0.6,
      "within_subject_correlation": 0.7,
      "power": 0.85,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr.t.test paired (R-validated 2026-01-28)",
    "reference_code": "library(pwr)\n# d_paired = 0.6/sqrt(2*(1-0.7)) = 0.6/sqrt(0.6) = 0.7746\npwr.t.test(d=0.6/sqrt(2*(1-0.7)), power=0.85, sig.level=0.05, type='paired')\n# n = 17 subjects",
    "tier": 2
  },
  {
    "id": "t2-mixed-008",
    "template": "mixed_effects_lmm",
    "difficulty": "advanced",
    "question": "I'm running a trial with 2 correlated outcomes (correlation = 0.6) measured at 3 time points. The treatment effect on the primary outcome is d = 0.4, and within-subject ICC is 0.5. I only need to power for the primary outcome (no multiplicity correction). How many subjects per group for 80% power at alpha = 0.05?",
    "expected_template": "mixed_effects_continuous",
    "ground_truth": {
      "subjects_per_group": 67,
      "total_subjects": 134,
      "outcomes": 2,
      "correlation_outcomes": 0.6,
      "time_points": 3,
      "effect_size_d": 0.4,
      "icc": 0.5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 3,
      "power": 0.03
    },
    "source": "pwr package + RM design effect (R-validated 2026-01-29)",
    "reference_code": "library(pwr)\n# pwr.t.test(d=0.4, power=0.80, sig.level=0.05, type='two.sample')$n = 99.08\n# ceiling(99.08) = 100 per group\n# RM design effect: DE = (1+(3-1)*0.5)/3 = 2/3 = 0.667\n# n_adjusted = ceiling(100 * 0.667) = ceiling(66.7) = 67/grp\n# Validated: R confirms base=100, adjusted=67 (2026-01-29)",
    "tier": 2
  },
  {
    "id": "t2-surv-001",
    "template": "survival_analysis",
    "difficulty": "basic",
    "question": "We're planning a survival trial where we expect the treatment to reduce the hazard by 30% (HR = 0.7). Control group median survival is 2 years. We'll have 3 years of accrual and 2 years of additional follow-up. How many patients per arm for 80% power at alpha = 0.05?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "subjects_per_arm": 198,
      "total_subjects": 396,
      "hazard_ratio": 0.7,
      "control_median_survival": 2,
      "accrual_years": 3,
      "followup_years": 2,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "Schoenfeld formula (R-validated 2026-01-29)",
    "reference_code": "# Schoenfeld: events = 4*(1.96+0.8416)^2/(log(0.7))^2 = 247\n# Uniform accrual (3yr) + FU (2yr), exponential survival:\n#   P_event_control = 68.9%, P_event_treatment = 56.3%, avg = 62.6%\n# N/arm = ceil(247/(2*0.626)) = ceil(197.3) = 198\n# Deterministic: question fully specifies all parameters",
    "tier": 2
  },
  {
    "id": "t2-surv-002",
    "template": "survival_analysis",
    "difficulty": "basic",
    "question": "In our survival study, the control group has 50% 2-year survival and we expect HR = 0.65 for the treatment. With 2-year accrual and 1-year additional follow-up, how many patients per arm for 80% power at alpha = 0.05?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "subjects_per_arm": 201,
      "total_subjects": 402,
      "hazard_ratio": 0.65,
      "control_2yr_survival": 0.5,
      "accrual_years": 2,
      "followup_years": 1,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "Schoenfeld formula (R-validated 2026-01-29)",
    "reference_code": "# lambda_c=log(2)/2=0.3466, lambda_t=0.3466*0.65=0.2253\n# Schoenfeld events = 4*(1.96+0.8416)^2/(log(0.65))^2 = 170\n# Uniform accrual (2yr) + FU (1yr), exponential survival:\n#   P_event_control = 49.0%, P_event_treatment = 35.7%, avg = 42.4%\n# N/arm = ceil(170/(2*0.424)) = ceil(200.5) = 201\n# Validated: R confirms N/arm=201 (2026-01-29)",
    "tier": 2
  },
  {
    "id": "t2-surv-003",
    "template": "survival_analysis",
    "difficulty": "intermediate",
    "question": "I'm analyzing whether a continuous biomarker predicts survival. I expect an HR of 1.3 per standard deviation increase in the biomarker. About 25% of patients will experience the event during the study. How many patients for 80% power at alpha = 0.05?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "sample_size": 460,
      "hazard_ratio": 1.3,
      "event_rate": 0.25,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "Schoenfeld formula for continuous predictor (verified 2026-01-29)",
    "reference_code": "# Schoenfeld for continuous predictor (no factor of 4):\n# E = (z_alpha/2 + z_beta)^2 / log(HR)^2\n# E = (1.96 + 0.8416)^2 / log(1.3)^2 = 7.849 / 0.0689 = 114.0\n# Events needed = ceiling(114.0) = 115\n# With 25% event rate: n = ceiling(115/0.25) = 460",
    "tier": 2
  },
  {
    "id": "t2-surv-004",
    "template": "survival_analysis",
    "difficulty": "intermediate",
    "question": "We're planning a survival trial expecting HR = 0.75. Control 1-year survival is 70%. We'll have 18 months of accrual and 12 months of follow-up, but expect 20% dropout. How many patients per arm do we need for 85% power at alpha = 0.05, accounting for the dropout?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "subjects_per_arm": 657,
      "total_subjects": 1314,
      "hazard_ratio": 0.75,
      "control_1yr_survival": 0.7,
      "dropout_rate": 0.2,
      "accrual_months": 18,
      "followup_months": 12,
      "power": 0.85,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "Schoenfeld formula + dropout inflation (R-validated 2026-01-29)",
    "reference_code": "# lambda_c=-log(0.70)=0.3567, lambda_t=0.3567*0.75=0.2675\n# Schoenfeld events (85% power) = 4*(1.96+1.036)^2/(log(0.75))^2 = 434\n# Uniform accrual (1.5yr) + FU (1yr), exponential:\n#   P_event_control=45.8%, P_event_treatment=36.9%, avg=41.4%\n# N/arm_no_dropout = ceil(434/(2*0.414)) = 525\n# N/arm_with_dropout = ceil(525/0.80) = ceil(656.25) = 657\n# Deterministic: question specifies uniform accrual + 20% overall attrition",
    "tier": 2
  },
  {
    "id": "t2-surv-005",
    "template": "survival_analysis",
    "difficulty": "advanced",
    "question": "We're planning a survival study to detect HR = 0.70 (treatment benefit). Control median survival is 18 months and total study duration is 36 months. With 1:1 allocation, 80% power, and alpha = 0.05, what's the total sample size needed across both arms?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "total_subjects": 361,
      "subjects_per_arm": 181,
      "events_needed": 247,
      "hazard_ratio": 0.7,
      "control_median_months": 18,
      "study_duration_months": 36,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "Schoenfeld two-group formula (R-validated 2026-02-07)",
    "reference_code": "# Schoenfeld (two-group, 1:1): d = 4*(1.96+0.842)^2 / log(0.7)^2 = 4*7.851/0.1272 = 247 events\n# P(event|ctrl,36mo) = 1-exp(-log(2)/18*36) = 0.75\n# P(event|trt,36mo) = 1-exp(-0.7*log(2)/18*36) = 0.621\n# Avg P(event) = 0.686, Total N = ceil(247/0.686) = 361, per arm = 181\n# Freedman formula / ssizeCT gives 184+184=368 (slightly different)",
    "tier": 2
  },
  {
    "id": "t2-surv-006",
    "template": "survival_analysis",
    "difficulty": "advanced",
    "question": "We're studying a condition where about 30% of control patients are long-term survivors (essentially cured). We expect the treatment to increase this cure fraction to 45%. Among those who aren't cured, median survival is 2 years and we expect a modest HR of 0.8 for treatment vs control. With 5-year follow-up, how many per arm for 80% power at alpha = 0.05?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "subjects_per_arm": 150,
      "total_subjects": 300,
      "cure_rate_control": 0.3,
      "cure_rate_treatment": 0.45,
      "hazard_ratio_uncured": 0.8,
      "median_survival_uncured": 2,
      "followup_years": 5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 15,
      "power": 0.03
    },
    "source": "R Monte Carlo simulation (validated 2026-01-29)",
    "reference_code": "# R Monte Carlo (1000 sims, log-rank test):\n# n=100/arm: 66.3%, n=150/arm: 81.5%, n=200/arm: 92.3%\n# Non-cured: exponential with median=2yr (lambda=0.347)\n# Treatment non-cured: HR=0.8 -> lambda=0.277\n# Cure: cured patients censored at 5yr (no event)\n# GT=150 gives 81.5% power; simulation variance justifies \u00b120",
    "tier": 2
  },
  {
    "id": "t2-surv-007",
    "template": "survival_analysis",
    "difficulty": "advanced",
    "question": "In our survival study, 15% of patients may experience a competing event (e.g., death from unrelated cause) that's independent of treatment. We want to detect a cause-specific HR = 0.7 for our primary event, with a control event rate of 30%. What sample size per arm for 80% power at alpha = 0.05?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "subjects_per_arm": 474,
      "total_subjects": 948,
      "hazard_ratio": 0.7,
      "control_event_rate": 0.3,
      "competing_event_rate": 0.15,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "Schoenfeld + cause-specific hazard competing risks model (verified 2026-01-29)",
    "reference_code": "# Schoenfeld events = ceiling(246.79) = 247\n# Cause-specific hazard model (exponential, competing independent of treatment):\n#   Control: P(primary)=0.30, P(competing)=0.15 -> lambda ratio a:b = 2:1\n#   (a+b)*T = -log(0.55) = 0.5978, a*T = 0.3985, b*T = 0.1993\n#   Treatment: a_t*T = 0.7*0.3985 = 0.2790, b_t*T = 0.1993\n#   P_t(primary) = 0.2790/(0.2790+0.1993) * (1-exp(-0.4783)) = 0.5833*0.3801 = 0.2217\n# Avg primary event prob = (0.30+0.2217)/2 = 0.2608\n# N/arm = ceil(247/(2*0.2608)) = ceil(473.3) = 474\n# Deterministic: question specifies cause-specific HR + independent competing",
    "tier": 2
  },
  {
    "id": "t2-surv-008",
    "template": "survival_analysis",
    "difficulty": "advanced",
    "question": "We're conducting a stratified survival analysis across 3 strata with different baseline hazards, but the treatment HR = 0.75 is assumed constant across strata. Over the 2-year study period, we expect 40% of controls to experience an event. What sample size per arm for 80% power at alpha = 0.05?",
    "expected_template": "survival_logrank",
    "ground_truth": {
      "subjects_per_arm": 475,
      "total_subjects": 950,
      "hazard_ratio": 0.75,
      "strata": 3,
      "control_event_rate": 0.4,
      "study_duration_years": 2,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 10,
      "power": 0.03
    },
    "source": "Schoenfeld formula (R-validated 2026-02-01)",
    "reference_code": "# Stratified log-rank using Schoenfeld formula\n# Events via Schoenfeld: d = 4*(1.96 + 0.84)^2 / log(0.75)^2 \u2248 379\n# With 40% event rate: n = 379/0.40 = 948 total \u2248 475 per arm\n# GT corrected based on Schoenfeld formula verification",
    "reference_code_note": "Schoenfeld: d=380 events. Conservative: N=380/0.40=950 total=475/arm. Average-rate method gives 529/arm.",
    "tier": 2
  },
  {
    "id": "t2-poisson-001",
    "template": "poisson_regression",
    "difficulty": "basic",
    "question": "We're comparing event rates between treatment and control. The control group has about 2 events per person-year, and we expect the treatment to increase this by 50% (rate ratio = 1.5). With 1-year follow-up, how many subjects per group for 80% power at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "subjects_per_group": 40,
      "total_subjects": 79,
      "rate_ratio": 1.5,
      "baseline_rate": 2,
      "followup_years": 1,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 2,
      "power": 0.03
    },
    "source": "pwrss package CRAN documentation",
    "reference_code": "library(pwrss)\npower.z.poisson(base.rate=2, rate.ratio=1.5, power=0.80, alpha=0.05, dist='bernoulli')\n# n = 79 total (40 per group)",
    "tier": 2
  },
  {
    "id": "t2-poisson-002",
    "template": "poisson_regression",
    "difficulty": "basic",
    "question": "We're testing whether a treatment reduces infection rate. Control patients average 0.5 infections per month, and we hope to reduce this to 0.3 per month. With 6 months of follow-up, how many per group for 80% power at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "subjects_per_group": 27,
      "total_subjects": 54,
      "control_rate": 0.5,
      "treatment_rate": 0.3,
      "followup_months": 6,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwrss + MC simulation (R-validated 2026-01-29)",
    "reference_code": "library(pwrss)\n# RR = 0.3/0.5 = 0.6, rates per 6 months: control=3, treatment=1.8\npwrss.z.poisson(exp.beta0=3, exp.beta1=0.6, power=0.80, alpha=0.05, dist='bernoulli')\n# n = 53 total (27 per group)\n# Deterministic: pwrss formula gives exactly 27/group",
    "tier": 2
  },
  {
    "id": "t2-poisson-003",
    "template": "poisson_regression",
    "difficulty": "intermediate",
    "question": "Our outcome is count data with overdispersion (variance greater than mean). The control group averages 5 events, treatment group expected to average 4 events, with dispersion parameter k = 2. How many per group for 80% power at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "subjects_per_group": 230,
      "total_subjects": 460,
      "control_mean": 5,
      "treatment_mean": 4,
      "dispersion_k": 2,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 23,
      "power": 0.03
    },
    "source": "Wald formula + MC simulation (verified 2026-01-29)",
    "reference_code": "library(MASS)\n# Wald: n = (z_a+z_b)^2 * (1/mu0+1/k + 1/mu1+1/k) / log(mu1/mu0)^2 = 229\n# MC simulation (5000 sims, seed=42):\n#   n=220: power=0.788, n=225: power=0.794, n=230: power=0.812\n# First n to cross 80%: n=230 per group",
    "tier": 2
  },
  {
    "id": "t2-poisson-004",
    "template": "poisson_regression",
    "difficulty": "intermediate",
    "question": "We're analyzing count data where patients have variable follow-up time (mean 0.8 years, SD 0.3 years). The baseline event rate is 3 per year, and we want to detect a rate ratio of 1.4. How many subjects per group for 80% power at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "subjects_per_group": 50,
      "total_subjects": 100,
      "rate_ratio": 1.4,
      "baseline_rate": 3,
      "mean_exposure": 0.8,
      "sd_exposure": 0.3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "R Monte Carlo simulation (validated 2026-01-29)",
    "reference_code": "# R Monte Carlo (2000 sims, Poisson GLM with offset):\n# n=20/grp: 43.6%, n=50/grp: 81.0%, n=80/grp: 93.8%\n# GT=50/grp gives 81% power; simulation variance justifies \u00b110",
    "tier": 2
  },
  {
    "id": "t2-poisson-005",
    "template": "poisson_regression",
    "difficulty": "advanced",
    "question": "Our count outcome has 30% structural zeros (patients who can never experience the event). Among those who can have events, the control mean is 3 and treatment mean is 2. What sample size per group for 80% power at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "subjects_per_group": 150,
      "total_subjects": 300,
      "zero_inflation": 0.3,
      "control_mean_nonzero": 3,
      "treatment_mean_nonzero": 2,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 15,
      "power": 0.03
    },
    "source": "R Monte Carlo simulation (validated 2026-01-29)",
    "reference_code": "# Zero-inflated Poisson (ZIP) model\n# Observed mean = (1-p_zero) * lambda\n# Control: 0.7 * 3 = 2.1, Treatment: 0.7 * 2 = 1.4\n# MC simulation with Wilcoxon test (2000 iter):\n# n=70: power=0.478, n=100: power=0.632, n=150: power=0.828\n# n \u2248 150 per group for 80% power",
    "tier": 2
  },
  {
    "id": "t2-poisson-006",
    "template": "poisson_regression",
    "difficulty": "advanced",
    "question": "I'm studying a condition where the event rate increases by about 10% each year. At baseline (time 0), the rate is 1 event per person-year. I want to detect if a treatment reduces the rate ratio to 0.8 over a 3-year follow-up period. What sample size per group for 80% power at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "subjects_per_group": 103,
      "total_subjects": 206,
      "rate_ratio": 0.8,
      "baseline_rate": 1,
      "rate_increase_per_year": 0.1,
      "study_years": 3,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "Wald formula (R-validated 2026-02-01)",
    "reference_code": "# Integrated rate over 3 years: lambda*integral(1+0.1t)dt from 0 to 3\n# = baseline*(3 + 0.1*9/2) = 1.0*3.45 = 3.45 expected events/person (control)\n# Treatment: 0.8*3.45 = 2.76 expected events/person\n# Wald: n = (z_a+z_b)^2*(1/mu_c+1/mu_t)/log(RR)^2 = (2.80)^2*(0.290+0.362)/0.0498 = 103\n# MC simulation (5000 sims): n=100: 0.780, n=110: 0.841\n# GT=105 per group for 80% power",
    "reference_code_note": "Integrated rate over 3yr: compound=3.47, linear=3.45 events/person. Wald gives 103/group. Query specifies rate increases 10%/year - must integrate time-varying rate.",
    "tier": 2
  },
  {
    "id": "t2-poisson-007",
    "template": "poisson_regression",
    "difficulty": "advanced",
    "question": "We have 20 clinics per arm with 25 subjects per clinic. The ICC is 0.03, baseline rate is 4 events per person-year, and we want to detect a rate ratio of 1.25. What power does this cluster design achieve at alpha = 0.05?",
    "expected_template": "poisson_regression",
    "ground_truth": {
      "given_clusters_per_arm": 20,
      "given_subjects_per_cluster": 25,
      "given_total_subjects": 1000,
      "rate_ratio": 1.25,
      "control_rate": 4,
      "icc": 0.03,
      "power": 0.99,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.03
    },
    "source": "Analytical DE + Wald formula (R-validated 2026-02-05)",
    "reference_code": "# Clustered Poisson with ICC=0.03, rate=4/yr\n# DE = 1 + 24*0.03 = 1.72\n# Effective n per arm = 500/1.72 = 290.7\n# Expected events per arm (ctrl) = 290.7 * 4 = 1163\n# Wald Z = |log(1.25)| * sqrt(1163) / sqrt(1/4 + 1/5) = 0.2231 * 34.1 / 0.6708 = 11.34\n# Power = Phi(11.34 - 1.96) = Phi(9.38) = 1.000 (essentially 100%)\n# GT=0.99 is conservative; analytical gives >0.999",
    "tier": 2
  },
  {
    "id": "t3-cluster-001",
    "template": "cluster_rct",
    "difficulty": "basic",
    "question": "We're planning a cluster randomized trial with 20 clinics per arm. Based on pilot data, the ICC is about 0.05 and we expect a treatment effect of d = 0.4. How many patients do we need per clinic to achieve 80% power at alpha = 0.05?",
    "expected_template": "cluster_rct",
    "ground_truth": {
      "clusters_per_arm": 20,
      "subjects_per_cluster": 7,
      "total_subjects": 280,
      "effect_size_d": 0.4,
      "icc": 0.05,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "R simulation verification (2026-01-26)",
    "reference_code": "library(pwr)\n# REVERSE CRT: given k=20 clusters/arm, find m (subjects/cluster)\n# n_ind = ceil(pwr.t.test(d=0.4, power=0.80)$n) = 100\n# m = ceil(n_ind * (1-ICC) / (k - n_ind*ICC))\n#   = ceil(100 * 0.95 / (20 - 5)) = ceil(6.33) = 7\n# Verify: DE = 1 + 6*0.05 = 1.30; eff_n = 20*7/1.30 = 107.7 >= 100\n# Simulation (5000 sims): m=7 gives 81% power\n# Deterministic analytical formula; tight tolerance \u00b13",
    "tier": 3
  },
  {
    "id": "t3-cluster-002",
    "template": "cluster_rct",
    "difficulty": "basic",
    "question": "We're designing a school-based intervention with 15 schools per arm, averaging 30 students per school. The ICC is estimated at 0.08. What is the minimum detectable effect size (Cohen's d) with 80% power?",
    "expected_template": "cluster_rct",
    "ground_truth": {
      "given_clusters_per_arm": 15,
      "given_subjects_per_cluster": 30,
      "given_total_subjects": 900,
      "icc": 0.08,
      "detectable_effect_d": 0.35,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "effect_size": 0.03,
      "power": 0.03
    },
    "source": "clusterPower package CRAN documentation",
    "reference_code": "library(clusterPower)\n# Design effect = 1 + 29*0.08 = 3.32\n# Effective n per arm = 15*30/3.32 = 136\n# Total effective n = 272\n# Detectable d \u2248 0.35",
    "tier": 3
  },
  {
    "id": "t3-cluster-003",
    "template": "cluster_rct",
    "difficulty": "intermediate",
    "question": "Our cluster RCT will have variable cluster sizes (CV = 0.4 in cluster sizes). We have 25 clusters per arm with a mean of 20 subjects per cluster. With ICC = 0.03 and effect size d = 0.3, what power do we achieve at alpha = 0.05?",
    "expected_template": "cluster_rct",
    "ground_truth": {
      "given_clusters_per_arm": 25,
      "given_mean_cluster_size": 20,
      "cv_cluster_size": 0.4,
      "given_total_subjects": 1000,
      "effect_size_d": 0.3,
      "icc": 0.03,
      "power": 0.94,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.05
    },
    "source": "R analytical + simulation verification",
    "reference_code": "# DE = 1+(20-1)*0.03 = 1.57, CV adjustment = 1+0.16 = 1.16\n# Adjusted DE = 1.57*1.16 = 1.82, eff_N/arm = 25*20/1.82 = 275\n# t-test n for d=0.3 at 80% = 176/arm; 275 >> 176, power very high\n# Simulation (1000 sims): power = 93.7%\n# GT=0.94; simulation variance justifies \u00b10.04",
    "tier": 3
  },
  {
    "id": "t3-cluster-004",
    "template": "cluster_rct",
    "difficulty": "intermediate",
    "question": "We're running a cluster trial with a binary outcome: 30% response in control, 20% expected in treatment. We have 12 clinics per arm and the ICC is 0.02. How many patients per clinic for 80% power at alpha = 0.05?",
    "expected_template": "cluster_rct",
    "ground_truth": {
      "clusters_per_arm": 12,
      "subjects_per_cluster": 48,
      "total_subjects": 1152,
      "control_rate": 0.3,
      "treatment_rate": 0.2,
      "icc": 0.02,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 3,
      "power": 0.03
    },
    "source": "Analytical formula (R-validated 2026-01-29)",
    "reference_code": "# Individual n per group (power.prop.test) \u2248 294\n# Solve: 12*m / (1 + (m-1)*0.02) >= 294\n# m = ceil(288.12/6.12) = ceil(47.08) = 48\n# Verify: DE=1+47*0.02=1.94, eff_n=12*48/1.94=296.9 >= 294 \u2713\n# Deterministic analytical formula; tolerance \u00b110 for method variation",
    "tier": 3
  },
  {
    "id": "t3-cluster-005",
    "template": "cluster_rct",
    "difficulty": "advanced",
    "question": "We're designing a stepped-wedge trial with 6 clusters over 4 time periods. Two clusters switch to treatment each period. Within-period ICC is 0.05, between-period correlation is 0.025. We want to detect d = 0.5 with at least 80% power. What is the minimum number of subjects per cluster-period?",
    "expected_template": "cluster_rct",
    "ground_truth": {
      "clusters": 6,
      "periods": 4,
      "switches_per_period": 2,
      "subjects_per_cluster_period": 7,
      "total_subjects": 168,
      "effect_size_d": 0.5,
      "icc": 0.05,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "swdpwr::swdpower (analytical, verified 2026-01-27)",
    "reference_code": "library(swdpwr)\ndesign <- matrix(0, 6, 4)\ndesign[1:2,2:4] <- 1; design[3:4,3:4] <- 1; design[5:6,4] <- 1\n# K=6: Power=0.792 (<80%), K=7: Power=0.840 (>=80%)\n# Minimum K for >=80% power = 7\n# Deterministic stepped-wedge formula; tight tolerance \u00b13",
    "tier": 3
  },
  {
    "id": "t3-cross-001",
    "template": "crossover_trial",
    "difficulty": "basic",
    "question": "I'm planning a 2x2 crossover trial where the between-subject effect size is d = 0.5 and the within-subject correlation is 0.7. How many subjects total do I need for 80% power at alpha = 0.05?",
    "expected_template": "crossover_2x2",
    "ground_truth": {
      "subjects": 21,
      "periods": 2,
      "effect_size_d": 0.5,
      "within_subject_correlation": 0.7,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr.t.test paired with adjusted d (R-validated 2026-02-01)",
    "reference_code": "library(pwr)\n# d_paired = d / sqrt(2*(1-rho)) = 0.5 / sqrt(0.6) = 0.6455\n# pwr.t.test(d=0.6455, power=0.80, sig.level=0.05, type=\"paired\")\n# n = ceiling(20.88) = 21 subjects",
    "tier": 3
  },
  {
    "id": "t3-cross-002",
    "template": "crossover_trial",
    "difficulty": "basic",
    "question": "I'm planning a simple 2-period crossover trial (AB/BA design) to compare two treatments. I expect a standardized within-subject effect of d = 0.3 (the treatment difference divided by within-subject SD). How many subjects total do I need for 90% power at alpha = 0.05?",
    "expected_template": "crossover_2x2",
    "ground_truth": {
      "subjects": 119,
      "periods": 2,
      "effect_size_d": 0.3,
      "power": 0.9,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr.t.test paired (R-validated 2026-01-29)",
    "reference_code": "library(pwr)\n# Crossover = paired design\npwr.t.test(d = 0.3, power = 0.90, sig.level = 0.05, type = 'paired')\n# n = 119 subjects",
    "tier": 3
  },
  {
    "id": "t3-cross-003",
    "template": "crossover_trial",
    "difficulty": "intermediate",
    "question": "I'm designing a 3-treatment Latin square crossover study. The primary comparison is treatment A vs control, with expected effect d = 0.4 and period-to-period correlation of 0.6. How many subjects for 80% power at alpha = 0.05?",
    "expected_template": "crossover_2x2",
    "ground_truth": {
      "subjects": 42,
      "treatments": 3,
      "periods": 3,
      "effect_size_d": 0.4,
      "period_correlation": 0.6,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr.t.test paired adjusted (R-validated 2026-01-29)",
    "reference_code": "# 3-treatment crossover, each subject gets all treatments\n# Planned single pairwise comparison: treatment vs control\n# Within-subject variance = 2*(1-rho) = 2*(1-0.6) = 0.8\n# d_adjusted = 0.4/sqrt(0.8) = 0.4472\n# pwr.t.test(d=0.4472, power=0.80, sig.level=0.05, type='paired')$n = 41.12 -> 42\n# Verified by R: ceiling(pwr.t.test(d=0.4/sqrt(2*(1-0.6)), power=0.80, sig.level=0.05, type='paired')$n) = 42",
    "tier": 3
  },
  {
    "id": "t3-cross-004",
    "template": "crossover_trial",
    "difficulty": "advanced",
    "question": "We're designing a 4-period replicate crossover study (ABAB/BABA) for a bioequivalence trial. The treatment effect is d = 0.35 (where d = mean difference divided by within-subject SD). How many subjects total for 85% power at alpha = 0.05?",
    "expected_template": "crossover_2x2",
    "ground_truth": {
      "subjects": 76,
      "periods": 4,
      "effect_size_d": 0.35,
      "within_subject_sd": 1,
      "power": 0.85,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr.t.test paired (R-validated 2026-02-01)",
    "reference_code": "library(pwr)\n# d=0.35 as paired effect size directly\n# pwr.t.test(d=0.35, power=0.85, sig.level=0.05, type=\"paired\")\n# n = ceiling(75.24) = 76 subjects",
    "tier": 3
  },
  {
    "id": "t3-fact-001",
    "template": "factorial_design",
    "difficulty": "basic",
    "question": "We're running a 2\u00d72 factorial trial where we want to detect a main effect of one factor with d = 0.4. We need 80% power at per-comparison alpha = 0.05 (no multiplicity adjustment). How many subjects per cell?",
    "expected_template": "factorial_2x2",
    "ground_truth": {
      "total_sample_size": 200,
      "per_cell": 50,
      "factors": 2,
      "levels_per_factor": 2,
      "effect_size_d": 0.4,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 2,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\n# Main effect powered as 2-group comparison\n# For d=0.4: n per group = 100\n# 2x2 design: each main effect compares n/2 vs n/2\n# Total n = 200 (50 per cell)\n# Tight tolerance \u00b18 for minor method variation",
    "tier": 3
  },
  {
    "id": "t3-fact-002",
    "template": "factorial_design",
    "difficulty": "intermediate",
    "question": "In our 2\u00d72 factorial, the main effects are d = 0.5 but we're primarily interested in detecting the interaction (d = 0.3). How many per cell for 80% power on the interaction at alpha = 0.05?",
    "expected_template": "factorial_2x2",
    "ground_truth": {
      "total_sample_size": 1400,
      "per_cell": 350,
      "main_effect_d": 0.5,
      "interaction_d": 0.3,
      "power_interaction": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 5,
      "power": 0.03
    },
    "source": "R simulation verification (2026-01-26)",
    "reference_code": "library(pwr)\n# 2x2 factorial interaction: f\u00b2 = d\u00b2/16 (NOT d\u00b2/4!)\n# f\u00b2 = 0.3\u00b2/16 = 0.005625\n# pwr.f2.test(u=1, f2=0.005625, sig.level=0.05, power=0.80)\n# \u2192 v=1396, N=1400, per_cell=350\n# Simulation (5000 sims): n=350/cell gives ~80% power\n# n=89/cell gives only 29% power (f\u00b2=d\u00b2/4 is WRONG)",
    "tier": 3
  },
  {
    "id": "t3-fact-003",
    "template": "factorial_design",
    "difficulty": "intermediate",
    "question": "We have a 3\u00d72 factorial design (3 doses \u00d7 2 formulations) and want to test the linear dose-response trend (effect size f = 0.25). How many total subjects for 80% power at alpha = 0.05?",
    "expected_template": "factorial_2x2",
    "ground_truth": {
      "total_sample_size": 132,
      "per_cell": 22,
      "dose_levels": 3,
      "formulations": 2,
      "effect_size_f": 0.25,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 1,
      "power": 0.03
    },
    "source": "pwr package CRAN vignette",
    "reference_code": "library(pwr)\n# Linear dose-response contrast (1 df)\n# f = 0.25 -> f\u00b2 = 0.0625\n# pwr.f2.test(u=1, f2=0.0625) gives v=126, N=v+6=132\n# This is for testing LINEAR TREND specifically (1 df contrast)\n# NOT the overall dose effect (2 df)\n# Tight tolerance \u00b15 for deterministic formula",
    "tier": 3
  },
  {
    "id": "t3-fact-004",
    "template": "factorial_design",
    "difficulty": "advanced",
    "question": "We're running a 2^4 fractional factorial (resolution IV, 8 runs) with 3 replicates per run. The standardized main effect is delta = 1.5 (in sigma units). What power do we have for detecting main effects at alpha = 0.05?",
    "expected_template": "factorial_2x2",
    "ground_truth": {
      "runs": 8,
      "replicates_per_run": 3,
      "total_observations": 24,
      "factors": 4,
      "standardized_effect": 1.5,
      "power": 0.93,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.03
    },
    "source": "MC simulation 5000 sims + noncentral F (R-validated 2026-01-29)",
    "reference_code": "# Monte Carlo sim (5000 sims): 94.3%\n# Noncentral F (lambda=13.5, df1=1, df2=16): 93.1%\n# pwr.f2.test (f2=ncp/v=13.5/16=0.844, u=1, v=16): 93.1%\n# NOTE: f2=delta^2/4=0.5625 is WRONG (gives 84.8%)\n# Correct ncp = n_per_level * delta^2 / 2 = 12*2.25/2 = 13.5\n# GT=0.93; tolerance +/-0.05 accepts 0.88-0.98",
    "tier": 3
  },
  {
    "id": "t3-simr-001",
    "template": "simulation_simr",
    "difficulty": "basic",
    "question": "I'm planning a mixed-effects study with 5 measurements per subject. The treatment effect is 0.5 units, random intercept SD is 0.8, and residual SD is 1.0. How many subjects per group do I need for 80% power?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "subjects_per_group": 50,
      "total_subjects": 100,
      "measurements_per_subject": 5,
      "fixed_effect": 0.5,
      "random_intercept_sd": 0.8,
      "residual_sd": 1,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 15,
      "power": 0.08
    },
    "source": "simr powerSim (R-validated 2026-02-01)",
    "reference_code": "library(lme4); library(simr)\n# IMPORTANT: create model at SMALL initial n, then extend UP\n# Creating at large n and extending down causes simr bugs\n# makeLmer at 10/group, extend along subject to test 20-80/group\n# powerSim at n=50/group (nsim=200): ~81% power\n# GT=50/group; tolerance +/-15 accepts 35-65 (simr stochastic)",
    "tier": 3
  },
  {
    "id": "t3-simr-002",
    "template": "simulation_simr",
    "difficulty": "intermediate",
    "question": "I'm powering a study with a binary outcome analyzed using a mixed-effects logistic model. Each subject provides 3 observations, the treatment OR is 2.0, baseline probability is 30%, and the random intercept SD is 0.5. How many subjects per group for 80% power?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "subjects_per_group": 58,
      "total_subjects": 116,
      "observations_per_subject": 3,
      "treatment_or": 2,
      "baseline_prob": 0.3,
      "random_intercept_sd": 0.5,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 20,
      "power": 0.08
    },
    "source": "simr powerSim nsim=500 multiple seeds (R-validated 2026-02-03)",
    "reference_code": "library(simr)\n# GLMM: binomial family, logit link\n# y ~ group + (1|subject), OR=2.0, p0=0.3, random intercept SD=0.5\n# VarCorr = 0.5^2 = 0.25 (simr VarCorr takes VARIANCE not SD)\n# nsim=500 verification: n=55: 76%, n=58: 81%, n=60: 85%\n# 80% power crossing at ~58/group\n# GT=58; tolerance=20 accepts 38-78 (GLMM binary has high MC variance)",
    "tier": 3
  },
  {
    "id": "t3-simr-003",
    "template": "simulation_simr",
    "difficulty": "intermediate",
    "question": "I have a random slopes model with 6 time points (0-5). I want to detect a slope difference of 0.1 per time unit between groups. Random intercept SD = 1.0, random slope SD = 0.2, residual SD = 1.0. With 40 subjects per group, what power do I achieve at alpha = 0.05?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "given_subjects_per_group": 40,
      "time_points": 6,
      "slope_difference": 0.1,
      "random_intercept_sd": 1,
      "random_slope_sd": 0.2,
      "residual_sd": 1,
      "power": 0.32,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.12
    },
    "source": "simr powerSim nsim=200 (R-validated 2026-02-01, multiple tests)",
    "reference_code": "library(lme4); library(simr)\n# makeLmer can give NaN thetas - use lmer + override instead\n# Simulate data, fit lmer, then set fixef/VarCorr/sigma\n# z-test: 36.5%, LRT: 30.0%, KR: 26.0%\n# GT=0.32 (midpoint); tolerance +/-0.12 accepts 0.20-0.44",
    "tier": 3
  },
  {
    "id": "t3-simr-004",
    "template": "simulation_simr",
    "difficulty": "advanced",
    "question": "We have a three-level educational study: students in classrooms in schools. Per arm: 10 schools, 4 classrooms per school, 25 students per classroom. Effect size d = 0.3, school variance = 0.05, classroom variance = 0.10, residual = 0.85. What power does this design achieve at alpha = 0.05?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "given_schools_per_arm": 10,
      "given_classrooms_per_school": 4,
      "given_students_per_classroom": 25,
      "given_total_students": 2000,
      "effect_size_d": 0.3,
      "school_icc": 0.05,
      "classroom_icc": 0.1,
      "power": 0.6,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.08
    },
    "source": "simr + MC lmer (R-validated 2026-02-01, corrected variance assignment)",
    "reference_code": "library(lme4); library(simr)\n# CRITICAL: makeLmer swaps variance components with / nesting syntax\n# Must verify VarCorr output matches intended: school=0.05, class=0.10\n# simr (correct variances, nsim=200): 63% (CI: 58%-69%)\n# MC lmerTest (500 sims): 57%\n# School-level t-test (500 sims): 54%\n# GT=0.60; tolerance +/-0.08 accepts 0.52-0.68",
    "tier": 3
  },
  {
    "id": "t3-simr-005",
    "template": "simulation_simr",
    "difficulty": "advanced",
    "question": "I'm analyzing clustered survival data with gamma frailty. We have 20 clusters per arm, 15 subjects per cluster, frailty variance = 0.5. We want to detect HR = 0.7 over 2-year follow-up (baseline 2-year survival = 60%). Using a standard Cox model without frailty, what is the marginal power at alpha = 0.05?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "given_clusters_per_arm": 20,
      "given_subjects_per_cluster": 15,
      "given_total_subjects": 600,
      "hazard_ratio": 0.7,
      "frailty_variance": 0.5,
      "event_rate": 0.4,
      "followup_years": 2,
      "power": 0.58,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.08
    },
    "source": "Monte Carlo simulation 2500 sims, marginal Cox (R-validated 2026-02-01)",
    "reference_code": "# Marginal Cox model (ignoring clustering):\n# coxph(Surv(time, event) ~ group) - no frailty term\n# lambda0 = -log(0.6)/2, HR=0.7, gamma frailty(var=0.5)\n# 2500 sims (5 seeds x 500): marginal power = 0.58 (SD=0.03)\n# NOTE: frailty-adjusted power is ~0.30, but GT uses marginal test\n# GT=0.58; tolerance +/-0.08 accepts 0.50-0.66",
    "tier": 3
  },
  {
    "id": "t3-simr-006",
    "template": "simulation_simr",
    "difficulty": "advanced",
    "question": "I'm analyzing a multivariate mixed model with 2 correlated outcomes (r = 0.6). Treatment effect is 0.4 on the primary outcome and 0.3 on secondary. Each subject has 4 timepoints, random intercept SD = 0.5, residual SD = 1.0. How many subjects per group for 80% power on the primary outcome?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "subjects_per_group": 55,
      "total_subjects": 110,
      "outcomes": 2,
      "outcome_correlation": 0.6,
      "primary_effect": 0.4,
      "secondary_effect": 0.3,
      "random_intercept_sd": 0.5,
      "residual_sd": 1,
      "timepoints": 4,
      "power": 0.8,
      "alpha": 0.05
    },
    "tolerance": {
      "sample_size": 15,
      "power": 0.08
    },
    "source": "R simulation verification (R-validated 2026-02-01)",
    "reference_code": "# Power for primary outcome treatment effect is the SAME\n# whether modeled alone (univariate) or jointly (multivariate).\n# Agent should recognize this and simplify to primary only.\n# Model: y_primary ~ group + (1|subject), 4 timepoints\n# Random intercept SD=0.5, residual SD=1.0, effect=0.4\n# MC sim (500): n=50: 78%, n=55: 82%, n=60: 86%\n# GT=55/group; tolerance=15 accepts 40-70",
    "tier": 3
  },
  {
    "id": "t3-simr-007",
    "template": "simulation_simr",
    "difficulty": "advanced",
    "question": "In our cluster trial, we have 15 clusters per arm with 20 subjects each, measured at 4 time points (0,1,2,3). The treatment effect on the slope is 0.15 per time unit. Random intercept SD = 0.5, random slope SD = 0.1, residual SD = 1.0. What power for the time \u00d7 treatment interaction at alpha = 0.05?",
    "expected_template": "simulation_based_simr",
    "ground_truth": {
      "given_clusters_per_arm": 15,
      "given_subjects_per_cluster": 20,
      "given_total_subjects": 600,
      "time_points": 4,
      "slope_effect": 0.15,
      "random_intercept_sd": 0.5,
      "random_slope_sd": 0.1,
      "residual_sd": 1,
      "power": 0.82,
      "alpha": 0.05
    },
    "tolerance": {
      "power": 0.1
    },
    "source": "simr powerSim nsim=200 (R-validated 2026-02-01)",
    "reference_code": "# y ~ time * treatment + (1 + time|cluster)\n# 15 clusters/arm, 20 subjects/cluster, 4 timepoints\n# simr powerSim (nsim=200): center ~82% (CI: 76%-87%)\n# nsim=100 can give anywhere from 70%-90% due to variance\n# GT=0.82; tolerance +/-0.10 accepts 0.72-0.92",
    "tier": 3
  },
  {
    "id": "t4-binary-001",
    "template": "riley_binary",
    "difficulty": "basic",
    "question": "We're developing a prediction model for liver cirrhosis using logistic regression. We have 24 candidate predictors and expect about 17% of patients will have cirrhosis. Based on similar models, we anticipate a Cox-Snell R\u00b2 of about 0.288. What's the minimum sample size needed for model development?",
    "expected_template": "riley_binary",
    "ground_truth": {
      "sample_size": 662,
      "events": 115,
      "predictors": 24,
      "prevalence": 0.174,
      "cs_rsquared": 0.288,
      "max_shrinkage": 0.9,
      "criteria": "riley_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize CRAN vignette - Example 1 (Cirrhosis prediction)",
    "reference_code": "library(pmsampsize)\npmsampsize(type = 'b', csrsquared = 0.288, parameters = 24, prevalence = 0.174)\n# Results: minimum sample size = 662, 115 events required",
    "tier": 4
  },
  {
    "id": "t4-binary-002",
    "template": "riley_binary",
    "difficulty": "basic",
    "question": "We're building a prediction model for hospital readmission with 24 candidate predictors. Outcome prevalence is about 17.4%, and we anticipate the model will have a Nagelkerke R-squared of 0.36. What sample size is needed using Riley criteria?",
    "expected_template": "riley_binary",
    "ground_truth": {
      "sample_size": 870,
      "events": 152,
      "predictors": 24,
      "prevalence": 0.174,
      "nagelkerke_rsquared": 0.36,
      "max_shrinkage": 0.9,
      "criteria": "riley_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='b', nagrsquared=0.36, parameters=24, prevalence=0.174)\n# N=870, events=152. Must use nagrsquared parameter for Nagelkerke R-squared.",
    "tier": 4
  },
  {
    "id": "t4-binary-003",
    "template": "riley_binary",
    "difficulty": "intermediate",
    "question": "I'm developing a prediction model for postoperative complications, which occur in only 5% of patients. We plan to include 12 predictors and expect a C-statistic of 0.80. Using Riley criteria with default shrinkage (0.90), what sample size is needed?",
    "expected_template": "riley_binary",
    "ground_truth": {
      "sample_size": 1679,
      "events": 84,
      "predictors": 12,
      "prevalence": 0.05,
      "c_statistic": 0.8,
      "max_shrinkage": 0.9,
      "criteria": "riley_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize package (R-validated 2026-01-29)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='b', parameters=12, prevalence=0.05, cstatistic=0.80)\n# Sample size = 1679, Events = 84",
    "tier": 4
  },
  {
    "id": "t4-binary-004",
    "template": "riley_binary",
    "difficulty": "advanced",
    "question": "We're developing a high-dimensional prediction model with 25 predictors (after variable selection), outcome prevalence 12%, and expected C-statistic of 0.82. We want to be more conservative with a shrinkage target of 0.95. What sample size is required?",
    "expected_template": "riley_binary",
    "ground_truth": {
      "sample_size": 2977,
      "events": 357,
      "predictors": 25,
      "prevalence": 0.12,
      "c_statistic": 0.82,
      "max_shrinkage": 0.95,
      "criteria": "riley_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize::pmsampsize (R-validated 2026-01-28)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='b', parameters=25, prevalence=0.12, cstatistic=0.82, shrinkage=0.95)\n# Criteria 1 (shrinkage): 2977, EPP=14.29",
    "tier": 4
  },
  {
    "id": "t4-binary-005",
    "template": "riley_binary",
    "difficulty": "intermediate",
    "question": "We're developing a prediction model for a very rare outcome (3% prevalence) with 8 candidate predictors. We expect a C-statistic of 0.85. What minimum sample size is needed using Riley criteria?",
    "expected_template": "riley_binary",
    "ground_truth": {
      "sample_size": 1214,
      "events": 37,
      "predictors": 8,
      "prevalence": 0.03,
      "c_statistic": 0.85,
      "max_shrinkage": 0.9,
      "criteria": "riley_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='b', parameters=8, prevalence=0.03, cstatistic=0.85)\n# N=1214, events=37, EPP=4.55. Shrinkage criterion dominates.",
    "tier": 4
  },
  {
    "id": "t4-surv-001",
    "template": "riley_survival",
    "difficulty": "basic",
    "question": "We are developing a survival prediction model for time to recurrent venous thromboembolism (VTE). We have 30 candidate predictors. The overall event rate is 0.065, and we anticipate R\u00b2_CS of 0.051. The timepoint of interest is 2 years with mean follow-up of 2.07 years. What minimum sample size is needed?",
    "expected_template": "riley_survival",
    "ground_truth": {
      "sample_size": 5143,
      "events": 692,
      "predictors": 30,
      "event_rate": 0.065,
      "r2_cs": 0.051,
      "timepoint": 2,
      "mean_followup": 2.07,
      "max_shrinkage": 0.9,
      "criteria": "riley_survival"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize CRAN vignette - Example 3 (VTE survival model)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='s', csrsquared=0.051, parameters=30, rate=0.065, timepoint=2, meanfup=2.07)\n# N=5143, events=692",
    "tier": 4
  },
  {
    "id": "t4-surv-002",
    "template": "riley_survival",
    "difficulty": "basic",
    "question": "I'm developing a cancer prognosis model with 6 predictors. The 5-year mortality is 40%, expected Cox-Snell R\u00b2 is 0.25, and mean follow-up is 3 years. What minimum sample size is needed for model development?",
    "expected_template": "riley_survival",
    "ground_truth": {
      "sample_size": 185,
      "events": 222,
      "predictors": 6,
      "event_rate": 0.4,
      "r2_cs": 0.25,
      "timepoint": 5,
      "mean_followup": 3,
      "max_shrinkage": 0.9,
      "criteria": "riley_survival"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize::pmsampsize (R-validated 2026-02-07)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='s', parameters=6, rate=0.40, csrsquared=0.25, timepoint=5, meanfup=3)\n# N=185, events=222 (=185*0.4*3)",
    "tier": 4
  },
  {
    "id": "t4-surv-003",
    "template": "riley_survival",
    "difficulty": "intermediate",
    "question": "I'm developing a cardiovascular risk prediction model with 12 predictors. The 10-year event rate is about 15%, and based on existing models I expect a Cox-Snell R\u00b2 of 0.15. Mean follow-up will be about 6 years. What sample size do I need?",
    "expected_template": "riley_survival",
    "ground_truth": {
      "sample_size": 659,
      "events": 593,
      "predictors": 12,
      "event_rate": 0.15,
      "r2_cs": 0.15,
      "timepoint": 10,
      "mean_followup": 6,
      "max_shrinkage": 0.9,
      "criteria": "riley_survival"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize package (R-validated 2026-01-29)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='s', parameters=12, rate=0.15, csrsquared=0.15, timepoint=10, meanfup=6)\n# N=659, events=593",
    "tier": 4
  },
  {
    "id": "t4-surv-004",
    "template": "riley_survival",
    "difficulty": "advanced",
    "question": "We're developing a survival prediction model where some patients experience a competing event. The primary event rate is 20%, we have 10 predictors, and expected R\u00b2_CS = 0.18. Timepoint of interest is 5 years with mean follow-up of 3 years. What's the Riley sample size for the primary event model?",
    "expected_template": "riley_survival",
    "ground_truth": {
      "sample_size": 449,
      "events": 269,
      "predictors": 10,
      "event_rate": 0.2,
      "r2_cs": 0.18,
      "timepoint": 5,
      "mean_followup": 3,
      "max_shrinkage": 0.9,
      "criteria": "riley_survival"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize package (R-validated 2026-02-07)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='s', parameters=10, rate=0.20, csrsquared=0.18, timepoint=5, meanfup=3)\n# N=449, events=269 (=floor(449*0.20*3))",
    "tier": 4
  },
  {
    "id": "t4-surv-005",
    "template": "riley_survival",
    "difficulty": "intermediate",
    "question": "I'm developing a survival model for a condition with high event rate (40%) over 3 years. We have 15 predictors and expect R\u00b2_CS = 0.20. Mean follow-up is 2.5 years. What minimum sample size is needed?",
    "expected_template": "riley_survival",
    "ground_truth": {
      "sample_size": 597,
      "events": 597,
      "predictors": 15,
      "event_rate": 0.4,
      "r2_cs": 0.2,
      "timepoint": 3,
      "mean_followup": 2.5,
      "max_shrinkage": 0.9,
      "criteria": "riley_survival"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='s', parameters=15, rate=0.40, csrsquared=0.20, timepoint=3, meanfup=2.5)\n# N=597, events=597, person-time=1492.5",
    "tier": 4
  },
  {
    "id": "t4-cont-001",
    "template": "riley_continuous",
    "difficulty": "basic",
    "question": "We are developing a linear regression model to predict fat-free mass in children from 25 candidate predictors. We anticipate an R\u00b2 of 0.2. The intercept is 1.9 and the standard deviation of the outcome is 0.6. What minimum sample size is needed?",
    "expected_template": "riley_continuous",
    "ground_truth": {
      "sample_size": 918,
      "predictors": 25,
      "r2": 0.2,
      "intercept": 1.9,
      "sd": 0.6,
      "max_shrinkage": 0.9,
      "criteria": "riley_continuous"
    },
    "tolerance": {
      "sample_size": 5
    },
    "source": "pmsampsize CRAN vignette - Example 4 (Fat-free mass in children)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='c', rsquared=0.2, parameters=25, intercept=1.9, sd=0.6)\n# N=918",
    "tier": 4
  },
  {
    "id": "t4-cont-002",
    "template": "riley_continuous",
    "difficulty": "basic",
    "question": "I'm building a prediction model for blood pressure with 5 predictors. I expect the model to explain about 30% of the variance. The population mean is around 120 mmHg with SD of 15 mmHg. What sample size do I need using Riley criteria?",
    "expected_template": "riley_continuous",
    "ground_truth": {
      "sample_size": 239,
      "predictors": 5,
      "r2": 0.3,
      "intercept": 120,
      "sd": 15,
      "max_shrinkage": 0.9,
      "criteria": "riley_continuous"
    },
    "tolerance": {
      "sample_size": 5
    },
    "source": "pmsampsize::pmsampsize (R-validated 2026-01-28)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='c', parameters=5, rsquared=0.30, intercept=120, sd=15)\n# N=239",
    "tier": 4
  },
  {
    "id": "t4-cont-003",
    "template": "riley_continuous",
    "difficulty": "intermediate",
    "question": "We're developing a continuous outcome prediction model with 15 predictors but expect only modest explanatory power (R\u00b2 = 0.15). The outcome is standardized (mean 0, SD 1). We want to be more conservative with a shrinkage target of 0.95 (less than 5% shrinkage). What sample size is needed?",
    "expected_template": "riley_continuous",
    "ground_truth": {
      "sample_size": 1508,
      "predictors": 15,
      "r2": 0.15,
      "intercept": 0,
      "sd": 1,
      "max_shrinkage": 0.95,
      "criteria": "riley_continuous"
    },
    "tolerance": {
      "sample_size": 5
    },
    "source": "pmsampsize package (R-validated 2026-02-07)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='c', parameters=15, rsquared=0.15, shrinkage=0.95, intercept=0, sd=1)\n# N=1508",
    "tier": 4
  },
  {
    "id": "t4-cont-004",
    "template": "riley_continuous",
    "difficulty": "advanced",
    "question": "After LASSO variable selection, we have 30 predictors for a continuous outcome model with expected R\u00b2 = 0.35. Using standardized outcome (mean 0, SD 1) and standard Riley criteria (shrinkage 0.90), what sample size is required?",
    "expected_template": "riley_continuous",
    "ground_truth": {
      "sample_size": 580,
      "predictors": 30,
      "r2": 0.35,
      "intercept": 0,
      "sd": 1,
      "max_shrinkage": 0.9,
      "criteria": "riley_continuous"
    },
    "tolerance": {
      "sample_size": 5
    },
    "source": "pmsampsize::pmsampsize (R-validated 2026-01-28)",
    "reference_code": "library(pmsampsize)\npmsampsize(type='c', parameters=30, rsquared=0.35, intercept=0, sd=1)\n# N=580",
    "tier": 4
  },
  {
    "id": "t4-valid-001",
    "template": "external_validation",
    "difficulty": "basic",
    "question": "We're validating a DVT diagnostic model in a new population. The linear predictor follows a normal distribution with mean = -1.75 and SD = 2.16. Expected C-statistic is 0.82, outcome prevalence is 22%. We want precision: C-stat CI width \u2264 0.10, calibration slope CI width \u2264 0.20, O/E ratio CI width \u2264 0.20. What validation sample size is needed?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 1832,
      "events": 403,
      "prevalence": 0.22,
      "cstatistic": 0.82,
      "cstat_ciwidth": 0.1,
      "calslope_ciwidth": 0.2,
      "oe_ciwidth": 0.2,
      "binding_criterion": "calibration_slope",
      "criteria": "pmvalsampsize_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmvalsampsize (R-validated 2026-02-06)",
    "reference_code": "library(pmvalsampsize)\n# lpnormal takes (mean, SD) \u2014 here SD=2.16\npmvalsampsize(type='b', prevalence=0.22, cstatistic=0.82, lpnormal=c(-1.75, 2.16), cstatciwidth=0.1, csciwidth=0.2, oeciwidth=0.2)\n# Criteria 1 (O/E): 1364, Criteria 2 (C-slope): 1832, Criteria 3 (C-stat): 400\n# Final: 1832 (calibration slope binds)",
    "tier": 4
  },
  {
    "id": "t4-valid-002",
    "template": "external_validation",
    "difficulty": "intermediate",
    "question": "We're externally validating a stroke risk model. Expected C-statistic is 0.75 in the new population, prevalence is 15%. The linear predictor follows a normal distribution (mean = -2.0, SD = 1.44). We want: C-stat CI width \u2264 0.08, calibration slope CI width \u2264 0.30, O/E CI width \u2264 0.30. What sample size is needed?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 1183,
      "events": 178,
      "prevalence": 0.15,
      "cstatistic": 0.75,
      "cstat_ciwidth": 0.08,
      "calslope_ciwidth": 0.3,
      "oe_ciwidth": 0.3,
      "binding_criterion": "calibration_slope",
      "criteria": "pmvalsampsize_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmvalsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmvalsampsize)\n# lpnormal takes (mean, SD) \u2014 here SD=1.44\npmvalsampsize(type='b', prevalence=0.15, cstatistic=0.75, lpnormal=c(-2.0, 1.44), cstatciwidth=0.08, csciwidth=0.3, oeciwidth=0.3)\n# Criteria 1 (O/E): 974, Criteria 2 (C-slope): 1183, Criteria 3 (C-stat): 1111\n# Final: 1183 (calibration slope binds)",
    "tier": 4
  },
  {
    "id": "t4-valid-003",
    "template": "external_validation",
    "difficulty": "advanced",
    "question": "We're validating a high-discrimination model (expected C = 0.90) with prevalence 30%. The LP is normally distributed (mean = -0.5, SD = 4.0). We need very precise C-statistic (CI width \u2264 0.05), plus calibration slope and O/E CI widths \u2264 0.20. What validation sample size is required?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 1674,
      "events": 503,
      "prevalence": 0.3,
      "cstatistic": 0.9,
      "cstat_ciwidth": 0.05,
      "calslope_ciwidth": 0.2,
      "oe_ciwidth": 0.2,
      "binding_criterion": "calibration_slope",
      "criteria": "pmvalsampsize_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmvalsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmvalsampsize)\n# lpnormal takes (mean, SD) \u2014 here SD=4.0\npmvalsampsize(type='b', prevalence=0.30, cstatistic=0.90, lpnormal=c(-0.5, 4.0), cstatciwidth=0.05, csciwidth=0.2, oeciwidth=0.2)\n# Criteria 1 (O/E): 898, Criteria 2 (C-slope): 1674, Criteria 3 (C-stat): 746\n# Final: 1674 (calibration slope binds)",
    "tier": 4
  },
  {
    "id": "t4-valid-004",
    "template": "external_validation",
    "difficulty": "intermediate",
    "question": "Our validation study has predicted probabilities following a Beta(2, 5) distribution. Expected C = 0.75, prevalence = 15%. We want: C-stat CI width \u2264 0.10, calibration slope CI width \u2264 0.20, O/E CI width \u2264 0.20. What sample size is needed?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 3733,
      "events": 560,
      "prevalence": 0.15,
      "cstatistic": 0.75,
      "cstat_ciwidth": 0.1,
      "calslope_ciwidth": 0.2,
      "oe_ciwidth": 0.2,
      "lp_distribution": "beta(2,5)",
      "binding_criterion": "calibration_slope",
      "criteria": "pmvalsampsize_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmvalsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmvalsampsize)\npmvalsampsize(type='b', prevalence=0.15, cstatistic=0.75, lpbeta=c(2, 5), cstatciwidth=0.1, csciwidth=0.2, oeciwidth=0.2)\n# Criteria 1 (O/E): 2179, Criteria 2 (C-slope): 3733, Criteria 3 (C-stat): 712\n# Final: 3733 (calibration slope binds)",
    "tier": 4
  },
  {
    "id": "t4-valid-005",
    "template": "external_validation",
    "difficulty": "basic",
    "question": "We're validating the same DVT model as before (LP normal with mean = -1.75, SD = 2.16, C = 0.82, prevalence = 22%) but with more relaxed precision targets: C-stat CI width \u2264 0.10, calibration slope CI width \u2264 0.30, O/E CI width \u2264 0.30. What's the reduced sample size needed?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 814,
      "events": 180,
      "prevalence": 0.22,
      "cstatistic": 0.82,
      "cstat_ciwidth": 0.1,
      "calslope_ciwidth": 0.3,
      "oe_ciwidth": 0.3,
      "binding_criterion": "calibration_slope",
      "criteria": "pmvalsampsize_binary"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmvalsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmvalsampsize)\n# lpnormal takes (mean, SD) \u2014 here SD=2.16\npmvalsampsize(type='b', prevalence=0.22, cstatistic=0.82, lpnormal=c(-1.75, 2.16), cstatciwidth=0.1, csciwidth=0.3, oeciwidth=0.3)\n# Criteria 1 (O/E): 610, Criteria 2 (C-slope): 814, Criteria 3 (C-stat): 400\n# Final: 814 (calibration slope binds)",
    "tier": 4
  },
  {
    "id": "t4-valid-006",
    "template": "external_validation",
    "difficulty": "advanced",
    "question": "We're validating a cancer screening model and also want to assess clinical utility. Expected C = 0.80, prevalence = 10%, LP is normal (mean = -3.0, SD = 2.25). We need standard precision (C-stat CI \u2264 0.10, cal slope CI \u2264 0.20, O/E CI \u2264 0.20). Additionally, at a 10% treatment threshold, expected sensitivity is 0.80, specificity is 0.70, and we want standardized net benefit CI width \u2264 0.20. What sample size is needed?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 3461,
      "events": 347,
      "prevalence": 0.1,
      "cstatistic": 0.8,
      "cstat_ciwidth": 0.1,
      "calslope_ciwidth": 0.2,
      "oe_ciwidth": 0.2,
      "threshold": 0.1,
      "sensitivity": 0.8,
      "specificity": 0.7,
      "nb_ciwidth": 0.2,
      "binding_criterion": "oe_ratio",
      "criteria": "pmvalsampsize_binary_netbenefit"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "pmvalsampsize (R-validated 2026-02-01)",
    "reference_code": "library(pmvalsampsize)\n# lpnormal takes (mean, SD) \u2014 here SD=2.25\npmvalsampsize(type='b', prevalence=0.10, cstatistic=0.80, lpnormal=c(-3.0, 2.25), cstatciwidth=0.1, csciwidth=0.2, oeciwidth=0.2, sensitivity=0.80, specificity=0.70, threshold=0.10, nbciwidth=0.2)\n# Criteria 1 (O/E): 3461, Criteria 2 (C-slope): 2435, Criteria 3 (C-stat): 836, Criteria 4 (St NB): 1090\n# Final: 3461 (O/E criterion binds)",
    "tier": 4
  },
  {
    "id": "t4-valid-007",
    "template": "external_validation",
    "difficulty": "basic",
    "question": "I'm validating an existing prediction model in a new population. The original C-statistic was 0.80, but I expect it to be 0.75 in this new population. Outcome prevalence is 12%. I want the 95% CI width for the C-statistic to be at most 0.10. What sample size do I need?",
    "expected_template": "riley_validation",
    "ground_truth": {
      "sample_size": 1095,
      "events": 132,
      "expected_c_statistic": 0.75,
      "prevalence": 0.12,
      "ci_width": 0.1,
      "criteria": "hanley_mcneil_validation"
    },
    "tolerance": {
      "sample_size": 5,
      "events": 2
    },
    "source": "Hanley-McNeil (1982) variance formula (R-validated 2026-02-07)",
    "reference_code": "C <- 0.75; p <- 0.12; w <- 0.10; z <- qnorm(0.975)\nQ1 <- C/(2-C); Q2 <- 2*C^2/(1+C)\nse <- w/(2*z)\nvar_coeff <- (Q1-C^2)/(1-p) + (Q2-C^2)/p\nN <- ceiling(var_coeff/se^2)  # = 1095",
    "tier": 4
  }
]